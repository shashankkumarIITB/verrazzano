Name:         weblogic-scripts-cm
Namespace:    bar
Labels:       weblogic.createdByOperator=true
              weblogic.operatorName=verrazzano-system
              weblogic.operatorVersion=3.4.0
Annotations:  weblogic.sha256: 1457ae9277ae09696b4b75c92d1d589c1cba1ea47ab0d936b41c2766e7cce72d

Data
====
auxImage.sh:
----
#!/bin/sh
# Copyright (c) 2021, 2022, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# Init container script for the auxiliary image feature.
# See 'domain.spec.serverPod.auxiliaryImages' for details.

# Notes:
# This script purposely tries to exit zero even on failure as
# the Operator monitors the container running this
# script for the Intropector job case, and we have
# seen issues with non-zero exiting scripts.
#
# The operator fails the introspector if it detects an
# ERROR/SEVERE, and succeeds if it detects
# 'executed successfully'.
#
# The main introspector and pod scripts will echo
# the contents of /${AUXILIARY_IMAGE_PATH}/auxiliary-image-logs/
# and fail if they are missing, or if any do not
# include 'executed successfully', or if the scripts
# cannot create (touch) files in /${AUXILIARY_IMAGE_PATH}.
# (See also utils.sh checkAuxiliaryImage function)

scriptDir="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"

if [ "${debug}" == "true" ]; then set -x; fi;

. ${scriptDir}/utils_base.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${scriptDir}/utils_base.sh" && exit 1
UNKNOWN_SHELL=true

checkEnv AUXILIARY_IMAGE_TARGET_PATH AUXILIARY_IMAGE_CONTAINER_NAME || exit 1

initAuxiliaryImage > /tmp/auxiliaryImage.out 2>&1
cat /tmp/auxiliaryImage.out
mkdir -p ${AUXILIARY_IMAGE_TARGET_PATH}/auxiliaryImageLogs
cp /tmp/auxiliaryImage.out ${AUXILIARY_IMAGE_TARGET_PATH}/auxiliaryImageLogs/${AUXILIARY_IMAGE_CONTAINER_NAME}.out
exit

introspectDomain.sh:
----
#!/bin/bash
# Copyright (c) 2018, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# This script introspects a WebLogic DOMAIN_HOME in order to generate:
#
#   - a description of the domain for the operator (domain name, cluster name, ports, etc)
#   - encrypted login files for accessing a NM
#   - encrypted boot.ini for booting WL 
#   - encrypted admin user password passed in via a plain-text secret (for use in sit config)
#   - md5 checksum of the DOMAIN_HOME/security/SerializedSystemIni.dat domain secret file
#   - situational config files for overriding the configuration within the DOMAIN_HOME
#   - Model in Image domain home zips and md5s (when the domain source type is MII)
# 
# It works as part of the following flow:
#
#   (1) When an operator discovers a new domain, it launches this script via an
#       introspector k8s job.
#   (2) This script then:
#       (2A) Generates MII domain home zips/md5 files if the domain home source type is MII
#       (2B) Configures and starts a NM via startNodeManager.sh (in NODEMGR_HOME)
#       (2C) Calls introspectDomain.py, which depends on the NM and puts files in stdout
#       (2D) Exits 0 on success, non-zero otherwise.
#   (5) Operator parses the output of introspectDomain.py into files and:
#       (5A) Uses one of the files to get the domain's name, cluster name, ports, etc.
#       (5B) Deploys a config map for the domain containing the files.
#   (6) Operator starts pods for domain's WebLogic servers.
#   (7) Pod 'startServer.sh' script loads files from the config map, 
#       generates a domain home from the files for the MII case,
#       copies/uses encrypted files, and applies sit config files. It
#       also checks that domain secret md5 cksum matches the cksum
#       obtained by this script.
#
# Prerequisites:
#
#    - Optionally set
#        ORACLE_HOME = Oracle Install Home - defaults via utils.sh/exportInstallHomes
#        MW_HOME     = MiddleWare Install Home - defaults to ${ORACLE_HOME}
#        WL_HOME     = WebLogic Install Home - defaults to ${ORACLE_HOME}/wlserver
#        INTROSPECTOR_LOG_FILE_MAX = Max number of log files to keep around (default 11).
#
#    - Transitively requires other env vars for startNodeManager.sh, wlst.sh, modelInImage.sh,
#      and introspectDomain.py (see these scripts to find out what else needs to be set).
#


SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"

#
# setup tracing
#

source ${SCRIPTPATH}/utils.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit 1

traceTiming "INTROSPECTOR '${DOMAIN_UID}' MAIN START"

# 
# if the auxiliary image feature is active, verify the mount, and log mount information
#
checkAuxiliaryImage || exit 1

#
# Local createFolder method which does an 'exit 1' instead of exitOrLoop for
# immediate failure during introspection
#

function createFolder {
  mkdir -m 750 -p $1
  if [ ! -d $1 ]; then
    trace SEVERE "Unable to create folder $1"
    exit 1
  fi
}

#
# setup MII functions in case this is a MII domain
#

source ${SCRIPTPATH}/modelInImage.sh
[ $? -ne 0 ] && trace SEVERE "Error sourcing ${SCRIPTPATH}/modelInImage.sh" && exit 1

#
# setup introspector log file
#   keep max 11 total by default (delete oldest first)
#

traceDirs before LOG_HOME

if [ ! -z "${LOG_HOME}" ] && [ ! -d "${LOG_HOME}" ]; then
  trace "Creating log home directory: '${LOG_HOME}'"
  createFolder ${LOG_HOME}
fi

ilog_dir="${LOG_HOME:-/tmp}"
ilog_file="${ilog_dir}/introspector_script.out"

if [ ! -d "${ilog_dir}" ]; then
  trace "Creating introspector log directory: '${ilog_dir}'"
  createFolder "${ilog_dir}"
fi

testLogFileRotate "${ilog_file}"
[ $? -ne 0 ] && trace SEVERE "Error accessing '${ilog_dir}'. See previous log messages." && exit 1

logFileRotate ${ilog_file} ${INTROSPECTOR_LOG_FILE_MAX:-11}

#
# main introspection function
#

function doIntrospect() {

  trace "Introspecting domain '${DOMAIN_UID}', log location: '$ilog_file'"

  traceDirs after LOG_HOME

  # list potentially interesting env-vars and dirs before they're updated by export.*Homes

  traceEnv before
  traceDirs before DOMAIN_HOME DATA_HOME

  # set defaults
  # set ORACLE_HOME/WL_HOME/MW_HOME to defaults if needed

  exportInstallHomes

  # check if prereq env-vars, files, and directories exist

  checkEnv -q \
         DOMAIN_UID \
         NAMESPACE \
         ORACLE_HOME \
         JAVA_HOME \
         NODEMGR_HOME \
         WL_HOME \
         MW_HOME \
         OPERATOR_ENVVAR_NAMES \
         || exit 1

  for script_file in "${SCRIPTPATH}/wlst.sh" \
                     "${SCRIPTPATH}/startNodeManager.sh"  \
                     "${SCRIPTPATH}/introspectDomain.py"; do
    [ ! -f "$script_file" ] && trace SEVERE "Missing file '${script_file}'." && exit 1 
  done 

  for dir_var in JAVA_HOME WL_HOME MW_HOME ORACLE_HOME; do
    [ ! -d "${!dir_var}" ] && trace SEVERE "Missing ${dir_var} directory '${!dir_var}'." && exit 1
  done

  #
  # DATA_HOME env variable exists implies override directory specified.  Attempt to create directory
  #
  if [ ! -z "${DATA_HOME}" ] && [ ! -d "${DATA_HOME}" ]; then
    trace "Creating data home directory: '${DATA_HOME}'"
    createFolder ${DATA_HOME}
  fi

  traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII CREATE DOMAIN START"

  # Add another env/attribute in domain yaml for model in image
  # log error if dir exists and attribute set
  DOMAIN_CREATED=0
  if [ ${DOMAIN_SOURCE_TYPE} == "FromModel" ]; then
    checkEnv WDT_MODEL_HOME WDT_INSTALL_HOME || exit 1
    trace "Beginning Model In Image"
    command -v gzip
    if [ $? -ne 0 ] ; then
      trace SEVERE "DomainSourceType is 'FromModel', 'gzip' is missing in the image. Please use an image with 'gzip' installed" && exit 1
    fi
    command -v tar
    if [ $? -ne 0 ] ; then
      trace SEVERE "DomainSourceType is 'FromModel', 'tar' is missing in the image. Please use an image with 'tar' installed" && exit 1
    fi
    command -v unzip
    if [ $? -ne 0 ] ; then
      trace SEVERE "DomainSourceType is 'FromModel', 'unzip' is missing in the image. Please use an image with 'unzip' installed" && exit 1
    fi
    mkdir -p ${DOMAIN_HOME}
    if [ $? -ne 0 ] ; then
      trace SEVERE "DomainSourceType is 'FromModel', cannot create domain home directory '${DOMAIN_HOME}'" && exit 1
    fi
    touch ${DOMAIN_HOME}/testaccess.tmp
    if [ $? -ne 0 ]; then
      trace SEVERE "DomainSourceType is 'FromModel', cannot write to domain home directory '${DOMAIN_HOME}'" && exit 1
    fi
    rm -f ${DOMAIN_HOME}/testaccess.tmp
    createWLDomain || exit 1
    created_domain=$DOMAIN_CREATED
    trace "Create domain return code = " ${created_domain}
  else
    created_domain=1
  fi

  traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII CREATE DOMAIN END" 

  # check DOMAIN_HOME for a config/config.xml, reset DOMAIN_HOME if needed

  exportEffectiveDomainHome || exit 1

  # list potentially interesting env-vars and dirs after they're updated by export.*Homes

  traceEnv after
  traceDirs after DOMAIN_HOME DATA_HOME

  # check if we're using a supported WebLogic version
  # (the check  will log a message if it fails)

  checkWebLogicVersion || exit 1

  # start node manager
  # run instrospector wlst script
  if [ ${created_domain} -ne 0 ]; then
    traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII NM START" 

    # introspectDomain.py uses an NM to setup credentials for the server NMs
    #  (see 'nmConnect' in introspectDomain.py)
    trace "Starting node manager"
    ${SCRIPTPATH}/startNodeManager.sh || exit 1

    traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII NM END" 
    traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII MD5 START"
    traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII NM END" 
    traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII MD5 START"

    # put domain secret's md5 cksum in file '/tmp/DomainSecret.md5'
    # the introspector wlst script and WL server pods will use this value
    generateDomainSecretMD5File '/tmp/DomainSecret.md5' || exit 1

    traceTiming "INTROSPECTOR '${DOMAIN_UID}' MII MD5 END"
    traceTiming "INTROSPECTOR '${DOMAIN_UID}' INTROSPECT START"

    trace "Running introspector WLST script ${SCRIPTPATH}/introspectDomain.py"
    ${SCRIPTPATH}/wlst.sh ${SCRIPTPATH}/introspectDomain.py || exit 1

    traceTiming "INTROSPECTOR '${DOMAIN_UID}' INTROSPECT END"
  fi
  trace "Domain introspection complete"
}

# we have different log file modes in case we need to revert 'tee' mode

case "${INTROSPECTOR_LOG_FILE_MODE:-tee}" in
  tee)
    set -o pipefail
    doIntrospect |& tee $ilog_file
    exit $?
    ;;
  bg_and_tail)
    ${SCRIPTPATH}/tailLog.sh $ilog_file /tmp/dontcare &
    tail_log_pid=$!
    doIntrospect >> $ilog_file 2>&1 &
    wait $!
    exitCode=$?
    # sleep 1 second in case background 'tail' needs time to catch up
    sleep 1
    kill -9 $tail_log_pid
    exit $exitCode
    ;;
  *)
    # no log file - everything simply goes to stdout/stderr (old behavior)
    doIntrospect
    exit $?
    ;;
esac

model-diff-v1.py:
----
# Copyright (c) 2019, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# ------------
# Description:
# ------------
#
#   This code compares python dictionaries.  It is used to compare the new vs the old version.
#   output is written as csv file /tmp/model_diff_rc  containing the return codes that represent the differences.
#   also the actual difference in model as yaml and json /tmp/diffed_model.json /tmp/diffed_model.yaml
#
#   This script is invoked by jython.  See modelInImage.sh diff_model
#
import re
import sets
import sys, os, traceback
from java.lang import System
UNSAFE_ONLINE_UPDATE=0
SAFE_ONLINE_UPDATE=1
FATAL_MODEL_CHANGES=2
MODELS_SAME=3
SECURITY_INFO_UPDATED=4
RCU_PASSWORD_CHANGED=5

# The following class is borrowed directly from the WDT project's yaml_tranlator.py
class PythonToYaml:
    """
    A class that converts a Python dictionary into Yaml and writes the output to a file.
    """
    # 4 spaces
    _indent_unit = '    '
    _requires_quotes_chars_regex = '[:{}\[\],&*#?|<>=!%@`-]'

    def __init__(self):
        return

    def _write_dictionary_to_yaml_file(self, dictionary, writer, indent=''):
        """
        Do the actual heavy lifting of converting a dictionary and writing it to the file.  This method is
        called recursively when a value of the dictionary entry is itself a dictionary.
        :param dictionary: the Python dictionary to convert
        :param writer: the java.io.PrintWriter for the output file
        :param indent: the amount of indent to use (based on the level of recursion)
        :raises: IOException: if an error occurs while writing the output
        """
        if dictionary is None:
            return

        for key, value in dictionary.iteritems():
            quoted_key = self._quotify_string(key)
            if isinstance(value, dict):
                writer.write(indent + quoted_key + ':' + '\n')
                self._write_dictionary_to_yaml_file(value, writer, indent + self._indent_unit)
            else:
                writer.write(indent + quoted_key + ': ' + self._get_value_string(value) + '\n')

        return

    def _get_value_string(self, value):
        """
        Convert the Python value into the proper Yaml value
        :param value: the Python value
        :return: the Yaml value
        """
        if value is None:
            result = 'null'
        elif type(value) is int or type(value) is long or type(value) is float:
            result = str(value)
        elif type(value) is list:
            new_value = '['
            for element in value:
                new_value += ' ' + self._get_value_string(element) + ','
            if len(new_value) > 1:
                new_value = new_value[:-1]
            new_value += ' ]'
            result = str(new_value)
        else:
            result = self._quotify_string(str(value))
        return result

    def _quotify_string(self, text):
        """
        Insert quotes around the string value if it contains Yaml special characters that require it.
        :param text: the input string
        :return: the quoted string, or the original string if no quoting was required
        """
        if bool(re.search(self._requires_quotes_chars_regex, text)):
            result = '\'' + self._quote_embedded_quotes(text) + '\''
        else:
            result = self._quote_embedded_quotes(text)
        return result

    def _quote_embedded_quotes(self, text):
        """
        Replace any embedded quotes with two quotes.
        :param text:  the text to quote
        :return:  the quoted text
        """
        result = text
        if '\'' in text:
            result = result.replace('\'', '\'\'')
        if '"' in text:
            result = result.replace('"', '""')
        return result


class ModelDiffer:

    def __init__(self, current_dict, past_dict):

        self.final_changed_model=dict()
        self.current_dict = current_dict
        self.past_dict = past_dict
        self.set_current = sets.Set()
        self.set_past = sets.Set()
        for item in self.current_dict.keys():
            self.set_current.add(item)
        for item in self.past_dict.keys():
            self.set_past.add(item)
        self.intersect = self.set_current.intersection(self.set_past)

    def added(self):
        return self.set_current - self.intersect

    def removed(self):
        return self.set_past - self.intersect

    def changed(self):
        result = sets.Set()
        for o in self.intersect:
            if self.past_dict[o] != self.current_dict[o]:
                result.add(o)
        return result

    def unchanged(self):
        result = sets.Set()
        for o in self.intersect:
            if self.past_dict[o] == self.current_dict[o]:
                result.add(o)
        return result

    def print_diff(self,s, category):
        print category
        if len(s) > 0:
            print s

    def recursive_changed_detail(self, key, token, root):
        debug("DEBUG: Entering recursive_changed_detail key=%s token=%s root=%s", key, token, root)
        a=ModelDiffer(self.current_dict[key], self.past_dict[key])
        diff=a.changed()
        added=a.added()
        removed=a.removed()
        saved_token=token
        debug('DEBUG: In recursive changed detail %s', diff)
        debug('DEBUG: In recursive added detail %s', added)
        if len(diff) > 0:
            for o in diff:
                token=saved_token
                # The token is a | separated string that is used to parse and rebuilt the structure later
                debug('DEBUG: in recursive changed detail walking down 1 %s', o)
                token=token+'|'+o
                if a.is_dict(o):
                    debug('DEBUG: in recursive changed detail walking down 2 %s', token)
                    a.recursive_changed_detail(o,token, root)
                    last=token.rfind('|')
                    token=root
                else:
                    all_changes.append(token)
                    last=token.rfind('|')
                    token=root


        # already out of recursive calls, add all entries from current dictionary
        # resources.JDBCSubsystemResources.* (note it may not have the lower level nodes
        added_token=token
        debug('DEBUG: current added token %s' , added_token)
        if len(added) > 0:
            for item in added:
                token=saved_token
                debug('DEBUG: recursive added token %s item %s ', token, item)
                all_added.append(token + '|' + item)

        # We don't really care about this, just put something here is enough

        if len(removed) > 0:
            for item in removed:
                debug('DEBUG: removed %s', item)
                all_removed.append(token + '|' + item)
        debug('DEBUG: Exiting recursive_changed_detail')

    def is_dict(self,key):
        if isinstance(self.current_dict[key],dict):
            return 1
        else:
            return 0

    def calculate_changed_model(self):
        """
        Calculate the changed model.
        """
        result = dict()
        changed=self.changed()

        for s in changed:
            token=s
            self.recursive_changed_detail(s, token, s)
            self._add_results(all_changes)
            self._add_results(all_added)
            # TODO:  delete needs more work, not simply added to the results
            #self._add_results(all_removed)


    def _add_results(self, ar_changes):

        # The ar_changes is the keys of changes in the dotted format
        #  'resources|JDBCSystemResource|Generic2|JdbcResource|JDBCConnectionPoolParams|TestConnectionsOnReserve
        #
        #  Now change it to python dictionrary
        for item in ar_changes:
            debug('DEBUG: add_results %s', item)

            splitted=item.split('|',1)
            n=len(splitted)
            result=dict()
            walked=[]

            while n > 1:
                tmp=dict()
                tmp[splitted[0]]=dict()
                if len(result) > 0:
                    # traverse to the leaf
                    leaf=result
                    for k in walked:
                        leaf = leaf[k]
                    leaf[splitted[0]]=dict()
                    walked.append(splitted[0])
                else:
                    result=tmp
                    walked.append(splitted[0])
                splitted=splitted[1].split('|',1)
                n=len(splitted)
            #
            # result is the dictionary format
            #
            leaf=result
            value_tree=self.current_dict
            for k in walked:
                leaf = leaf[k]
                value_tree=value_tree[k]

            # walk the current dictionary and set the value
            # doesn't work in delete case
            #
            leaf[splitted[0]] = value_tree[splitted[0]]
            self.merge_dictionaries(self.final_changed_model, result)


    def merge_dictionaries(self, dictionary, new_dictionary):
        """
         Merge the values from the new dictionary to the existing one.
        :param dictionary: the existing dictionary
        :param new_dictionary: the new dictionary to be merged
        """
        for key in new_dictionary:
            new_value = new_dictionary[key]
            if key not in dictionary:
                dictionary[key] = new_value
            else:
                value = dictionary[key]
                if isinstance(value, dict) and isinstance(new_value, dict):
                    self.merge_dictionaries(value, new_value)
                else:
                    dictionary[key] = new_value

    def is_safe_diff(self, model):
        """
        Is it a safe difference for update.
        :param model: diffed model
        return 0 - always return 0 for V1
        """

        # check for phase 1 any security changes in the domainInfo intersection

        if model.has_key('domainInfo'):
            domain_info = model['domainInfo']
            if domain_info.has_key('AdminUserName') or domain_info.has_key('AdminPassword') \
                    or domain_info.has_key('WLSRoles'):
                changed_items.append(SECURITY_INFO_UPDATED)

            if domain_info.has_key('RCUDbInfo'):
                rcu_db_info = domain_info['RCUDbInfo']
                if rcu_db_info.has_key('rcu_schema_password'):
                    changed_items.append(RCU_PASSWORD_CHANGED)

                if rcu_db_info.has_key('rcu_db_conn_string') \
                        or rcu_db_info.has_key('rcu_prefix'):
                    changed_items.append(SECURITY_INFO_UPDATED)

        if model.has_key('topology'):
            if model['topology'].has_key('Security') or model['topology'].has_key('SecurityConfiguration'):
                changed_items.append(SECURITY_INFO_UPDATED)

        return 0

    def _is_safe_addition(self, items):
        """
        check the items in all_added to see if can be used for online update
        return 0 false ;
            1 true ;
            2 for fatal
        """
        # allows add attribute to existing entity

        found_in_past_dictionary = 1
        has_topology=0
        for itm in items:
            if itm.find('topology.') == 0:
                has_topology = 1

            debug('DEBUG: is_safe_addition %s', itm)
            found_in_past_dictionary = self._in_model(self.past_dict, itm)
            debug('DBUEG: found_in_past_dictionary %s', found_in_past_dictionary)
            if not found_in_past_dictionary:
                break
            else:
                # check whether it is in the forbidden list
                if self.in_forbidden_list(itm):
                    print 'Found changes not supported for update: %s. Exiting' % (itm)
                    return FATAL_MODEL_CHANGES


        # if there is a shape change
        # return 2 ?
        if has_topology and not found_in_past_dictionary:
            print 'Found changes not supported for update: %s. Exiting' % (itm)
            return FATAL_MODEL_CHANGES

        if found_in_past_dictionary:
            return SAFE_ONLINE_UPDATE

        # allow new additions for anything ??
        return SAFE_ONLINE_UPDATE

    def _in_model(self, dictionary, keylist):
        """
        check whether the keys is in the dictionary
        :param dictionary dictonary to check
        :param keylist  dot separted key list
        return 1 if it is in model
               0 if it is not in model
        """
        debug('DBEUG: in model keylist=%s dictionary %s', keylist, dictionary)

        splitted=keylist.split('|')
        n=len(splitted)
        i=0
        root_key = splitted[0]

        # loop through the keys and use it to walk the dictionary
        # if it can walk down 3 levels, safely assume it is in the
        # dictionary, otherwise it is a total new addition

        for i in range(0, n):
            if dictionary.has_key(splitted[i]):
                if isinstance(dictionary[splitted[i]], dict):
                    dictionary = dictionary[splitted[i]]
                continue
            else:
                break

        if i > 2:
            return 1

        return 0

    def in_forbidden_list(self, itm):
        forbidden_list = [ '.ListenPort', '.ListenAddress' ]
        for forbidden in forbidden_list:
            if itm.endswith(forbidden):
                return 1
        return 0

    def get_final_changed_model(self):
        """
        Return the changed model.
        """
        return self.final_changed_model



class ModelFileDiffer:

    def __init__(self, current_dict, past_dict):

        self.current_dict_file = current_dict
        self.past_dict_file = past_dict

    def eval_file(self, file):
        true = True
        false = False
        fh = open(file, 'r')
        content = fh.read()
        return eval(content)


    def write_dictionary_to_json_file(self, dictionary, writer, indent=''):
        """
        Write the python dictionary in json syntax using the provided writer stream.
        :param dictionary: python dictionary to convert to json syntax
        :param writer: where to write the dictionary into json syntax
        :param indent: current string indention of the json syntax. If not provided, indent is an empty string
        """
        _start_dict = "{\n"
        _end_dict = "}\n"

        if dictionary is None:
            return
        end_line = ''
        writer.write(_start_dict)
        end_indent = indent

        indent += ' '
        for key, value in dictionary.iteritems():
            writer.write(end_line)
            end_line = ",\n"
            writer.write(indent + '"' + self.quote_embedded_quotes(key) + '" : ')
            if isinstance(value, dict):
                self.write_dictionary_to_json_file(value, writer, indent)
            else:
                writer.write(self.format_json_value(value))
        writer.write(str(end_indent + _end_dict))

        return

    def quote_embedded_quotes(self, text):
        """
        Quote all embedded double quotes in a string with a backslash.
        :param text: the text to quote
        :return: the quotes result
        """
        result = text
        if type(text) is str and '"' in text:
            result = text.replace('"', '\\"')
        return result

    def format_json_value(self, value):
        """
        Format the value as a JSON snippet.
        :param value: the value
        :return: the JSON snippet
        """
        import java.lang.StringBuilder as StringBuilder
        builder = StringBuilder()
        debug("DEBUG: value %s TYPE %s", value, type(value))
        if type(value) == bool or (type(value) == str and (value == 'true' or value == 'false')):
            if value:
                v = "true"
            else:
                v = "false"
            builder.append(v)
        elif type(value) == str:
            builder.append('"').append(self.quote_embedded_quotes(value)).append('"')
        elif type(value) == list:
            builder.append("[ ")
            ind = 0
            for list_item in value:
                if ind > 0:
                    builder.append(", ")
                builder.append('"').append(list_item).append('"')
                ind = ind+1

            builder.append(" ]")
        else:
            builder.append(value)
        return builder.toString()

    def compare(self):
        current_dict = self.eval_file(sys.argv[1])
        past_dict = self.eval_file(sys.argv[2])
        obj = ModelDiffer(current_dict, past_dict)
        obj.calculate_changed_model()
        net_diff = obj.get_final_changed_model()
        fh = open('/tmp/diffed_model.json', 'w')
        self.write_dictionary_to_json_file(net_diff, fh)
        #print all_added
        fh.close()
        fh = open('/tmp/diffed_model.yaml', 'w')
        pty = PythonToYaml()
        pty._write_dictionary_to_yaml_file(net_diff, fh)
        fh.close()
        return obj.is_safe_diff(net_diff)

def debug(format_string, *arguments):
    if os.environ.has_key('DEBUG_INTROSPECT_JOB'):
        print format_string % (arguments)
    return

def main():
    try:
        obj = ModelFileDiffer(sys.argv[1], sys.argv[2])
        rc=obj.compare()
        rcfh = open('/tmp/model_diff_rc', 'w')
        rcfh.write(",".join(map(str,changed_items)))
        rcfh.close()
        System.exit(0)
    except:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        eeString = traceback.format_exception(exc_type, exc_obj, exc_tb)
        print eeString
        System.exit(-1)
if __name__ == "__main__":
    all_changes = []
    all_added = []
    all_removed = []
    changed_items = []
    main()


model-wdt-create-filter.py:
----
# Copyright (c) 2018, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# ------------
# Description:
# ------------
#  This is a WDT filter for primordial domain creation. It filters out all resources and
#  apps deployments, leaving only the domainInfo and admin server in topology.
#
def filter_model(model):
  if model and 'topology' in model:
            topology = model['topology']
            if model['topology']['AdminServerName'] != None:
                admin_server = topology['AdminServerName']
                model['topology'] = {}
                model['topology']['AdminServerName'] = admin_server
                model['topology']['Server'] = {}
                model['topology']['Server'][admin_server] = topology['Server'][admin_server]   
            else:
                model['topology'] = {}

            if 'Name' in topology:
                model['topology']['Name'] = topology['Name']

            if 'Security' in topology:
                model['topology']['Security'] = topology['Security']


  if model and 'appDeployments' in model:
            model['appDeployments'] = {}

  if model and 'resources' in model:
            model['resources'] = {}


startServer.sh:
----
#!/bin/bash

# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# startServer.sh
# This is the script WebLogic Operator WLS Pods use to start their WL Server.
#

if [ -z ${SCRIPTPATH+x} ]; then
  SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
fi

echo "script path is ${SCRIPTPATH}"
source ${SCRIPTPATH}/utils.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exitOrLoop

traceTiming "POD '${SERVICE_NAME}' MAIN START"

trace "Starting WebLogic Server '${SERVER_NAME}'."

source ${SCRIPTPATH}/modelInImage.sh

if [ $? -ne 0 ]; then
      trace SEVERE "Error sourcing modelInImage.sh" && exit 1
fi

exportInstallHomes

#
# Define helper fn to copy a file only if src & tgt differ
#

function copyIfChanged() {
  [ ! -f "${1?}" ] && trace SEVERE "File '$1' not found." && exit 1
  if [ ! -f "${2?}" ] || [ ! -z "`diff $1 $2 2>&1`" ]; then
    trace "Copying '$1' to '$2'."
    cp $1 $2
    [ $? -ne 0 ] && trace SEVERE "failed cp $1 $2" && exitOrLoop
    if [ -O "$2" ]; then
      chmod 770 $2
      [ $? -ne 0 ] && trace SEVERE "failed chmod 770 $2" && exitOrLoop
    fi
  else
    trace "Skipping copy of '$1' to '$2' -- these files already match."
  fi
}

# 
# if the auxiliary image feature is active, verify the mount, and log mount information
#
checkAuxiliaryImage || exitOrLoop

#
# Define function to start WebLogic
#

function startWLS() {
  #
  # Start NM
  #

  traceTiming "POD '${SERVICE_NAME}' NM START"

  trace "Start node manager"
  # call script to start node manager in same shell
  # $SERVER_OUT_FILE, SERVER_PID_FILE, and SHUTDOWN_MARKER_FILE will be set in startNodeManager.sh
  . ${SCRIPTPATH}/startNodeManager.sh
  [ $? -ne 0 ] && trace SEVERE "failed to start node manager" && exitOrLoop

  traceTiming "POD '${SERVICE_NAME}' NM RUNNING"

  #
  # Verify that the domain secret hasn't changed
  #

  traceTiming "POD '${SERVICE_NAME}' MD5 BEGIN"

  checkDomainSecretMD5 || exitOrLoop

  traceTiming "POD '${SERVICE_NAME}' MD5 END"

  #
  # We "tail" the future WL Server .out file to stdout in background _before_ starting 
  # the WLS Server because we use WLST 'nmStart()' to start the server and nmStart doesn't return
  # control until WLS reaches the RUNNING state.
  #

  if [ "${SERVER_OUT_IN_POD_LOG}" == 'true' ] ; then
    trace "Showing the server out file from ${SERVER_OUT_FILE}"
    ${SCRIPTPATH}/tailLog.sh ${SERVER_OUT_FILE} ${SERVER_PID_FILE} &
  fi

  #
  # Start WL Server
  #

  # TBD We should probably || exitOrLoop if start-server.py itself fails, and dump NM log to stdout

  traceTiming "POD '${SERVICE_NAME}' WLS STARTING"

  trace "Start WebLogic Server via the nodemanager"
  ${SCRIPTPATH}/wlst.sh $SCRIPTPATH/start-server.py

  traceTiming "POD '${SERVICE_NAME}' WLS STARTED"

  FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR=${FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR:-true}
  SERVER_OUT_MONITOR_INTERVAL=${SERVER_OUT_MONITOR_INTERVAL:-3}
  if [ ${DOMAIN_SOURCE_TYPE} != "FromModel" ]; then
    # If Domain source type is not FromModel (MII) then monitor server logs for invalid
    # Situational config override files.
    if [ ${FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR} == 'true' ] ; then
      ${SCRIPTPATH}/monitorLog.sh ${SERVER_OUT_FILE} ${SERVER_OUT_MONITOR_INTERVAL} &
    fi
  fi
}

function mockWLS() {

  trace "Mocking WebLogic Server"

  STATEFILE_DIR=${DOMAIN_HOME}/servers/${SERVER_NAME}/data/nodemanager
  STATEFILE=${STATEFILE_DIR}/${SERVER_NAME}.state

  createFolder $STATEFILE_DIR
  echo "RUNNING:Y:N" > $STATEFILE
}

# Define helper fn to copy sit cfg xml files from one dir to another
#   $src_dir files are assumed to start with $fil_prefix and end with .xml
#   Copied $tgt_dir files are stripped of their $fil_prefix
#   Any .xml files in $tgt_dir that are not in $src_dir/$fil_prefix+FILE are deleted
#
# This method is called during boot, see 'copySitCfgWhileRunning' in 'livenessProbe.sh'
# for the similar method that is periodically called while the server is running.

function copySitCfgWhileBooting() {
  # Helper fn to copy sit cfg xml files to the WL server's domain home.
  #   - params $1/$2/$3 == 'src_dir tgt_dir fil_prefix'
  #   - $src_dir files are assumed to start with $fil_prefix and end with .xml
  #   - copied $tgt_dir files are stripped of their $fil_prefix
  #   - any .xml files in $tgt_dir that are not in $src_dir/$fil_prefix+FILE are deleted
  #
  # This method is called before the server boots, see
  # 'copySitCfgWhileRunning' in 'livenessProbe.sh' for a similar method that
  # is called periodically while the server is running. 

  src_dir=${1?}
  tgt_dir=${2?}
  fil_prefix=${3?}

  trace "Copying files starting with '$src_dir/$fil_prefix' to '$tgt_dir' without the prefix."

  createFolder $tgt_dir

  ls ${src_dir}/${fil_prefix}*.xml > /dev/null 2>&1
  if [ $? = 0 ]; then
    for local_fname in ${src_dir}/${fil_prefix}*.xml ; do
      copyIfChanged $local_fname $tgt_dir/`basename ${local_fname/${fil_prefix}//}`
      trace "Printing contents of situational configuration file $local_fname:"
      cat $local_fname
    done
  fi

  ls ${tgt_dir}/*.xml 2>&1 > /dev/null 2>&1
  if [ $? = 0 ]; then
    for local_fname in ${tgt_dir}/*.xml ; do
      if [ ! -f "$src_dir/${fil_prefix}`basename ${local_fname}`" ]; then
        trace "Deleting '$local_fname' since it has no corresponding '$src_dir' file."
        rm -f $local_fname
        [ $? -ne 0 ] && trace SEVERE "failed rm -f $local_fname" && exitOrLoop
      fi
    done
  fi
}


# trace env vars and dirs before export.*Home calls

traceEnv before
traceDirs before DOMAIN_HOME LOG_HOME DATA_HOME

traceTiming "POD '${SERVICE_NAME}' MII UNZIP START"

if [ ${DOMAIN_SOURCE_TYPE} == "FromModel" ]; then
  prepareMIIServer
  if [ $? -ne 0 ] ; then
    trace SEVERE  "Domain Source Type is FromModel, unable to start the server, check other error messages in the log"
    exitOrLoop
  fi

fi

traceTiming "POD '${SERVICE_NAME}' MII UNZIP COMPLETE"

#
# Configure startup mode
#

if [ ! -z "$STARTUP_MODE" ] && [[ $JAVA_OPTIONS != *"-Dweblogic.management.startupMode="* ]]; then
  export JAVA_OPTIONS="$JAVA_OPTIONS -Dweblogic.management.startupMode=$STARTUP_MODE"
fi

#
# Check input env vars
#

checkEnv -q \
  DOMAIN_UID \
  DOMAIN_NAME \
  DOMAIN_HOME \
  NODEMGR_HOME \
  ORACLE_HOME \
  SERVER_NAME \
  SERVICE_NAME \
  ADMIN_NAME \
  ADMIN_PORT \
  SERVER_OUT_IN_POD_LOG \
  AS_SERVICE_NAME || exitOrLoop

# If DATA_HOME env variable exists than this implies override directory (dataHome attribute of CRD) specified
# so we need to try and link the server's 'data' directory to the centralized DATA_HOME directory
if [ ! -z ${DATA_HOME} ]; then
  # Create $DATA_HOME directory for server if doesn't exist
  if [ ! -d ${DATA_HOME}/${SERVER_NAME}/data ]; then
    trace "Creating directory '${DATA_HOME}/${SERVER_NAME}/data'"
    createFolder ${DATA_HOME}/${SERVER_NAME}/data
  else
    trace "Directory '${DATA_HOME}/${SERVER_NAME}/data' exists"
  fi

  # The following is experimental code that handles the specific case of services that don't provide a configurable way to
  # control the location of their persistent file data.  For example, web applications can configure file-based
  # session persistence where the default persistent file store location is automatically created in the
  # <server-name>\data\store\default directory.
  # If 'EXPERIMENTAL_LINK_SERVER_DEFAULT_DATA_DIR' env is defined and 'KEEP_DEFAULT_DATA_HOME' environment variable is not defined then
  # try to link server's default 'data' directory (${DOMAIN_HOME}/servers/${SERVER_NAME}/data) to $DATA_HOME/${SERVER_NAME}/data.
  # If 'EXPERIMENTAL_LINK_SERVER_DEFAULT_DATA_DIR' env is defined and 'KEEP_DEFAULT_DATA_HOME' env variable is defined then
  # we will NOT link the server's 'data' directory to the centralized DATA_HOME directory and instead keep the server's
  # 'data' directory in its default location of ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
  if [ ! -z ${EXPERIMENTAL_LINK_SERVER_DEFAULT_DATA_DIR} ] && [ -z ${KEEP_DEFAULT_DATA_HOME} ]; then
    linkServerDefaultDir
  fi
fi

#
# check DOMAIN_HOME for a config/config.xml, reset DOMAIN_HOME if needed:
#

exportEffectiveDomainHome || exitOrLoop

# trace env vars and dirs after export.*Home calls

traceEnv after
traceDirs after DOMAIN_HOME LOG_HOME DATA_HOME

#
# Check if introspector actually ran.  This should never fail since
# the operator shouldn't try run a wl pod if the introspector failed.
#

bootpfile="/weblogic-operator/introspector/boot.properties"
if [ ! -f ${bootpfile} ]; then
  trace SEVERE "Missing introspector file '${bootpfile}'.  Introspector failed to run."
  exitOrLoop
fi

#
# Check if we're using a supported WebLogic version. The check  will
# log a message if it fails.
#

if ! checkWebLogicVersion ; then
  exitOrLoop
fi

#
# Copy/update domain introspector files for the domain
#
# - Copy introspector boot.properties to '${DOMAIN_HOME}/servers/${SERVER_NAME}/security
# 
# - Copy introspector situational config files to '${DOMAIN_HOME}/optconfig
#
# - Note:  We don't update a file when it's unchanged because a new timestamp might
#          trigger unnecessary situational config overhead.
#

createFolder ${DOMAIN_HOME}/servers/${SERVER_NAME}/security
copyIfChanged /weblogic-operator/introspector/boot.properties \
              ${DOMAIN_HOME}/servers/${SERVER_NAME}/security/boot.properties

# remove write and execute permissions for group to prevent insecure file system warnings.
chmod g-wx  ${DOMAIN_HOME}/servers/${SERVER_NAME}/security/boot.properties


if [ ${DOMAIN_SOURCE_TYPE} != "FromModel" ]; then
  trace "Copying situational configuration files from operator cm to ${DOMAIN_HOME}/optconfig directory"
  copySitCfgWhileBooting /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig             'Sit-Cfg-CFG--'
  copySitCfgWhileBooting /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig/jms         'Sit-Cfg-JMS--'
  copySitCfgWhileBooting /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig/jdbc        'Sit-Cfg-JDBC--'
  copySitCfgWhileBooting /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig/diagnostics 'Sit-Cfg-WLDF--'
fi

if [[ "${KUBERNETES_PLATFORM^^}" == "OPENSHIFT" ]]; then
    # When the Operator is running on Openshift platform, disable insecure file system warnings.
    export JAVA_OPTIONS="-Dweblogic.SecureMode.WarnOnInsecureFileSystem=false $JAVA_OPTIONS"
    if [[ "${DOMAIN_SOURCE_TYPE}" == "Image" ]]; then
      # Change the file permissions in the DOMAIN_HOME dir to give group same permissions as user .
      chmod -R g=u ${DOMAIN_HOME} || return 1
    fi

fi

#
# Start WLS
#

if [ "${MOCK_WLS}" == 'true' ]; then
  mockWLS
else
  startWLS
fi

#
# Wait forever. Kubernetes will monitor this pod via liveness and readyness probes.
#

waitForShutdownMarker

stopServer.sh:
----
#!/bin/bash

# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# This script is used to attempt to gracefully shutdown a WL Server
# before its pod is deleted.  It requires the SERVER_NAME env var.
#

SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
source ${SCRIPTPATH}/utils.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit 1

# setup ".out" location for a WL server
serverLogHome="${LOG_HOME:-${DOMAIN_HOME}/servers/${SERVER_NAME}/logs}"
STOP_OUT_FILE="${serverLogHome}/${SERVER_NAME}.stop.out"
SHUTDOWN_MARKER_FILE="${serverLogHome}/${SERVER_NAME}.shutdown"
SERVER_PID_FILE="${serverLogHome}/${SERVER_NAME}.pid"


trace "Stop server ${SERVER_NAME}" &>> ${STOP_OUT_FILE}

checkEnv SERVER_NAME || exit 1

if [ "${MOCK_WLS}" == 'true' ]; then
  touch ${SHUTDOWN_MARKER_FILE}
  exit 0
fi

function check_for_shutdown() {
  [ ! -f "${SCRIPTPATH}/readState.sh" ] && trace SEVERE "Missing file '${SCRIPTPATH}/readState.sh'." && exit 1

  state=`${SCRIPTPATH}/readState.sh`
  exit_status=$?
  if [ $exit_status -ne 0 ]; then
    trace "Node manager not running or server instance not found; assuming shutdown" &>> ${STOP_OUT_FILE}
    return 0
  fi

  if [ "$state" = "SHUTDOWN" ]; then
    trace "Server is shutdown" &>> ${STOP_OUT_FILE}
    return 0
  fi

  if [[ "$state" =~ ^FAILED ]]; then
    trace "Server in failed state" &>> ${STOP_OUT_FILE}
    return 0
  fi

  if [ -e /tmp/diefast ]; then
    trace "Found '/tmp/diefast' file; skipping clean shutdown" &>> ${STOP_OUT_FILE}

    # Adjust PATH if necessary before calling jps
    adjustPath

    kill -9 `jps -v | grep " NodeManager " | awk '{ print $1 }'`
    kill -9 `jps -v | grep " -Dweblogic.Name=${SERVER_NAME} " | awk '{ print $1 }'`
    touch ${SHUTDOWN_MARKER_FILE}
    return 0
  fi

  trace "Server is currently in state $state" &>> ${STOP_OUT_FILE}
  return 1
}


# Check if the server is already shutdown
check_for_shutdown
[ $? -eq 0 ] && trace "Server already shutdown or failed" &>>  ${STOP_OUT_FILE} && exit 0

# Otherwise, connect to the node manager and stop the server instance
[ ! -f "${SCRIPTPATH}/wlst.sh" ] && trace SEVERE "Missing file '${SCRIPTPATH}/wlst.sh'." && exit 1

# Arguments for shutdown
export SHUTDOWN_PORT_ARG=${LOCAL_ADMIN_PORT:-${MANAGED_SERVER_PORT:-8001}}
export SHUTDOWN_PROTOCOL_ARG=${LOCAL_ADMIN_PROTOCOL:-t3}
export SHUTDOWN_TIMEOUT_ARG=${SHUTDOWN_TIMEOUT:-30}
export SHUTDOWN_IGNORE_SESSIONS_ARG=${SHUTDOWN_IGNORE_SESSIONS:-false}
export SHUTDOWN_TYPE_ARG=${SHUTDOWN_TYPE:-Graceful}

trace "Before stop-server.py [${SERVER_NAME}] ${SCRIPTDIR}" &>> ${STOP_OUT_FILE}
${SCRIPTPATH}/wlst.sh /weblogic-operator/scripts/stop-server.py &>> ${STOP_OUT_FILE}
trace "After stop-server.py" &>> ${STOP_OUT_FILE}

# at this point node manager should have terminated the server
# but let's try looking for the server process and
# kill the server if the process still exists,
# just in case we failed to stop it via wlst

# Adjust PATH if necessary before calling jps
adjustPath

pid=$(jps -v | grep " -Dweblogic.Name=${SERVER_NAME} " | awk '{print $1}')
if [ ! -z $pid ]; then
  echo "Killing the server process $pid" &>> ${STOP_OUT_FILE}
  kill -15 $pid
fi

touch ${SHUTDOWN_MARKER_FILE}

if [ -f ${SERVER_PID_FILE} ]; then
  kill -2 $(< ${SERVER_PID_FILE} )
fi

trace "Exit script"  &>> ${STOP_OUT_FILE}

model-encryption-util.py:
----
# Copyright (c) 2019, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# ------------
# Description:
# ------------
#
#   This code uses WDT encryption utility class EncryptionUtils
#   for encrypting and decryption the domain secret SerializedSystemIni.dat
#
#   This script is invoked by jython.  See modelInImage.sh encrypt_decrypt_domain_secret
#   It's the user responsibility to save off the original file if needed
#

from oracle.weblogic.deploy.encrypt import EncryptionUtils
from oracle.weblogic.deploy.encrypt import EncryptionException
from java.lang import String
import sys, os, traceback
from java.lang import System
from utils import trace

def decrypt_file(cipher_text, password, outputfile):
      try:
        pwd = String(password)
        x = EncryptionUtils.decryptString(cipher_text, pwd.toCharArray())
        restored_text = String(x)
        fh = open(outputfile, "w")
        fh.write(str(restored_text))
        fh.close()
        System.exit(0)
      except EncryptionException, e:
          # Catch the exact exception to get the real cause
          trace("SEVERE", "Error in decrypting file: %s" % e.getCause())
          System.exit(-1)
      except:
          exc_type, exc_obj, exc_tb = sys.exc_info()
          eeString = traceback.format_exception(exc_type, exc_obj, exc_tb)
          trace("SEVERE", "Error in decrypting file: %s" % eeString)
          System.exit(-1)

def encrypt_file(clear_text, password, outputfile):
      try:
        pwd = String(password)
        x = EncryptionUtils.encryptString(clear_text, pwd.toCharArray())
        encrypted_text = String(x)
        fh = open(outputfile, "w")
        fh.write(str(encrypted_text))
        fh.close()
        System.exit(0)
      except EncryptionException, e:
          # Catch the exact exception to get the real cause
          trace("SEVERE", "Error in encrypting file: %s" % e.getCause())
          System.exit(-1)
      except:
          exc_type, exc_obj, exc_tb = sys.exc_info()
          eeString = traceback.format_exception(exc_type, exc_obj, exc_tb)
          trace("SEVERE", "Error in encrypting file: %s" % eeString)
          System.exit(-1)

if __name__ == "__main__":
    if sys.argv[1] == 'encrypt':
        encrypt_file(sys.argv[2], sys.argv[3], sys.argv[4])
    else:
        if sys.argv[1] == 'decrypt':
            decrypt_file(sys.argv[2], sys.argv[3], sys.argv[4])



modelInImage.sh:
----
#!/usr/bin/env bash
# Copyright (c) 2018, 2022, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# This script contains the all the function of model in image
# It is used by introspectDomain.sh job and startServer.sh

source ${SCRIPTPATH}/utils.sh

OPERATOR_ROOT=${TEST_OPERATOR_ROOT:-/weblogic-operator}
INTROSPECTCM_IMAGE_MD5="/weblogic-operator/introspectormii/inventory_image.md5"
INTROSPECTCM_CM_MD5="/weblogic-operator/introspectormii/inventory_cm.md5"
INTROSPECTCM_PASSPHRASE_MD5="/weblogic-operator/introspectormii/inventory_passphrase.md5"
INTROSPECTCM_MERGED_MODEL="/weblogic-operator/introspectormii/merged_model.json"
INTROSPECTCM_WLS_VERSION="/weblogic-operator/introspectormii/wls.version"
INTROSPECTCM_JDK_PATH="/weblogic-operator/introspectormii/jdk.path"
INTROSPECTCM_SECRETS_AND_ENV_MD5="/weblogic-operator/introspectormii/secrets_and_env.md5"
PRIMORDIAL_DOMAIN_ZIPPED="/weblogic-operator/introspectormii/primordial_domainzip.secure"
INTROSPECTJOB_IMAGE_MD5="/tmp/inventory_image.md5"
INTROSPECTJOB_CM_MD5="/tmp/inventory_cm.md5"
INTROSPECTJOB_PASSPHRASE_MD5="/tmp/inventory_passphrase.md5"
LOCAL_PRIM_DOMAIN_ZIP="/tmp/prim_domain.tar.gz"
LOCAL_PRIM_DOMAIN_TAR="/tmp/prim_domain.tar"
NEW_MERGED_MODEL="/tmp/new_merged_model.json"
WDT_CONFIGMAP_ROOT="/weblogic-operator/wdt-config-map"
RUNTIME_ENCRYPTION_SECRET_PASSWORD="/weblogic-operator/model-runtime-secret/password"

# we export the opss password file location because it's also used by introspectDomain.py
export OPSS_KEY_PASSPHRASE="/weblogic-operator/opss-walletkey-secret/walletPassword"
OPSS_KEY_B64EWALLET="/weblogic-operator/opss-walletfile-secret/walletFile"
IMG_MODELS_HOME="${WDT_MODEL_HOME:-/u01/wdt/models}"
IMG_MODELS_ROOTDIR="${IMG_MODELS_HOME}"
IMG_ARCHIVES_ROOTDIR="${IMG_MODELS_HOME}"
IMG_VARIABLE_FILES_ROOTDIR="${IMG_MODELS_HOME}"
WDT_ROOT="${WDT_INSTALL_HOME:-/u01/wdt/weblogic-deploy}"
WDT_OUTPUT_DIR="${LOG_HOME:-/tmp}"
WDT_OUTPUT="${WDT_OUTPUT_DIR}/wdt_output.log"
WDT_CREATE_DOMAIN_LOG=createDomain.log
WDT_UPDATE_DOMAIN_LOG=updateDomain.log
WDT_VALIDATE_MODEL_LOG=validateModel.log
WDT_COMPARE_MODEL_LOG=compareModel.log
WDT_BINDIR="${WDT_ROOT}/bin"
WDT_FILTER_JSON="/weblogic-operator/scripts/model-filters.json"
WDT_CREATE_FILTER="/weblogic-operator/scripts/model-wdt-create-filter.py"
WDT_MII_FILTER="/weblogic-operator/scripts/model_wdt_mii_filter.py"
UPDATE_RCUPWD_FLAG=""
WLSDEPLOY_PROPERTIES="${WLSDEPLOY_PROPERTIES} -Djava.security.egd=file:/dev/./urandom"
ARCHIVE_ZIP_CHANGED=0
WDT_ARTIFACTS_CHANGED=0
PROG_RESTART_REQUIRED=103
MII_UPDATE_NO_CHANGES_TO_APPLY=false
# return codes for model_diff
UNSAFE_ONLINE_UPDATE=0
SAFE_ONLINE_UPDATE=1
FATAL_MODEL_CHANGES=2
MERGED_MODEL_ENVVARS_SAME="false"
SECURITY_INFO_UPDATED=4
RCU_PASSWORD_CHANGED=5
NOT_FOR_ONLINE_UPDATE=6
SCRIPT_ERROR=255

WDT_ONLINE_MIN_VERSION="1.9.9"
WDT_OFFLINE_MIN_VERSION="1.7.3"

FATAL_JRF_INTROSPECTOR_ERROR_MSG="Model In Image JRF domain creation and schema initialization encountered an unrecoverable error.
 If it is a database credential related error such as wrong password, schema prefix, or database connect
 string, then correct the error and patch the domain resource 'domain.spec.introspectVersion' with a new
 value. If the error is not related to a database credential, then you must also drop and recreate the
 JRF schemas before patching the domain resource. Introspection Error: "

export WDT_MODEL_SECRETS_DIRS="/weblogic-operator/config-overrides-secrets"
[ ! -d ${WDT_MODEL_SECRETS_DIRS} ] && unset WDT_MODEL_SECRETS_DIRS

#TBD: CREDENTIALS_SECRET_NAME is unexpectedly empty. Maybe that's a regression?
#  export WDT_MODEL_SECRETS_NAME_DIR_PAIRS="__weblogic-credentials__=/weblogic-operator/secrets,__WEBLOGIC-CREDENTIALS__=/weblogic-operator/secrets,${CREDENTIALS_SECRET_NAME}=/weblogic-operator/secret"
#For now:
export WDT_MODEL_SECRETS_NAME_DIR_PAIRS="__weblogic-credentials__=/weblogic-operator/secrets,__WEBLOGIC-CREDENTIALS__=/weblogic-operator/secrets"

if [ ! -d "${WDT_OUTPUT_DIR}" ]; then
  trace "Creating WDT standard output directory: '${WDT_OUTPUT_DIR}'"
  createFolder "${WDT_OUTPUT_DIR}"
fi

# sort_files  sort the files according to the names and naming conventions and write the result to stdout
#    $1  directory
#    $2  extension
#

function sort_files() {
  shopt -s nullglob
  root_dir=$1
  ext=$2
  declare -A sequence_array
  for file in ${root_dir}/*${ext} ;
    do
      actual_filename=$(basename $file)
      base_filename=$(basename ${file%.*})
      sequence="${base_filename##*.}"
      sequence_array[${actual_filename}]=${sequence}
    done
  for k in "${!sequence_array[@]}" ;
    do
      # MUST use echo , caller depends on stdout
      echo $k ' - ' ${sequence_array["$k"]}
    done |
  sort -n -k3  | cut -d' ' -f 1
  shopt -u nullglob
}

#
# compareArtifactsMD5  checks the WDT artifacts MD5s in the introspect config map against the current introspect job
# WDT artifacts MD5s
#
# If there are any differences, set WDT_ARTIFACTS_CHANGED=1
# If there are any WDT archives changed set ARCHIVE_ZIP_CHANGED=1 (for online update)
#

function compareArtifactsMD5() {

  local has_md5=0

  trace "Entering checkExistInventory"

  trace "Checking wdt artifacts in image"
  if [ -f ${INTROSPECTCM_IMAGE_MD5} ] ; then
    has_md5=1
    # introspectorDomain py put two blank lines in the configmap, use -B to ignore blank lines
    diff -wB ${INTROSPECTCM_IMAGE_MD5} ${INTROSPECTJOB_IMAGE_MD5} > /tmp/imgmd5diff
    if [ $? -ne 0 ] ; then
      trace "WDT artifacts in image changed: create domain again"
      WDT_ARTIFACTS_CHANGED=1
    fi
  fi

  trace "Checking wdt artifacts in config map"
  if [ -f ${INTROSPECTCM_CM_MD5} ] ; then
    has_md5=1
    diff -wB  ${INTROSPECTCM_CM_MD5} ${INTROSPECTJOB_CM_MD5}
    if [ $? -ne 0 ] ; then
      trace "WDT artifacts in wdt config map changed: create domain again"
      WDT_ARTIFACTS_CHANGED=1
    fi
  else
    # if no config map before but adding one now
    if [ -f ${INTROSPECTJOB_CM_MD5} ]; then
      trace "New inventory in cm: create domain"
      WDT_ARTIFACTS_CHANGED=1
    fi
  fi

  if [ $has_md5 -eq 0 ]; then
    # Initial deployment
    trace "no md5 found: create domain"
    WDT_ARTIFACTS_CHANGED=1
  fi

  trace "Exiting checkExistInventory"
}

# get_opss_key_wallet   returns opss key wallet ewallet.p12 location
#
# if there is one from the user config map, use it first
# otherwise use the one in the introspect job config map
#

function get_opss_key_wallet() {
  if [ -f ${OPSS_KEY_B64EWALLET} ]; then
    echo ${OPSS_KEY_B64EWALLET}
  else
    echo "/weblogic-operator/introspectormii/ewallet.p12"
  fi
}

#
# buildWDTParams_MD5   Setup the WDT artifacts MD5 for comparison between updates
#  Also setup the wdt parameters
#

function buildWDTParams_MD5() {
  trace "Entering setupInventoryList"

  model_list=""
  archive_list=""
  variable_list="/u01/_k8s_generated_props.properties"

  #
  # First build the command line parameters for WDT
  # based on the file listing in the image or config map
  #

  for file in $(sort_files $IMG_MODELS_ROOTDIR ".yaml") ;
    do
      md5sum ${IMG_MODELS_ROOTDIR}/${file} >> ${INTROSPECTJOB_IMAGE_MD5}
      if [ "$model_list" != "" ]; then
        model_list="${model_list},"
      fi
      model_list="${model_list}${IMG_MODELS_ROOTDIR}/${file}"
    done

  for file in $(sort_files $WDT_CONFIGMAP_ROOT ".yaml") ;
    do
      md5sum ${WDT_CONFIGMAP_ROOT}/$file >> ${INTROSPECTJOB_CM_MD5}
      if [ "$model_list" != "" ]; then
        model_list="${model_list},"
      fi
      model_list="${model_list}${WDT_CONFIGMAP_ROOT}/${file}"
    done

  for file in $(sort_files ${IMG_ARCHIVES_ROOTDIR} "*.zip") ;
    do
      md5sum ${IMG_ARCHIVES_ROOTDIR}/$file >> ${INTROSPECTJOB_IMAGE_MD5}
      if [ "$archive_list" != "" ]; then
        archive_list="${archive_list},"
      fi
      archive_list="${archive_list}${IMG_ARCHIVES_ROOTDIR}/${file}"
    done

  # Merge all properties together
  local SPACE_BLANK_LINE=" "
  for file in $(sort_files ${IMG_VARIABLE_FILES_ROOTDIR} ".properties") ;
    do
      md5sum ${IMG_VARIABLE_FILES_ROOTDIR}/$file >> ${INTROSPECTJOB_IMAGE_MD5}
      cat ${IMG_VARIABLE_FILES_ROOTDIR}/${file} >> ${variable_list}
      # Make sure there is an extra line
      echo $SPACE_BLANK_LINE >> ${variable_list}
    done

  for file in $(sort_files ${WDT_CONFIGMAP_ROOT} ".properties") ;
    do
      md5sum  ${WDT_CONFIGMAP_ROOT}/$file >> ${INTROSPECTJOB_CM_MD5}
      echo $SPACE_BLANK_LINE >> ${variable_list}
      cat ${WDT_CONFIGMAP_ROOT}/${file} >> ${variable_list}
    done

  if [ -f ${variable_list} ]; then
    variable_list="-variable_file ${variable_list}"
  else
    variable_list=""
  fi

  if [ "$archive_list" != "" ]; then
    archive_list="-archive_file ${archive_list}"
  fi

  if [ "$model_list" != "" ]; then
    model_list="-model_file ${model_list}"
  fi

  if [ "${WDT_DOMAIN_TYPE}" == "JRF" ] && [ ! -f "${OPSS_KEY_PASSPHRASE}" ] ; then
    trace SEVERE "The domain resource 'spec.domainHomeSourceType'" \
       "is 'FromModel' and the 'spec.configuration.model.domainType' is 'JRF';" \
       "this combination requires specifying a" \
       "'spec.configuration.model.walletPasswordSecret' in your domain" \
       "resource and deploying this secret with a 'walletPassword' key," \
       "but the secret does not have this key."
    exitOrLoop
  fi

  #  We cannot strictly run create domain for JRF type because it's tied to a database schema
  #  We shouldn't require user to drop the db first since it may have data in it
  #
  opss_wallet=$(get_opss_key_wallet)
  if [ -f "${opss_wallet}" ] ; then
    trace "A wallet file was passed in using walletFileSecret, so we're using an existing rcu schema."
    mkdir -p /tmp/opsswallet
    base64 -d  ${opss_wallet} > /tmp/opsswallet/ewallet.p12
    OPSS_FLAGS="-opss_wallet /tmp/opsswallet"
  else
    OPSS_FLAGS=""
  fi

  if [ "true" == "${MII_USE_ONLINE_UPDATE}" ] ; then
    overrideWDTTimeoutValues
  fi

  trace "Exiting setupInventoryList"
}

function changeTimeoutProperty() {
  if [ ! -z $2 ] ; then
    sed -i "s/\($1=\).*\$/\1$2/" ${WDT_ROOT}/lib/tool.properties || exitOrLoop
  fi
}

function overrideWDTTimeoutValues() {
  start_trap
  trace "Entering overrideWDTTimeoutValues"
  # WDT defaults
  #
  #  connect.timeout=120000
  #  activate.timeout=180000
  #  deploy.timeout=180000
  #  redeploy.timeout=180000
  #  undeploy.timeout=180000
  #  start.application.timeout=180000
  #  stop.application.timeout=180000
  #  set.server.groups.timeout=30000

  changeTimeoutProperty "connect.timeout" ${WDT_CONNECT_TIMEOUT}
  changeTimeoutProperty "activate.timeout" ${WDT_ACTIVATE_TIMEOUT}
  changeTimeoutProperty "deploy.timeout" ${WDT_DEPLOY_TIMEOUT}
  changeTimeoutProperty "redeploy.timeout" ${WDT_REDEPLOY_TIMEOUT}
  changeTimeoutProperty "undeploy.timeout" ${WDT_UNDEPLOY_TIMEOUT}
  changeTimeoutProperty "start.application.timeout" ${WDT_START_APPLICATION_TIMEOUT}
  changeTimeoutProperty "stop.application.timeout" ${WDT_STOP_APPLICATION_TIMEOUT}
  changeTimeoutProperty "set.server.groups.timeout" ${WDT_SET_SERVER_GROUPS_TIMEOUT}

  trace "Exiting setupInventoryList"
  stop_trap
}

# createWLDomain
#

function createWLDomain() {
  start_trap
  trace "Entering createWLDomain"

  if [ ! -f ${RUNTIME_ENCRYPTION_SECRET_PASSWORD} ] ; then
    trace SEVERE "The domain resource 'spec.domainHomeSourceType'" \
       "is 'FromModel';" \
       "this requires specifying a" \
       "'spec.configuration.model.runtimeEncryptionSecret' in your domain" \
       "resource and deploying this secret with a 'password' key," \
       "but the secret does not have this key."
    exitOrLoop
  fi

  if [ ! -f "${WDT_ROOT}/lib/weblogic-deploy-core.jar" ]; then
    trace SEVERE "The domain resource 'spec.domainHomeSourceType'" \
         "is 'FromModel' " \
         "and a WebLogic Deploy Tool (WDT) install is not located at " \
         "'spec.configuration.model.wdtInstallHome' " \
         "which is currently set to '${WDT_ROOT}'. A WDT install " \
         "is normally created when you use the WebLogic Image Tool " \
         "to create an image for Model in Image."
     exitOrLoop
  fi

  # Check if modelHome (default /u01/wdt/models) and wdtInstallHome (default /u01/wdt/weblogic-deploy) exists
  checkDirNotExistsOrEmpty ${IMG_MODELS_HOME}
  checkDirNotExistsOrEmpty ${WDT_BINDIR}

  checkModelDirectoryExtensions
  if [ "true" != "${WDT_BYPASS_WDT_VERSION_CHECK}" ] ; then
    checkWDTVersion
  fi

  # copy the filter related files to the wdt lib
  cp ${WDT_FILTER_JSON} ${WDT_ROOT}/lib/model_filters.json || logSevereAndExit ${WDT_FILTER_JSON}
  cp ${WDT_CREATE_FILTER} ${WDT_ROOT}/lib || logSevereAndExit ${WDT_CREATE_FILTER}
  cp ${WDT_MII_FILTER} ${WDT_ROOT}/lib || logSevereAndExit ${WDT_MII_FILTER}

  # check to see if any model including changed (or first model in image deploy)
  # if yes. then run create domain again


  local current_version=$(getWebLogicVersion)
  local current_jdkpath=$(readlink -f $JAVA_HOME)
  # check for version:  can only be rolling

  local version_changed=0
  local jdk_changed=0
  SECRETS_AND_ENV_CHANGED=0
  trace "current version "${current_version}

  getSecretsAndEnvMD5
  local current_secrets_and_env_md5=$(cat /tmp/secrets_and_env.md5)

  trace "Checking changes in secrets and jdk path"

  if [ -f ${INTROSPECTCM_SECRETS_AND_ENV_MD5} ] ; then
    previous_secrets_and_env_md5=$(cat ${INTROSPECTCM_SECRETS_AND_ENV_MD5})
    if [ "${current_secrets_and_env_md5}" != "${previous_secrets_and_env_md5}" ]; then
      trace "Secrets and env different: old_md5=${previous_secrets_and_env_md5} new_md5=${current_secrets_and_env_md5}"
      SECRETS_AND_ENV_CHANGED=1
    fi
  fi

  if [ -f ${INTROSPECTCM_JDK_PATH} ] ; then
    previous_jdkpath=$(cat ${INTROSPECTCM_JDK_PATH})
    if [ "${current_jdkpath}" != "${previous_jdkpath}" ]; then
      trace "jdkpath different: before: ${previous_jdkpath} current: ${current_jdkpath}"
      jdk_changed=1
    fi
  fi

  # write out version, introspectDomain.py will write it to the configmap

  echo ${current_version} > /tmp/wls_version
  echo $(readlink -f $JAVA_HOME) > /tmp/jdk_path

  # setup wdt parameters and also associative array before calling comparing md5 in checkExistInventory
  #
  trace "Building WDT parameters and MD5s"

  buildWDTParams_MD5

  compareArtifactsMD5

  # Set this so that the introspectDomain.sh can decide to call the python script of not
  DOMAIN_CREATED=0

  # something changed in the wdt artifacts or wls version changed
  # create domain again

  if  [ ${WDT_ARTIFACTS_CHANGED} -ne 0 ] || [ ${jdk_changed} -eq 1 ] \
    || [ ${SECRETS_AND_ENV_CHANGED} -ne 0 ] ; then

    trace "Need to create domain ${WDT_DOMAIN_TYPE}"
    createModelDomain
    if [ "${MERGED_MODEL_ENVVARS_SAME}" == "false" ] ; then
      DOMAIN_CREATED=1
    fi
  else
    trace "Nothing changed no op"
  fi
  trace "Exiting createWLDomain"
  stop_trap
}

# checkDirNotExistsOrEmpty
#  Test directory exists or empty

function checkDirNotExistsOrEmpty() {
  trace "Entering checkDirNotExistsOrEmpty"

  if [ $# -eq 1 ] ; then
    if [ ! -d $1 ] ; then
      trace SEVERE "Directory '$1' does not exist"
      exitOrLoop
    else
      if [ -z "$(ls -A $1)" ] ; then
        trace SEVERE "Directory '$1' is empty"
        exitOrLoop
      fi
    fi
  fi

  trace "Exiting checkDirNotExistsOrEmpty"
}

# limit the file extensions in the model directories

function checkModelDirectoryExtensions() {
  trace "Entering checkModelDirectoryExtensions"

  cd ${IMG_MODELS_HOME}
  counter=$(ls  -I  "*.yaml" -I "*.zip" -I "*.properties" | wc -l)
  if [ $counter -ne 0 ] ; then
    trace SEVERE "Model image home '${IMG_MODELS_HOME}' contains files with unsupported extensions." \
      "Expected extensions: '.yaml', '.properties', or '.zip'." \
      "Files with unsupported extensions: '$(ls -I "*.yaml" -I "*.zip" -I "*.properties")'"
    exitOrLoop
  fi
  if [ -d ${WDT_CONFIGMAP_ROOT} ] ; then
    cd ${WDT_CONFIGMAP_ROOT}
    counter=$(ls  -I  "*.yaml" -I "*.properties" | wc -l)
    if [ $counter -ne 0 ] ; then
      trace SEVERE "Model 'spec.configuration.model.configMap' contains files with unsupported extensions." \
        "Expected extensions: '.yaml' or '.properties'." \
        "Files with unsupported extensions: '$(ls -I "*.yaml" -I "*.properties")'"
      exitOrLoop
    fi
  fi

  trace "Exiting checkModelDirectoryExtensions"
}

# Check for WDT version

function checkWDTVersion() {
  trace "Entering checkWDTVersion"
  unzip -c ${WDT_ROOT}/lib/weblogic-deploy-core.jar META-INF/MANIFEST.MF > /tmp/wdtversion.txt || exitOrLoop
  WDT_VERSION="$(grep "Implementation-Version" /tmp/wdtversion.txt | cut -f2 -d' ' | tr -d '\r' )" || exitOrLoop

  # trim out any non numeric character except dot and numbers, this avoid handling SNAPSHOT release
  WDT_VERSION=$(echo "${WDT_VERSION}" | tr -dc ^[.0-9]) || exitOrLoop

  if [ "true" = "${MII_USE_ONLINE_UPDATE}" ]; then
    local actual_min="$WDT_ONLINE_MIN_VERSION"
  else
    local actual_min="$WDT_OFFLINE_MIN_VERSION"
  fi

  if [ -z "${WDT_VERSION}" ] || ! versionGE "${WDT_VERSION}" "${actual_min}" ; then
    trace SEVERE "The domain resource 'spec.domainHomeSourceType' is 'FromModel'" \
      "and its image's WebLogic Deploy Tool installation has version '${WDT_VERSION:-unknown version}'" \
      "but introspection requires at least version '${WDT_ONLINE_MIN_VERSION}'" \
      "when 'spec.configuration.model.onlineUpdate.enabled' is set to 'true'" \
      "or at least version '${WDT_OFFLINE_MIN_VERSION}' otherwise." \
      "To fix this, create another image with an updated version of the WebLogic Deploy" \
      "Tool and redeploy the domain again."
    exitOrLoop
  fi

  trace "Exiting checkWDTVersion"
}

# getSecretsAndEnvMD5
#
# concatenate all the secrets and env, calculate the md5 and delete the file.
# The md5 is used to determine whether the domain needs to be recreated
# Note: the secrets are two levels indirections, so use find and filter out the ..data
# output:  /tmp/secrets_and_env.md5

function getSecretsAndEnvMD5() {
  trace "Entering getSecretsAndEnvMD5"

  local secrets_and_env_text="/tmp/secrets.txt"
  local override_secrets="/weblogic-operator/config-overrides-secrets/"
  local weblogic_secrets="/weblogic-operator/secrets/"
  local env_var

  rm -f ${secrets_and_env_text}

  for env_var in ${OPERATOR_ENVVAR_NAMES//,/ }; do
    echo "$env_var='${!env_var}'"
  done | sort >> ${secrets_and_env_text}

  if [ -d "${override_secrets}" ] ; then
    # find the link and exclude ..data so that the normalized file name will be found
    # otherwise it will return ../data/xxx ..etc. Note: the actual file is in a timestamp linked directory
    find ${override_secrets} -type l -not -name "..data" -print  | sort  | xargs cat >> ${secrets_and_env_text}
  fi

  if [ -d "${weblogic_secrets}" ] ; then
    find ${weblogic_secrets} -type l -not -name "..data" -print |  sort  | xargs cat >> ${secrets_and_env_text}
  fi

  if [ ! -f "${secrets_and_env_text}" ] ; then
    echo "0" > ${secrets_and_env_text}
  fi
  local secrets_and_env_md5=$(md5sum ${secrets_and_env_text} | cut -d' ' -f1)
  echo ${secrets_and_env_md5} > /tmp/secrets_and_env.md5
  trace "Found secrets and env: md5=${secrets_and_env_md5}"
  rm ${secrets_and_env_text}
  trace "Exiting getSecretsAndEnvMD5"
}


#
# createModelDomain call WDT to create the domain
#

function createModelDomain() {

  trace "Entering createModelDomain"
  createPrimordialDomain

  if [ "${MERGED_MODEL_ENVVARS_SAME}" == "false" ] ; then
    # if there is a new primordial domain created then use newly created primordial domain otherwise
    # if the primordial domain already in the configmap, restore it
    #

    if [ -f "${LOCAL_PRIM_DOMAIN_ZIP}" ] ; then
      trace "Using newly created domain"
    elif [ -f ${PRIMORDIAL_DOMAIN_ZIPPED} ] ; then
      trace "Using existing primordial domain"
      cd / && base64 -d ${PRIMORDIAL_DOMAIN_ZIPPED} > ${LOCAL_PRIM_DOMAIN_ZIP} && tar -pxzf ${LOCAL_PRIM_DOMAIN_ZIP}
      # create empty lib since we don't archive it in primordial zip and WDT will fail without it
      mkdir ${DOMAIN_HOME}/lib
      # Since the SerializedSystem ini is encrypted, restore it first
      local MII_PASSPHRASE=$(cat ${RUNTIME_ENCRYPTION_SECRET_PASSWORD})
      encrypt_decrypt_domain_secret "decrypt" ${DOMAIN_HOME} ${MII_PASSPHRASE}
    fi

    wdtUpdateModelDomain

    # This will be a no op if MII_USE_ONLINE_UPDATE is not defined or false
    wdtHandleOnlineUpdate

  fi

  trace "Exiting createModelDomain"
}


# Expands into the root directory the MII domain configuration, stored in one or more config maps
function restoreDomainConfig() {
  restoreEncodedTar "domainzip.secure" || return 1

  chmod u+x ${DOMAIN_HOME}/bin/*.sh ${DOMAIN_HOME}/*.sh  || return 1
}

# Expands into the root directory the MII primordial domain, stored in one or more config maps
function restorePrimordialDomain() {
  restoreEncodedTar "primordial_domainzip.secure" || return 1
}

# Restores the specified directory, targz'ed and stored in one or more config maps after base 64 encoding
# args:
# $1 the name of the encoded file in the config map
function restoreEncodedTar() {
  cd / || return 1
  cat $(ls ${OPERATOR_ROOT}/introspector*/${1} | sort -t- -k3) > /tmp/domain.secure || return 1
  base64 -d "/tmp/domain.secure" > /tmp/domain.tar.gz || return 1

  tar -pxzf /tmp/domain.tar.gz || return 1
}

# This is before WDT compareModel implementation
#
function diff_model_v1() {
  trace "Entering diff_model v1"

  #
  local ORACLE_SERVER_DIR=${ORACLE_HOME}/wlserver
  local JAVA_PROPS="-Dpython.cachedir.skip=true ${JAVA_PROPS}"
  local JAVA_PROPS="-Dpython.path=${ORACLE_SERVER_DIR}/common/wlst/modules/jython-modules.jar/Lib ${JAVA_PROPS}"
  local JAVA_PROPS="-Dpython.console= ${JAVA_PROPS} -Djava.security.egd=file:/dev/./urandom"
  local CP=${ORACLE_SERVER_DIR}/server/lib/weblogic.jar
  ${JAVA_HOME}/bin/java -cp ${CP} \
    ${JAVA_PROPS} \
    org.python.util.jython \
    ${SCRIPTPATH}/model-diff-v1.py $1 $2 > ${WDT_OUTPUT} 2>&1
  if [ $? -ne 0 ] ; then
    trace SEVERE "Failed to compare models. Check logs for error. Comparison output:"
    cat ${WDT_OUTPUT}
    exitOrLoop
  fi
  trace "Exiting diff_model v1"
}

# This is WDT compareModel.sh implementation

function diff_model() {
  trace "Entering diff_model"
  # wdt shell script or logFileRotate may return non-zero code if trap is on, then it will go to trap instead
  # temporarily disable it
  stop_trap

  export __WLSDEPLOY_STORE_MODEL__=1
  # $1 - new model, $2 original model

  ${WDT_BINDIR}/compareModel.sh -oracle_home ${ORACLE_HOME} -output_dir /tmp $1 $2 > ${WDT_OUTPUT} 2>&1
  ret=$?
  if [ $ret -ne 0 ]; then
    trace SEVERE "WDT Compare Model failed:"
    cat ${WDT_OUTPUT}
    exitOrLoop
  fi

  if [ "true" == "$MII_USE_ONLINE_UPDATE" ] ; then
    if [  ! -f "/tmp/diffed_model.yaml" ] ; then
      if [ -f "/tmp/compare_model_stdout" ] ; then
        trace SEVERE "WDT Compare Model detected 'There are no changes to apply between the old and new models'" \
          "and 'spec.configuration.model.onlineUpdate.enabled' is set to 'true'. This indicates" \
          "there may be incompatible changes for online update," \
          "such as trying to remove an existing attribute." \
          "Please correct the attributes listed below or use offline update:"
        cat /tmp/compare_model_stdout
        exitOrLoop
      else
        if [ ${SECRETS_AND_ENV_CHANGED} -eq 0 ] ; then
          # Merged model and env vars are identical, tell introspectDomain.sh not to run python and short circuit
          trace "Merged models and environment variables are identical, this introspection should be no-op."
          MERGED_MODEL_ENVVARS_SAME="true"
        fi
      fi
    fi
  fi

  if [ "${MERGED_MODEL_ENVVARS_SAME}" == "false" ] ; then
    # Generate diffed model update compatibility result
    local ORACLE_SERVER_DIR=${ORACLE_HOME}/wlserver
    local JAVA_PROPS="-Dpython.cachedir.skip=true ${JAVA_PROPS}"
    local JAVA_PROPS="-Dpython.path=${ORACLE_SERVER_DIR}/common/wlst/modules/jython-modules.jar/Lib ${JAVA_PROPS}"
    local JAVA_PROPS="-Dpython.console= ${JAVA_PROPS} -Djava.security.egd=file:/dev/./urandom"
    local CP=${ORACLE_SERVER_DIR}/server/lib/weblogic.jar
    ${JAVA_HOME}/bin/java -cp ${CP} \
      ${JAVA_PROPS} \
      org.python.util.jython \
      ${SCRIPTPATH}/model-diff.py $2 > ${WDT_OUTPUT} 2>&1
    if [ $? -ne 0 ] ; then
      trace SEVERE "Failed to compare models. Error output:"
      cat ${WDT_OUTPUT}
      exitOrLoop
    fi
  fi

  wdtRotateAndCopyLogFile "${WDT_COMPARE_MODEL_LOG}"

  # restore trap
  start_trap

  trace "Exiting diff_model"
}

#
# createPrimordialDomain will create the primordial domain
#

function createPrimordialDomain() {
  trace "Entering createPrimordialDomain"
  local create_primordial_tgz=0
  local recreate_domain=0
  if [  -f ${PRIMORDIAL_DOMAIN_ZIPPED} ] ; then
    # If there is an existing domain in the cm - this is update in the lifecycle
    # Call WDT validateModel.sh to generate the new merged mdoel
    trace "Checking if security info has been changed"

    generateMergedModel

    # decrypt the merged model from introspect cm
    local DECRYPTED_MERGED_MODEL="/tmp/decrypted_merged_model.json"
    local MII_PASSPHRASE=$(cat ${RUNTIME_ENCRYPTION_SECRET_PASSWORD})

    # Maintain backward compatibility - check first byte to see if it is a json file
    # if yes then it is the not a gzipped and encrypted model, just use it
    # else base64d to gzip file and unzip it
    encrypt_decrypt_model "decrypt" ${INTROSPECTCM_MERGED_MODEL}  ${MII_PASSPHRASE} \
      ${DECRYPTED_MERGED_MODEL}

    if [ "{" != $(head -c 1 ${DECRYPTED_MERGED_MODEL}) ] ; then
      base64 -d ${DECRYPTED_MERGED_MODEL} > ${DECRYPTED_MERGED_MODEL}.gz  || exitOrLoop
      rm ${DECRYPTED_MERGED_MODEL}  || exitOrLoop
      gunzip ${DECRYPTED_MERGED_MODEL}.gz  || exitOrLoop
    fi

    if  versionGE ${WDT_VERSION} ${WDT_ONLINE_MIN_VERSION} ; then
      diff_model ${NEW_MERGED_MODEL} ${DECRYPTED_MERGED_MODEL}
    else
      diff_model_v1 ${NEW_MERGED_MODEL} ${DECRYPTED_MERGED_MODEL}
    fi

    if [ "${MERGED_MODEL_ENVVARS_SAME}" == "false" ] ; then

      diff_rc=$(cat /tmp/model_diff_rc)
      rm ${DECRYPTED_MERGED_MODEL}
      trace "createPrimordialDomain: model diff return code list (can be empty): "${diff_rc}

      local security_info_updated="false"
      local cannot_perform_online_update="false"
      security_info_updated=$(contain_returncode ${diff_rc} ${SECURITY_INFO_UPDATED})
      cannot_perform_online_update=$(contain_returncode ${diff_rc} ${NOT_FOR_ONLINE_UPDATE})

      if [ ${cannot_perform_online_update} == "true" ] ; then
        trace SEVERE \
          "The Domain resource specified 'spec.configuration.model.onlineUpdate.enabled=true'," \
          "but there are unsupported model changes for online update. Examples of unsupported" \
          "changes include: changing ListenPort, ListenAddress, SSL, changing top level Topology attributes," \
          "or deleting a ServerTemplate."
        exitOrLoop
      fi

      # recreate the domain if there is an unsafe security update such as admin password update or security roles

      # Always use the schema password in RCUDbInfo.  Since once the password is updated by the DBA.  The
      # RCU cache table SCHEMA_COMPONENT_INFO stored password will never be correct,  and subsequently any
      # other updates such as admin credentials or security roles that caused the re-create of the primordial
      # domain will fail since without this flag set, defaults is to use the RCU cached info. (aka. wlst
      # getDatabaseDefaults).
      #
      if [ ${security_info_updated} == "true" ] ; then
        recreate_domain=1
        if [ ${WDT_DOMAIN_TYPE} == "JRF" ] ; then
          UPDATE_RCUPWD_FLAG="-updateRCUSchemaPassword"
        fi
      fi

      # if the domain is JRF and the schema password has been changed. Set this so that the changes are persisted
      # in the primordial domain.

      local rcu_password_updated="false"
      rcu_password_updated=$(contain_returncode ${diff_rc} ${RCU_PASSWORD_CHANGED})
      if [ ${WDT_DOMAIN_TYPE} == "JRF" ] && [ ${rcu_password_updated} == "true" ] ; then
          recreate_domain=1
          UPDATE_RCUPWD_FLAG="-updateRCUSchemaPassword"
      fi
    fi

  fi

  # If there is no primordial domain or needs to recreate one due to security changes

  if [ ! -f ${PRIMORDIAL_DOMAIN_ZIPPED} ] || [ ${recreate_domain} -eq 1 ]; then

    if [ "true" == "$MII_USE_ONLINE_UPDATE" ] \
       && [ "true" == "${security_info_updated}" ] \
       && [  ${recreate_domain} -eq 1 ] ; then
      trace SEVERE "There are unsupported security realm related changes to a Model In Image model and" \
        "'spec.configuration.model.onlineUpdate.enabled=true'; WDT currently does not" \
        "support online changes for most security realm related mbeans. Use offline update" \
        "to update the domain by setting 'domain.spec.configuration.model.onlineUpdate.enabled'" \
        "to 'false' and trying again."
      exitOrLoop
    fi

    trace "No primordial domain or need to create again because of changes require domain recreation"
    wdtCreatePrimordialDomain
    create_primordial_tgz=1
    MII_USE_ONLINE_UPDATE=false
  fi

  # tar up primordial domain with em.ear if it is there.  The zip will be added to the introspect config map by the
  # introspectDomain.py

  if [ ${create_primordial_tgz} -eq 1 ]; then
    empath=""
    if [ "${WDT_DOMAIN_TYPE}" != "WLS" ] ; then
      empath=$(grep "/em.ear" ${DOMAIN_HOME}/config/config.xml | grep -oPm1 "(?<=<source-path>)[^<]+")
    fi

    # Before targz it, we encrypt the SerializedSystemIni.dat, first save the original

    cp ${DOMAIN_HOME}/security/SerializedSystemIni.dat /tmp/sii.dat.saved

    local MII_PASSPHRASE=$(cat ${RUNTIME_ENCRYPTION_SECRET_PASSWORD})
    encrypt_decrypt_domain_secret "encrypt" ${DOMAIN_HOME} ${MII_PASSPHRASE}

    if [[ "${KUBERNETES_PLATFORM^^}" == "OPENSHIFT" ]]; then
      # Operator running on Openshift platform - change file permissions in the DOMAIN_HOME dir to give
      # group same permissions as user .
      chmod -R g=u ${DOMAIN_HOME} || return 1
    fi

    tar -pczf ${LOCAL_PRIM_DOMAIN_ZIP} --exclude ${DOMAIN_HOME}/wlsdeploy --exclude ${DOMAIN_HOME}/sysman/log  \
    --exclude ${DOMAIN_HOME}/lib --exclude ${DOMAIN_HOME}/backup_config ${empath} ${DOMAIN_HOME}/*

    # Put back the original one so that update can continue
    mv  /tmp/sii.dat.saved ${DOMAIN_HOME}/security/SerializedSystemIni.dat

  fi

  trace "Exiting createPrimordialDomain"

}

#
# Generate model from wdt artifacts
#
function generateMergedModel() {
  # wdt shell script may return non-zero code if trap is on, then it will go to trap instead
  # temporarily disable it
  trace "Entering generateMergedModel"
  stop_trap

  export __WLSDEPLOY_STORE_MODEL__="${NEW_MERGED_MODEL}"

  local wdtArgs=""
  wdtArgs+=" -oracle_home ${ORACLE_HOME}"
  wdtArgs+=" ${model_list} ${archive_list} ${variable_list}"
  wdtArgs+=" -domain_type ${WDT_DOMAIN_TYPE}"

  trace "About to call '${WDT_BINDIR}/validateModel.sh ${wdtArgs}'."

  ${WDT_BINDIR}/validateModel.sh ${wdtArgs} > ${WDT_OUTPUT} 2>&1
  ret=$?
  if [ $ret -ne 0 ]; then
    trace SEVERE "Model in Image: the WDT validate model tool detected an error with the fully merged model:"
    cat ${WDT_OUTPUT}
    exitOrLoop
  fi

  wdtRotateAndCopyLogFile "${WDT_VALIDATE_MODEL_LOG}"

  # restore trap
  start_trap
  trace "Exiting generateMergedModel"
}


# wdtCreatePrimordialDomain
# Create the actual primordial domain using WDT
#

function wdtCreatePrimordialDomain() {
  # wdt shell script may return non-zero code if trap is on, then it will go to trap instead
  # temporarily disable it
  trace "Entering wdtCreatePrimordialDomain"
  stop_trap

  export __WLSDEPLOY_STORE_MODEL__=1

  if [ "JRF" == "$WDT_DOMAIN_TYPE" ] ; then
    if [ -z "${OPSS_FLAGS}" ] ; then
      trace INFO "An OPSS wallet was not supplied for the Model in Image JRF domain in its " \
        "'spec.configuration.opss.walletFileSecret' attribute; therefore, it's assumed that this is the first time " \
        "the OPSS RCU database is being accessed by the domain, so a schema and a wallet file will be created. " \
        "Consult the Model in Image documentation for instructions about preserving the OPSS wallet file."
    else
      trace "Creating JRF Primordial Domain"
    fi
  fi

  local wdtArgs=""
  wdtArgs+=" -oracle_home ${ORACLE_HOME}"
  wdtArgs+=" -domain_home ${DOMAIN_HOME}"
  wdtArgs+=" ${model_list} ${archive_list} ${variable_list}"
  wdtArgs+=" -domain_type ${WDT_DOMAIN_TYPE}"
  wdtArgs+=" ${OPSS_FLAGS}"
  wdtArgs+=" ${UPDATE_RCUPWD_FLAG}"

  trace "About to call '${WDT_BINDIR}/createDomain.sh ${wdtArgs}'."

  if [ -z "${OPSS_FLAGS}" ]; then

    # We get here for WLS domains, and for the JRF 'first time' case

    # JRF wallet generation note:
    #  If this is JRF, the unset OPSS_FLAGS indicates no wallet file was specified
    #  via spec.configuration.opss.walletFileSecret and so we assume that this is
    #  the first time this domain started for this RCU database. We also assume
    #  that 'createDomain.sh' will perform the one time initialization of the
    #  empty RCU schema for the domain in the database (where the empty schema
    #  itself must be setup external to the Operator by calling 'create_rcu_schema.sh'
    #  or similar prior to deploying the domain for the first time).
    #
    #  The 'introspectDomain.py' script, which runs later, will create a wallet
    #  file using the spec.configuration.opss.walletPasswordSecret as its passphrase
    #  so that an administrator can then retrieve the file from the introspector's
    #  output configmap and save it for reuse.

    ${WDT_BINDIR}/createDomain.sh ${wdtArgs} > ${WDT_OUTPUT} 2>&1
  else
    # We get here only for JRF domain 'second time' (or more) case.

    # JRF wallet reuse note:
    #  The set OPSS_FLAGS indicates a wallet file was specified
    #  via spec.configuration.opss.walletFileSecret on the domain resource.
    #  So we assume that this domain already
    #  has its RCU tables and the wallet file will give us access to them.

    echo $(cat ${OPSS_KEY_PASSPHRASE}) | \
      ${WDT_BINDIR}/createDomain.sh ${wdtArgs} > ${WDT_OUTPUT} 2>&1

  fi

  ret=$?
  if [ $ret -ne 0 ]; then
    # Important:
    # The "FatalIntrospectorError" keyword is detected by DomainProcessorImpl.isShouldContinue
    # If it is detected then it will stop the periodic retry
    # We need to prevent retries with a "MII Fatal Error" because JRF without the OPSS_FLAGS indicates
    # a likely attempt to initialize the RCU DB schema for this domain, and we don't want to retry when this fails
    # without admin intervention (retrying can compound the problem and obscure the original issue).
    #
    if [ "JRF" == "$WDT_DOMAIN_TYPE" ] && [ -z "${OPSS_FLAGS}" ] ; then
      trace SEVERE "Model in Image: FatalIntrospectorError: WDT Create Domain Failed, return ${ret}. " \
        ${FATAL_JRF_INTROSPECTOR_ERROR_MSG}
    else
      trace SEVERE "Model in Image: WDT Create Primordial Domain Failed, ret=${ret}"
    fi
    cat ${WDT_OUTPUT}
    exitOrLoop
  else
    trace "WDT Create Domain Succeeded, ret=${ret}:"
    cat ${WDT_OUTPUT}
    if [ "JRF" == "$WDT_DOMAIN_TYPE" ]; then
      CREATED_JRF_PRIMODIAL="true"
    fi
  fi

  wdtRotateAndCopyLogFile "${WDT_CREATE_DOMAIN_LOG}"

  # restore trap
  start_trap
  trace "Exiting wdtCreatePrimordialDomain"

}

#
# wdtUpdateModelDomain  use WDT to update the model domain over the primordial domain
#

function wdtUpdateModelDomain() {

  trace "Entering wdtUpdateModelDomain"
  # wdt shell script may return non-zero code if trap is on, then it will go to trap instead
  # temporarily disable it

  stop_trap
  # make sure wdt create write out the merged model to a file in the root of the domain
  export __WLSDEPLOY_STORE_MODEL__=1

  local wdtArgs=""
  wdtArgs+=" -oracle_home ${ORACLE_HOME}"
  wdtArgs+=" -domain_home ${DOMAIN_HOME}"
  wdtArgs+=" ${model_list} ${archive_list} ${variable_list}"
  wdtArgs+=" -domain_type ${WDT_DOMAIN_TYPE}"
  wdtArgs+=" ${UPDATE_RCUPWD_FLAG}"

  trace "About to call '${WDT_BINDIR}/updateDomain.sh ${wdtArgs}'."

  ${WDT_BINDIR}/updateDomain.sh ${wdtArgs} > ${WDT_OUTPUT} 2>&1
  ret=$?

  if [ $ret -ne 0 ]; then
    if [ "true" == "${CREATED_JRF_PRIMODIAL}" ] ; then
      trace SEVERE "Model in Image: FatalIntrospectorError: WDT Update Domain Failed, return ${ret}. " \
        ${FATAL_JRF_INTROSPECTOR_ERROR_MSG}
    else
      trace SEVERE "WDT Update Domain command Failed:"
    fi
    cat ${WDT_OUTPUT}
    exitOrLoop
  fi

  # update the wallet
  if [ ! -z ${UPDATE_RCUPWD_FLAG} ]; then
    trace "Updating wallet because schema password changed"
    gunzip ${LOCAL_PRIM_DOMAIN_ZIP}
    if [ $? -ne 0 ] ; then
      trace SEVERE "WDT Update Domain failed: failed to upzip primordial domain"
      exitOrLoop
    fi
    tar uf ${LOCAL_PRIM_DOMAIN_TAR} ${DOMAIN_HOME}/config/fmwconfig/bootstrap/cwallet.sso
    if [ $? -ne 0 ] ; then
      trace SEVERE "WDT Update Domain failed: failed to tar update wallet file"
      exitOrLoop
    fi
    gzip ${LOCAL_PRIM_DOMAIN_TAR}
    if [ $? -ne 0 ] ; then
      trace SEVERE "WDT Update Domain failed: failed to zip up primordial domain"
      exitOrLoop
    fi
  fi

  # This is the complete model and used for life-cycle comparision, encrypt this before storing in
  # config map by the operator
  #
  local MII_PASSPHRASE=$(cat ${RUNTIME_ENCRYPTION_SECRET_PASSWORD})

  gzip ${DOMAIN_HOME}/wlsdeploy/domain_model.json || exitOrLoop
  base64 ${DOMAIN_HOME}/wlsdeploy/domain_model.json.gz > ${DOMAIN_HOME}/wlsdeploy/domain_model.json.b64 || exitOrLoop
  encrypt_decrypt_model "encrypt" ${DOMAIN_HOME}/wlsdeploy/domain_model.json.b64 ${MII_PASSPHRASE} \
    ${DOMAIN_HOME}/wlsdeploy/domain_model.json

  wdtRotateAndCopyLogFile "${WDT_UPDATE_DOMAIN_LOG}"

  # restore trap
  start_trap
  trace "Exiting wdtUpdateModelDomain"
}

function wdtHandleOnlineUpdate() {

  trace "Entering wdtHandleOnlineUpdate"
  # wdt shell script may return non-zero code if trap is on, then it will go to trap instead
  # temporarily disable it
  stop_trap
  if [ -z ${MII_USE_ONLINE_UPDATE} ] || [ "false" == "${MII_USE_ONLINE_UPDATE}" ] || [ ! -f /tmp/diffed_model.yaml ] ; then
      # no op for offline use case or no change in model with new image
      trace "Domain resource specified 'domain.spec.configuration.model.onlineUpdate=false' or not defined or no " \
        " merged model is the same, no need for online update."
      trace "Exiting wdtHandleOnlineUpdate"
      return
  fi

  # We need to extract all the archives, WDT online checks for file existence
  # even for delete
  #
  mkdir -p ${DOMAIN_HOME}/lib || exitOrLoop
  for file in $(sort_files ${IMG_ARCHIVES_ROOTDIR} "*.zip")
    do
        # expand the archive domain libraries to the domain lib, 11 is caution when zip entry doesn't exists
        cd ${DOMAIN_HOME}/lib || exitOrLoop
        unzip -jo ${IMG_ARCHIVES_ROOTDIR}/${file} wlsdeploy/domainLibraries/*
        if [ $? -ne 0 && $? -ne 11 ] ; then
          trace SEVERE  "Domain Source Type is FromModel, error in extracting domainLibraries " \
          "${IMG_ARCHIVES_ROOTDIR}/${file}"
          exitOrLoop
        fi

        # expand the domain bin, in update case user may only update a file in the domainBin archive, 11 is caution when
        # zip entry doesn't exists
        cd ${DOMAIN_HOME}/bin || exitOrLoop
        unzip -jo ${IMG_ARCHIVES_ROOTDIR}/${file} wlsdeploy/domainBin/*
        if [ $? -ne 0 && $? -ne 11 ] ; then
          trace SEVERE  "Domain Source Type is FromModel, error in extracting domainBin " \
          "${IMG_ARCHIVES_ROOTDIR}/${file}"
          exitOrLoop
        fi

        # expand the archive apps and shared lib to the wlsdeploy/* directories
        # the config.xml is referencing them from that path

        cd ${DOMAIN_HOME} || exitOrLoop
        ${JAVA_HOME}/bin/jar xf ${IMG_ARCHIVES_ROOTDIR}/${file} wlsdeploy/

        if [ $? -ne 0 ] ; then
          trace SEVERE "Error extracting application archive '${IMG_ARCHIVES_ROOTDIR}/${file}'"
          exitOrLoop
        fi


    done


  # Save off the encrypted model
  cp ${DOMAIN_HOME}/wlsdeploy/domain_model.json /tmp/encrypted_merge_model.json
  local admin_user=$(cat /weblogic-operator/secrets/username)
  local admin_pwd=$(cat /weblogic-operator/secrets/password)

  if [ -z ${AS_SERVICE_NAME} ] || [ -z ${ADMIN_PORT} ] ; then
    trace SEVERE "Cannot find admin service name or port"
    exitOrLoop
  fi

  local admin_url
  if [ -z "${ADMIN_PORT_SECURE}" ] ; then
    admin_url="t3://${AS_SERVICE_NAME}:${ADMIN_PORT}"
  else
    admin_url="t3s://${AS_SERVICE_NAME}:${ADMIN_PORT}"
  fi
  echo ${admin_pwd} | ${WDT_BINDIR}/updateDomain.sh -oracle_home ${MW_HOME} \
   -admin_url ${admin_url} -admin_user ${admin_user} -model_file \
   /tmp/diffed_model.yaml -domain_home ${DOMAIN_HOME} ${archive_list} \
   -discard_current_edit -output_dir /tmp  >  ${WDT_OUTPUT} 2>&1

  local ret=$?

  trace "Completed online update="${ret}
  if [ ${ret} -eq ${PROG_RESTART_REQUIRED} ] ; then
    write_updatedresult ${ret}
    write_non_dynamic_changes_text_file
  elif [ ${ret} -ne 0 ] ; then
    trace SEVERE "Online update failed" \
       "(the Domain resource specified 'spec.configuration.model.onlineUpdate.enabled=true')." \
       "Depending on the type of failure, the model may have an unsupported change," \
       "you may need to try an offline update, or you may need to shutdown the entire domain and then restart it."
    cat ${WDT_OUTPUT}
    write_updatedresult ${ret}
    exitOrLoop
  else
    write_updatedresult ${ret}
  fi

  # Restore encrypted merge model otherwise the on in the domain will be the diffed model

  cp  /tmp/encrypted_merge_model.json ${DOMAIN_HOME}/wlsdeploy/domain_model.json

  wdtRotateAndCopyLogFile ${WDT_UPDATE_DOMAIN_LOG} 

  trace "wrote updateResult"

  start_trap
  trace "Exiting wdtHandleOnlineUpdate"

}

function write_updatedresult() {
    # The >>> updatedomainResult is used in the operator code
    trace ">>>  updatedomainResult=${1}"
}

function write_non_dynamic_changes_text_file() {
    # Containing text regarding the non dynmaic mbean details
    if [ -f /tmp/non_dynamic_changes.file ] ; then
      echo ">>> /tmp/non_dynamic_changes.file"
      cat /tmp/non_dynamic_changes.file
      echo ">>> EOF"
    fi
}

function contain_returncode() {
  if echo ",$1," | grep -q ",$2,"
  then
    echo "true"
  else
    echo "false"
  fi
}

#
# Encrypt WDT model (Full encryption)
#
# parameter:
#   1 -  action (encrypt| decrypt)
#   2 -  input file
#   3 -  password
#   4 -  output file
#
function encrypt_decrypt_model() {
  trace "Entering encrypt_wdtmodel $1"

  local ORACLE_SERVER_DIR=${ORACLE_HOME}/wlserver
  local JAVA_PROPS="-Dpython.cachedir.skip=true ${JAVA_PROPS}"
  local JAVA_PROPS="-Dpython.path=${ORACLE_SERVER_DIR}/common/wlst/modules/jython-modules.jar/Lib ${JAVA_PROPS}"
  local JAVA_PROPS="-Dpython.console= ${JAVA_PROPS} -Djava.security.egd=file:/dev/./urandom"
  local CP=${ORACLE_SERVER_DIR}/server/lib/weblogic.jar:${WDT_BINDIR}/../lib/weblogic-deploy-core.jar
  ${JAVA_HOME}/bin/java -cp ${CP} \
    ${JAVA_PROPS} \
    org.python.util.jython \
    ${SCRIPTPATH}/model-encryption-util.py $1 "$(cat $2)" $3 $4 > ${WDT_OUTPUT} 2>&1
  rc=$?
  if [ $rc -ne 0 ]; then
    trace SEVERE "Failed to '$1' domain model. Check to see if the secret" \
    "referenced in the 'spec.configuration.model.runtimeEncryptionSecret' domain resource field" \
    "has been changed since the" \
    "creation of the domain. You can either reset the password to the original one and try again" \
    "or shutdown the entire domain and restart it." \
    "Failure output:"
    cat ${WDT_OUTPUT}
    exitOrLoop
  fi

  trace "Exiting encrypt_wdtmodel $1"
}

# encrypt_decrypt_domain_secret
# parameter:
#   1 - action (encrypt|decrypt)
#   2 -  domain home
#   3 -  password
#   4 -  output file

function encrypt_decrypt_domain_secret() {
  trace "Entering encrypt_decrypt_domain_secret $1"
  # Do not use trap for this startServer.sh fail for some not zero function call

  local tmp_output="/tmp/tmp_encrypt_decrypt_output.file"
  if [ "$1" == "encrypt" ] ; then
    base64 $2/security/SerializedSystemIni.dat > /tmp/secure.ini
  else
    cp $2/security/SerializedSystemIni.dat  /tmp/secure.ini
  fi

  #
  local ORACLE_SERVER_DIR=${ORACLE_HOME}/wlserver
  local JAVA_PROPS="-Dpython.cachedir.skip=true ${JAVA_PROPS}"
  local JAVA_PROPS="-Dpython.path=${ORACLE_SERVER_DIR}/common/wlst/modules/jython-modules.jar/Lib ${JAVA_PROPS}"
  local JAVA_PROPS="-Dpython.console= ${JAVA_PROPS} -Djava.security.egd=file:/dev/./urandom"
  local CP=${ORACLE_SERVER_DIR}/server/lib/weblogic.jar:${WDT_BINDIR}/../lib/weblogic-deploy-core.jar
  ${JAVA_HOME}/bin/java -cp ${CP} \
    ${JAVA_PROPS} \
    org.python.util.jython \
    ${SCRIPTPATH}/model-encryption-util.py $1 "$(cat /tmp/secure.ini)" $3 ${tmp_output} > ${WDT_OUTPUT} 2>&1
  rc=$?
  if [ $rc -ne 0 ]; then
    trace SEVERE "Failed to '$1' domain model. Check to see if the secret" \
    "referenced in the 'spec.configuration.model.runtimeEncryptionSecret' domain resource field" \
    "has been changed since the" \
    "creation of the domain. You can either reset the password to the original one and try again" \
    "or shutdown the entire domain and restart it." \
    "Failure output:"
    cat ${WDT_OUTPUT}
    exitOrLoop
  fi

  if [ "$1" == "decrypt" ] ; then
    base64 -d ${tmp_output} > $2/security/SerializedSystemIni.dat
  else
    cp ${tmp_output} $2/security/SerializedSystemIni.dat
  fi
  rm ${tmp_output}
  trace "Exiting encrypt_decrypt_domain_secret"
}

# prepare mii server

function prepareMIIServer() {

  trace "Model-in-Image: Creating domain home."
  local ret=0
  # primordial domain contain the basic structures, security and other fmwconfig templated info
  # domainzip only contains the domain configuration (config.xml jdbc/ jms/)
  # Both are needed for the complete domain reconstruction

  if [ ! -f /weblogic-operator/introspector/primordial_domainzip.secure ] ; then
    trace SEVERE "Domain Source Type is FromModel, the primordial model archive is missing, cannot start server"
    return 1
  fi

  if [ ! -f /weblogic-operator/introspector/domainzip.secure ] ; then
    trace SEVERE  "Domain type is FromModel, the domain configuration archive is missing, cannot start server"
    return 1
  fi

  trace "Model-in-Image: Restoring primordial domain"
  restorePrimordialDomain || return 1

  trace "Model-in-Image: Restore domain secret"
  # decrypt the SerializedSystemIni first
  if [ -f ${RUNTIME_ENCRYPTION_SECRET_PASSWORD} ] ; then
    MII_PASSPHRASE=$(cat ${RUNTIME_ENCRYPTION_SECRET_PASSWORD})
  else
    trace SEVERE "Domain Source Type is 'FromModel' which requires specifying a runtimeEncryptionSecret " \
    "in your domain resource and deploying this secret with a 'password' key, but the secret does not have this key."
    return 1
  fi
  encrypt_decrypt_domain_secret "decrypt" ${DOMAIN_HOME} ${MII_PASSPHRASE}

  # restore the config zip
  #
  trace "Model-in-Image: Restore domain config"
  restoreDomainConfig || return 1

  # restore the archive apps and libraries
  #
  trace "Model-in-Image: Restoring apps and libraries"

  mkdir -p ${DOMAIN_HOME}/lib
  if [ $? -ne 0 ] ; then
    trace  SEVERE "Domain Source Type is FromModel, cannot create ${DOMAIN_HOME}/lib "
    return 1
  fi
  local WLSDEPLOY_DOMAINLIB="wlsdeploy/domainLibraries"

  for file in $(sort_files ${IMG_ARCHIVES_ROOTDIR} "*.zip")
    do

        # expand the archive domain libraries to the domain lib, 11 is caution when zip entry doesn't exists
        cd ${DOMAIN_HOME}/lib || exitOrLoop
        unzip -jo ${IMG_ARCHIVES_ROOTDIR}/${file} wlsdeploy/domainLibraries/*
        ret=$?
        if [ $ret -ne 0 ] && [ $ret -ne 11 ] ; then
          trace SEVERE  "Domain Source Type is FromModel, error in extracting domainLibraries " \
          "${IMG_ARCHIVES_ROOTDIR}/${file}"
          exitOrLoop
        fi

        # expand the domain bin, in update case user may only update a file in the domainBin archive, 11 is caution when
        # zip entry doesn't exists
        cd ${DOMAIN_HOME}/bin || exitOrLoop
        unzip -jo ${IMG_ARCHIVES_ROOTDIR}/${file} wlsdeploy/domainBin/*
        ret=$?
        if [ $ret -ne 0 ] && [ $ret -ne 11 ] ; then
          trace SEVERE  "Domain Source Type is FromModel, error in extracting domainBin " \
          "${IMG_ARCHIVES_ROOTDIR}/${file}"
          exitOrLoop
        fi

        # expand the archive apps and shared lib to the wlsdeploy/* directories
        # the config.xml is referencing them from that path

        cd ${DOMAIN_HOME} || return 1
        ${JAVA_HOME}/bin/jar xf ${IMG_ARCHIVES_ROOTDIR}/${file} wlsdeploy/
        if [ $? -ne 0 ] ; then
          trace SEVERE "Domain Source Type is FromModel, error in extracting application archive ${IMG_ARCHIVES_ROOTDIR}/${file}"
          return 1
        fi
        # No need to have domainLibraries in domain home
        rm -fr ${WLSDEPLOY_DOMAINLIB}
    done
  return 0
}

#
# Generic error handler
#
function error_handler() {
    if [ $1 -ne 0 ]; then
        # Use FINE instead of SEVERE, avoid showing in domain status
        trace FINE  "Script Error: There was an error at line: ${2} command: ${@:3:20}"
        stop_trap
        exitOrLoop
    fi
}

function start_trap() {
    set -eE
    trap 'error_handler $? $BASH_LINENO $BASH_COMMAND ' ERR EXIT SIGHUP SIGINT SIGTERM SIGQUIT
}

function stop_trap() {
    trap -  ERR EXIT SIGHUP SIGINT SIGTERM SIGQUIT
    set +eE
}

function cleanup_mii() {
  rm -f /tmp/*.md5 /tmp/*.gz /tmp/*.ini /tmp/*.json
}

function logSevereAndExit() {
  trace SEVERE "cp '$1' failed"
  exitOrLoop
}

# Function to rotate WDT script log file and copy the file to WDT output dir.
# parameter:
#   1 - Name of the log file to rotate and copy to WDT output directory.
function wdtRotateAndCopyLogFile() {
  local logFileName=$1

  logFileRotate "${WDT_OUTPUT_DIR}/${logFileName}" "${WDT_LOG_FILE_MAX:-11}"

  cp ${WDT_ROOT}/logs/${logFileName} ${WDT_OUTPUT_DIR}/
}

model_wdt_mii_filter.py:
----
# Copyright (c) 2018, 2022, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# ------------
# Description:
# ------------
#   This is a model-in-image WDT filter for overriding WLS configuration, it
#   replaces 'situational configuration overrides'.
#
#   This code is used by the operator during introspection for MII to manipulate
#   the domain model.  It generates domain configuration information that's
#   useful for running the domain, setting up its networking, and for overriding
#   specific parts of its configuration so that it can run in k8s.
#
#   For more details, see the Model Filters description in the WebLogic Deploy
#   Tooling in Github.
#
# ---------------------
# Prerequisites/Inputs:
# ---------------------
#
#   A domain model as a Jython dictionary.
#
#   The following env vars are required:
#     DOMAIN_UID         - completely unique id for this domain
#     DOMAIN_HOME        - path for the domain configuration
#     LOG_HOME           - path to override WebLogic server log locations
#     CREDENTIALS_SECRET_NAME  - name of secret containing credentials
#
#   The following env vars are optional:
#     ACCESS_LOG_IN_LOG_HOME - HTTP access log files will be written to
#                              the logHome directory.
#     DATA_HOME - in-pod location for data storage of default and custom file
#                 stores.
#     CREDENTIALS_SECRET_PATH - directory path to secret containing credentials
#
# ---------------------------------
# Result
# ---------------------------------
#
#   The configuration overrides are directly modified in the domain model and
#   include listen addresses, log file locations, etc.  The WebLogic Deploy
#   Tooling will then generate/update the domain with the appropriate
#   configuration.
#

import copy
import inspect
import os
import sys

tmp_callerframerecord = inspect.stack()[0]    # 0 represents this line # 1 represents line at caller
tmp_info = inspect.getframeinfo(tmp_callerframerecord[0])
tmp_scriptdir=os.path.dirname(tmp_info[0])
sys.path.append(tmp_scriptdir)

env = None
ISTIO_NAP_NAMES = ['tcp-cbt', 'tcp-ldap', 'tcp-iiop', 'tcp-snmp', 'http-default', 'tcp-default', 'https-secure', 'tls-ldaps', 'tls-default', 'tls-cbts', 'tls-iiops', 'https-admin']


class OfflineWlstEnv(object):

  def open(self, model):

    self.model = model

    # before doing anything, get each env var and verify it exists

    self.DOMAIN_UID               = self.getEnv('DOMAIN_UID')
    self.DOMAIN_HOME              = self.getEnv('DOMAIN_HOME')
    self.LOG_HOME                 = self.getEnv('LOG_HOME')
    self.ACCESS_LOG_IN_LOG_HOME   = self.getEnvOrDef('ACCESS_LOG_IN_LOG_HOME', 'true')
    self.DATA_HOME                = self.getEnvOrDef('DATA_HOME', "")
    self.CREDENTIALS_SECRET_NAME  = self.getEnv('CREDENTIALS_SECRET_NAME')

    # initialize globals
    self.CREDENTIALS_SECRET_PATH = self.getEnvOrDef('CREDENTIALS_SECRET_PATH', '/weblogic-operator/secrets')
    self.TOPOLOGY_YAML_PATH = '/weblogic-operator/introspectormii/topology.yaml'
    self.DOMAIN_NAME = None

    if model and 'topology' in model:
      self.DOMAIN_NAME = model['topology']['Name']

    if self.DOMAIN_NAME is None and os.path.exists(self.TOPOLOGY_YAML_PATH):
      self.readDomainNameFromTopologyYaml(self.TOPOLOGY_YAML_PATH)

  def readDomainNameFromTopologyYaml(self, path):
    file = open(path, 'r')
    content = file.readlines()
    file.close()
    # access line containing domain name and strip leading and trailing spaces
    line = content[2].strip()
    # create key-value pair (e.g. name="sample-domain1")
    (key, val) = line.split()
    if key == 'name:':
      # strip leading and trailing double quotes from value
      self.DOMAIN_NAME = val.strip('"')

  def getDomainName(self):
    return self.DOMAIN_NAME

  def getDomainUID(self):
    return self.DOMAIN_UID

  def getDomainHome(self):
    return self.DOMAIN_HOME

  def getDomainLogHome(self):
    return self.LOG_HOME

  def getDataHome(self):
    return self.DATA_HOME

  def isAccessLogInLogHome(self):
    return self.ACCESS_LOG_IN_LOG_HOME == 'true'

  def readFile(self, path):
    file = open(path, 'r')
    contents = file.read()
    file.close()
    return contents

  def getEnv(self, name):
    val = os.getenv(name)
    if val is None or val == "null":
      print("SEVERE: Env var %s not set." % name)
      sys.exit(1)
    return val

  def getEnvOrDef(self, name, deflt):
    val = os.getenv(name)
    if val == None or val == "null" or len(val) == 0:
      return deflt
    return val

  def toDNS1123Legal(self, address):
    return address.lower().replace('_','-')

  def getModel(self):
    return self.model

  def wlsVersionEarlierThan(self, version):
    # unconventional import within function definition for unit testing
    from weblogic.management.configuration import LegalHelper
    # WLS Domain versions supported by operator are 12.2.1.3 + patches, 12.2.1.4
    # and 14.1.1.0 so current version will only be one of these that are listed.
    return LegalHelper.versionEarlierThan("14.1.1.0", version)

class SecretManager(object):

  def __init__(self, env):
    self.env = env

  def readCredentialsSecret(self, key):
    path = self.env.CREDENTIALS_SECRET_PATH + '/' + key
    return self.env.readFile(path)


def filter_model(model):
  if model is not None:
    if getOfflineWlstEnv() is None:
        initOfflineWlstEnv(model)

    initSecretManager(env)

    if model and 'resources' in model:
      customizeCustomFileStores(model)

    if model and 'topology' in model:
      topology = model['topology']
      customizeNodeManagerCreds(topology)
      customizeDomainLogPath(topology)

      if 'Cluster' in topology:
        # If Istio enabled, inject replication channel for each cluster
        # before creating the corresponding NAP for each server and
        # server-template
        customizeIstioClusters(model)

      if 'Server' in topology:
        customizeServers(model)

      if 'ServerTemplate' in topology:
        customizeServerTemplates(model)

def initOfflineWlstEnv(model):
  global env
  env = OfflineWlstEnv()
  env.open(model)


def getOfflineWlstEnv():
  if env is not None:
    return env
  return None


def setOfflineWlstEnv(offlineWlstEnv):
  env = offlineWlstEnv

def initSecretManager(env):
  global secret_manager
  secret_manager = SecretManager(env)

def customizeServerTemplates(model):
  topology = model['topology']
  if 'ServerTemplate' not in topology:
    return

  serverTemplates = topology['ServerTemplate']
  template_names = serverTemplates.keys()
  if template_names is not None:
    for template_name in template_names:
      template = serverTemplates[template_name]
      cluster_name = getClusterNameOrNone(template)
      if cluster_name is not None:
        customizeServerTemplate(topology, template, template_name)



def customizeServerTemplate(topology, template, template_name):
  server_name_prefix = getServerNamePrefix(topology, template)
  domain_uid = env.getDomainUID()
  customizeLog(server_name_prefix + "${id}", template)
  customizeAccessLog(server_name_prefix + "${id}", template)
  customizeDefaultFileStore(template)
  listen_address=env.toDNS1123Legal(domain_uid + "-" + server_name_prefix + "${id}")
  setServerListenAddress(template, listen_address)
  customizeNetworkAccessPoints(template, listen_address)
  customizeManagedIstioNetworkAccessPoint(template, listen_address)
  customizeIstioReplicationChannel(template, template_name, listen_address)
  if getCoherenceClusterSystemResourceOrNone(topology, template) is not None:
    customizeCoherenceMemberConfig(template, listen_address)

def customizeIstioClusters(model):
  istio_enabled = env.getEnvOrDef("ISTIO_ENABLED", "false")
  if istio_enabled == 'false':
    return
  if 'topology' in model and 'Cluster' in model['topology']:
    for cluster in model['topology']['Cluster']:
      if 'ReplicationChannel' not in model['topology']['Cluster'][cluster]:
        model['topology']['Cluster'][cluster]['ReplicationChannel'] = {}

      repl_channel = model['topology']['Cluster'][cluster]['ReplicationChannel']
      if repl_channel is None or len(repl_channel) == 0:
        model['topology']['Cluster'][cluster]['ReplicationChannel'] = 'istiorepl'

def getServerNamePrefix(topology, template):
  server_name_prefix = None
  cluster_name = getClusterNameOrNone(template)
  if cluster_name is not None:
    cluster = getClusterOrNone(topology, cluster_name)
    if cluster is not None:
      dynamicServer = getDynamicServerOrNone(cluster)
      if dynamicServer is not None:
        server_name_prefix = getDynamicServerPropertyOrNone(dynamicServer, 'ServerNamePrefix')

  return server_name_prefix


def customizeNodeManagerCreds(topology):
  username = getSecretManager().readCredentialsSecret('username')
  pwd = getSecretManager().readCredentialsSecret('password')

  if not ('SecurityConfiguration' in topology):
    topology['SecurityConfiguration'] = {}

  topology['SecurityConfiguration']['NodeManagerUsername'] = username
  topology['SecurityConfiguration']['NodeManagerPasswordEncrypted'] = pwd


def customizeDomainLogPath(topology):
  customizeLog(env.getDomainName(), topology)


def customizeLog(name, topologyOrServer):
  if name is None:
    # domain name is req'd to create domain log configuration.
    # Missing domain name indicates our model is a fragment
    return

  logs_dir = env.getDomainLogHome()
  if logs_dir is None or len(logs_dir) == 0:
    return

  if 'Log' not in topologyOrServer:
    topologyOrServer['Log'] = {}

  topologyOrServer['Log']['FileName'] = logs_dir + "/" + name + ".log"


def customizeCustomFileStores(model):
  customizeFileStores(model['resources'])


def customizeFileStores(resources):
  data_dir = env.getDataHome()
  if data_dir is None or len(data_dir) == 0:
    # do not override if dataHome not specified or empty ("")
    return

  if 'FileStore' not in resources:
    return

  filestores = resources['FileStore']
  names = filestores.keys()
  for name in names:
    filestore = filestores[name]
    customizeFileStore(filestore, data_dir)


def customizeFileStore(filestore, data_dir):
  filestore['Directory'] = data_dir


def customizeServers(model):
  if 'Server' not in model['topology']:
    return

  servers = model['topology']['Server']
  names = servers.keys()
  for name in names:
    server = servers[name]
    customizeServer(model, server, name)


def customizeServer(model, server, name):
  listen_address=env.toDNS1123Legal(env.getDomainUID() + "-" + name)

  adminServer = None
  if 'AdminServerName' in model['topology'] and len(model['topology']['AdminServerName']) > 0:
    adminServer = model['topology']['AdminServerName']

  customizeLog(name, server)
  customizeAccessLog(name, server)
  customizeDefaultFileStore(server)
  setServerListenAddress(server, listen_address)
  customizeNetworkAccessPoints(server,listen_address)
  customizeServerIstioNetworkAccessPoint(server, listen_address)
  # If the admin server name is not provided in the WDT model,
  # use the default name 'AdminServer'.
  if (name == adminServer or  name == 'AdminServer'):
    addAdminChannelPortForwardNetworkAccessPoints(server)
  else:
    customizeIstioReplicationChannel(server, name, listen_address)
  if getCoherenceClusterSystemResourceOrNone(model['topology'], server) is not None:
    customizeCoherenceMemberConfig(server, listen_address)


def customizeCoherenceMemberConfig(server, listen_address):
  if 'CoherenceMemberConfig ' not in server:
    server['CoherenceMemberConfig'] = {}

  cmc = server['CoherenceMemberConfig']
  cmc['UnicastListenAddress'] = listen_address


def getAdministrationPort(server, topology):
  port = 0
  if 'AdministrationPort' in server:
    port = int(server['AdministrationPort'])
  if port == 0 and 'AdministrationPort' in topology:
    port = int(topology['AdministrationPort'])
  if port == 0:
    port =9002
  return port


def isAdministrationPortEnabledForServer(server, topology):
  administrationPortEnabled = False
  if 'AdministrationPortEnabled' in server:
    administrationPortEnabled = server['AdministrationPortEnabled']
  else:
    administrationPortEnabled = isAdministrationPortEnabledForDomain(topology)
  return administrationPortEnabled


def isAdministrationPortEnabledForDomain(topology):
  administrationPortEnabled = False

  if 'AdministrationPortEnabled' in topology:
    administrationPortEnabled = topology['AdministrationPortEnabled']
  else:
    # AdministrationPortEnabled is not explicitly set so going with the default
    # Starting with 14.1.2.0, the domain's AdministrationPortEnabled default is derived from the domain's SecureMode
    administrationPortEnabled = isSecureModeEnabledForDomain(topology)
  return administrationPortEnabled


# Derive the default value for SecureMode of a domain
def isSecureModeEnabledForDomain(topology):
  secureModeEnabled = False
  if 'SecurityConfiguration' in topology and 'SecureMode' in topology['SecurityConfiguration'] and 'SecureModeEnabled' in topology['SecurityConfiguration']['SecureMode']:
    secureModeEnabled = topology['SecurityConfiguration']['SecureMode']['SecureModeEnabled']
  else:
    is_production_mode_enabled = False
    if 'ProductionModeEnabled' in topology:
      is_production_mode_enabled = topology['ProductionModeEnabled']
    secureModeEnabled = is_production_mode_enabled and not env.wlsVersionEarlierThan("14.1.2.0")
  return secureModeEnabled


def getSSLOrNone(server):
  if 'SSL' not in server:
    return None
  return server['SSL']


def _writeIstioNAP(name, server, listen_address, listen_port, protocol, http_enabled="true",
                   bind_to_localhost="true", use_fast_serialization='false', tunneling_enabled='false',
                   outbound_enabled='false'):

  if 'NetworkAccessPoint' not in server:
    server['NetworkAccessPoint'] = {}

  naps = server['NetworkAccessPoint']
  if name not in naps:
    naps[name] = {}

  nap = naps[name]
  nap['Protocol'] = protocol
  if bind_to_localhost == 'true':
    nap['ListenAddress'] = '127.0.0.1'
  else:
    nap['ListenAddress'] = '%s' % (listen_address)
  nap['PublicAddress'] = '%s.%s' % (listen_address, env.getEnvOrDef("ISTIO_POD_NAMESPACE", "default"))
  nap['ListenPort'] = listen_port
  nap['HttpEnabledForThisProtocol'] = http_enabled
  nap['TunnelingEnabled'] = tunneling_enabled
  nap['OutboundEnabled'] = outbound_enabled
  nap['Enabled'] = 'true'
  nap['UseFastSerialization'] = use_fast_serialization

def _get_ssl_listen_port(server):
  ssl = getSSLOrNone(server)
  ssl_listen_port = None
  model = env.getModel()
  if ssl is not None and 'Enabled' in ssl and ssl['Enabled']:
    ssl_listen_port = ssl['ListenPort']
    if ssl_listen_port is None:
      ssl_listen_port = "7002"
  elif ssl is None and isSecureModeEnabledForDomain(model['topology']):
    ssl_listen_port = "7002"
  return ssl_listen_port

def customizeServerIstioNetworkAccessPoint(server, listen_address):
  istio_enabled = env.getEnvOrDef("ISTIO_ENABLED", "false")
  if istio_enabled == 'false':
    return
  istio_readiness_port = env.getEnvOrDef("ISTIO_READINESS_PORT", None)
  if istio_readiness_port is None:
    return
  admin_server_port = server['ListenPort']
  # Set the default if it is not provided to avoid nap default to 0 which fails validation.

  if admin_server_port is None:
    admin_server_port = 7001

  # readiness probe
  _writeIstioNAP(name='http-probe', server=server, listen_address=listen_address,
                   listen_port=istio_readiness_port, protocol='http', http_enabled="true")

  # Generate NAP for each protocols
  if istioVersionRequiresLocalHostBindings():
    _writeIstioNAP(name='tcp-ldap', server=server, listen_address=listen_address,
                      listen_port=admin_server_port, protocol='ldap')

    _writeIstioNAP(name='tcp-default', server=server, listen_address=listen_address,
                      listen_port=admin_server_port, protocol='t3')

    _writeIstioNAP(name='http-default', server=server, listen_address=listen_address,
                      listen_port=admin_server_port, protocol='http')

    _writeIstioNAP(name='tcp-snmp', server=server, listen_address=listen_address,
                      listen_port=admin_server_port, protocol='snmp')

    _writeIstioNAP(name='tcp-cbt', server=server, listen_address=listen_address,
                      listen_port=admin_server_port, protocol='CLUSTER-BROADCAST')

    _writeIstioNAP(name='tcp-iiop', server=server, listen_address=listen_address,
                      listen_port=admin_server_port, protocol='iiop')

    ssl_listen_port = _get_ssl_listen_port(server)
    model = env.getModel()

    if ssl_listen_port is not None:
      _writeIstioNAP(name='https-secure', server=server, listen_address=listen_address,
                        listen_port=ssl_listen_port, protocol='https', http_enabled="true")

      _writeIstioNAP(name='tls-ldaps', server=server, listen_address=listen_address,
                        listen_port=ssl_listen_port, protocol='ldaps')

      _writeIstioNAP(name='tls-default', server=server, listen_address=listen_address,
                        listen_port=ssl_listen_port, protocol='t3s')

      _writeIstioNAP(name='tls-cbts', server=server, listen_address=listen_address,
                        listen_port=ssl_listen_port, protocol='CLUSTER-BROADCAST-SECURE')

      _writeIstioNAP(name='tls-iiops', server=server, listen_address=listen_address,
                        listen_port=ssl_listen_port, protocol='iiops')

    if isAdministrationPortEnabledForServer(server, model['topology']):
      _writeIstioNAP(name='https-admin', server=server, listen_address=listen_address,
                        listen_port=getAdministrationPort(server, model['topology']), protocol='https', http_enabled="true")
  else:
    # readiness probe NAP binding to server IP pod address Istio versions >= 1.10.x
    _writeIstioNAP(name='http-probe-ext', server=server, listen_address=listen_address,
                   listen_port=istio_readiness_port, protocol='http', http_enabled="true",
                   bind_to_localhost="false")

def customizeIstioReplicationChannel(server, name, listen_address):
  istio_enabled = env.getEnvOrDef("ISTIO_ENABLED", "false")
  if istio_enabled == 'false' or server['Cluster'] is None:
    return

  # verify if server or server template is associated with a cluster
  cluster_name = getClusterNameOrNone(server)
  if cluster_name is None or len(cluster_name) == 0:
    return

  # verify cluster is defined in the topology of the model
  model = env.getModel()
  if model is None or 'topology' not in model:
    return

  topology = model['topology']
  cluster = getClusterOrNone(topology, cluster_name)
  if cluster is None:
    return

  # verify if a replication channel is defined for the cluster
  if 'ReplicationChannel' not in cluster:
    return

  repl_channel = cluster['ReplicationChannel']
  if repl_channel is not None and repl_channel != 'istiorepl':
    return

  istio_repl_listen_port = env.getEnvOrDef("ISTIO_REPLICATION_PORT", 4564)

  verify_replication_port_conflict(server, name, istio_repl_listen_port)

  if istioVersionRequiresLocalHostBindings():
    _writeIstioNAP(name='istiorepl', server=server, listen_address=listen_address,
                 listen_port=istio_repl_listen_port, protocol='t3', http_enabled="true",
                 bind_to_localhost="true", use_fast_serialization='true',
                 tunneling_enabled='false')
  else:
    _writeIstioNAP(name='istiorepl', server=server, listen_address=listen_address,
                   listen_port=istio_repl_listen_port, protocol='t3', http_enabled="true",
                   bind_to_localhost="false", use_fast_serialization='true',
                   tunneling_enabled='false')

def raise_replication_port_conflict(name, listen_port, replication_port, SSL):
  raise ValueError('Server/ServerTemplate %s %s listen port %s conflicts with default replication channel port %s when '
                   'istio is enabled, please specify a different replication port for istio in '
                   'domain.spec.configuration.istio.replicationPort' % (name, SSL, listen_port, replication_port))

def verify_replication_port_conflict(server, name, replication_port):
  listen_port = server['ListenPort']
  ssl_listen_port = _get_ssl_listen_port(server)

  if listen_port == replication_port:
    raise_replication_port_conflict(name, listen_port, replication_port, '')

  if ssl_listen_port == replication_port:
    raise_replication_port_conflict(name, ssl_listen_port, replication_port, 'SSL')

  if 'NetworkAccessPoint' in server:
    for nap_name in server['NetworkAccessPoint']:
      nap = server['NetworkAccessPoint'][nap_name]
      listen_port = nap['ListenPort']
      ssl_listen_port = _get_ssl_listen_port(nap)

      if listen_port == replication_port:
        raise_replication_port_conflict(nap_name, listen_port, replication_port, '')

      if ssl_listen_port == replication_port:
        raise_replication_port_conflict(nap_name, ssl_listen_port, replication_port, 'SSL')

def customizeManagedIstioNetworkAccessPoint(template, listen_address):
  istio_enabled = env.getEnvOrDef("ISTIO_ENABLED", "false")
  if istio_enabled == 'false':
    return
  istio_readiness_port = env.getEnvOrDef("ISTIO_READINESS_PORT", None)
  if istio_readiness_port is None:
    return
  listen_port = template['ListenPort']
  # Set the default if it is not provided to avoid nap default to 0 which fails validation.
  if listen_port is None:
    listen_port = 7001

  # readiness probe
  _writeIstioNAP(name='http-probe', server=template, listen_address=listen_address,
                   listen_port=istio_readiness_port, protocol='http', http_enabled="true")

  # Generate NAP for each protocols
  if istioVersionRequiresLocalHostBindings():
    _writeIstioNAP(name='tcp-ldap', server=template, listen_address=listen_address,
                 listen_port=listen_port, protocol='ldap')

    _writeIstioNAP(name='tcp-default', server=template, listen_address=listen_address,
                 listen_port=listen_port, protocol='t3')

    _writeIstioNAP(name='http-default', server=template, listen_address=listen_address,
                 listen_port=listen_port, protocol='http')

    _writeIstioNAP(name='tcp-snmp', server=template, listen_address=listen_address,
                 listen_port=listen_port, protocol='snmp')

    _writeIstioNAP(name='tcp-cbt', server=template, listen_address=listen_address,
                 listen_port=listen_port, protocol='CLUSTER-BROADCAST')

    _writeIstioNAP(name='tcp-iiop', server=template, listen_address=listen_address,
                 listen_port=listen_port, protocol='iiop')

    ssl = getSSLOrNone(template)
    ssl_listen_port = None
    model = env.getModel()
    if ssl is not None and 'Enabled' in ssl and ssl['Enabled']:
      ssl_listen_port = ssl['ListenPort']
      if ssl_listen_port is None:
        ssl_listen_port = "7002"
    elif ssl is None and isSecureModeEnabledForDomain(model['topology']):
      ssl_listen_port = "7002"

    if ssl_listen_port is not None:
      _writeIstioNAP(name='https-secure', server=template, listen_address=listen_address,
                   listen_port=ssl_listen_port, protocol='https', http_enabled="true")

      _writeIstioNAP(name='tls-ldaps', server=template, listen_address=listen_address,
                   listen_port=ssl_listen_port, protocol='ldaps')

      _writeIstioNAP(name='tls-default', server=template, listen_address=listen_address,
                   listen_port=ssl_listen_port, protocol='t3s')

      _writeIstioNAP(name='tls-cbts', server=template, listen_address=listen_address,
                   listen_port=ssl_listen_port, protocol='CLUSTER-BROADCAST-SECURE')

      _writeIstioNAP(name='tls-iiops', server=template, listen_address=listen_address,
                   listen_port=ssl_listen_port, protocol='iiops')
  else:
    # readiness probe NAP binding to pod IP address for Istio versions >= 1.10.x
    _writeIstioNAP(name='http-probe-ext', server=template, listen_address=listen_address,
                   listen_port=istio_readiness_port, protocol='http', http_enabled="true",
                   bind_to_localhost="false")

def addAdminChannelPortForwardNetworkAccessPoints(server):
  istio_enabled = env.getEnvOrDef("ISTIO_ENABLED", "false")
  admin_channel_port_forwarding_enabled = env.getEnvOrDef("ADMIN_CHANNEL_PORT_FORWARDING_ENABLED", "true")
  if (admin_channel_port_forwarding_enabled == 'false') or \
      (istio_enabled == 'true' and istioVersionRequiresLocalHostBindings()):
    return

  admin_server_port = server['ListenPort']
  # Set the default if it is not provided to avoid nap default to 0 which fails validation.

  if admin_server_port is None:
    admin_server_port = 7001

  model = env.getModel()

  if 'NetworkAccessPoint' not in server:
    server['NetworkAccessPoint'] = {}

  naps = server['NetworkAccessPoint']
  nap_names = list(naps)
  index = 0
  for nap_name in nap_names:
    nap = naps[nap_name]
    if nap['Protocol'] == 'admin':
      index += 1
      customAdminChannelPort = nap['ListenPort']
      _writeAdminChannelPortForwardNAP(name='internal-admin' + str(index), server=server,
                                       listen_port=customAdminChannelPort, protocol='admin')

  if isAdministrationPortEnabledForServer(server, model['topology']):
    _writeAdminChannelPortForwardNAP(name='internal-admin', server=server,
                                     listen_port=getAdministrationPort(server, model['topology']), protocol='admin')
  elif index == 0:
    _writeAdminChannelPortForwardNAP(name='internal-t3', server=server, listen_port=admin_server_port, protocol='t3')

    ssl = getSSLOrNone(server)
    ssl_listen_port = None
    if ssl is not None and 'Enabled' in ssl and ssl['Enabled']:
      ssl_listen_port = ssl['ListenPort']
      if ssl_listen_port is None:
        ssl_listen_port = "7002"
    elif ssl is None and isSecureModeEnabledForDomain(model['topology']):
      ssl_listen_port = "7002"

    if ssl_listen_port is not None:
      _writeAdminChannelPortForwardNAP(name='internal-t3s', server=server, listen_port=ssl_listen_port, protocol='t3s')

def _writeAdminChannelPortForwardNAP(name, server, listen_port, protocol):

  if 'NetworkAccessPoint' not in server:
    server['NetworkAccessPoint'] = {}

  naps = server['NetworkAccessPoint']
  if name not in naps:
    naps[name] = {}

  nap = naps[name]
  nap['Protocol'] = protocol
  nap['ListenAddress'] = 'localhost'
  nap['ListenPort'] = listen_port
  nap['HttpEnabledForThisProtocol'] = 'true'
  nap['TunnelingEnabled'] = 'false'
  nap['Enabled'] = 'true'


def customizeNetworkAccessPoints(server, listen_address):
  if 'NetworkAccessPoint' not in server:
    return

  naps = server['NetworkAccessPoint']
  nap_names = naps.keys()
  for nap_name in nap_names:
    nap = naps[nap_name]
    customizeNetworkAccessPoint(nap_name, nap, listen_address)


def customizeNetworkAccessPoint(nap_name, nap, listen_address):
  if nap_name in ISTIO_NAP_NAMES:
    # skip creating ISTIO channels
    return

  istio_enabled = env.getEnvOrDef("ISTIO_ENABLED", "false")
  if istio_enabled == 'true' and istioVersionRequiresLocalHostBindings():
    listen_address = '127.0.0.1'

  # fix NAP listen address
  if 'ListenAddress' in nap:
    original_listen_address = nap['ListenAddress']
    if len(original_listen_address) > 0:
      nap['ListenAddress'] = listen_address

def setServerListenAddress(serverOrTemplate, listen_address):
  serverOrTemplate['ListenAddress'] = listen_address


def customizeDefaultFileStore(server):
  data_dir = env.getDataHome()
  if data_dir is None or len(data_dir) == 0:
    # do not override if dataHome not specified or empty ("")
    return

  if 'DefaultFileStore' not in server:
    server['DefaultFileStore'] = {}

  server['DefaultFileStore']['Directory'] = data_dir


def customizeAccessLog(name, server):
  # do not customize if LOG_HOME is not set
  logs_dir = env.getDomainLogHome()
  if logs_dir is None or len(logs_dir) == 0:
    return

  # customize only if ACCESS_LOG_IN_LOG_HOME is 'true'
  if env.isAccessLogInLogHome():
    if 'WebServer' not in server:
      server['WebServer'] = {}

    web_server = server['WebServer']
    if 'WebServerLog' not in web_server:
      web_server['WebServerLog'] = {}

    web_server_log = web_server['WebServerLog']
    if 'FileName' not in web_server_log:
      web_server_log['FileName'] = {}

    web_server_log['FileName'] = logs_dir + "/" + name + "_access.log"


def getLogOrNone(config):
  if 'Log' not in config:
    return None

  return config['Log']


def getClusterNameOrNone(serverOrTemplate):
  if 'Cluster' not in serverOrTemplate:
    return None

  return serverOrTemplate['Cluster']


def getClusterOrNone(topology, name):
  if 'Cluster' not in topology:
    return

  clusters = topology['Cluster']

  if name in clusters:
    return clusters[name]

  return None

def getCoherenceClusterSystemResourceOrNone(topology, serverOrTemplate):

  cluster_name = getClusterNameOrNone(serverOrTemplate)
  if cluster_name is not None:
    cluster = getClusterOrNone(topology, cluster_name)
    if cluster is not None:
      if 'CoherenceClusterSystemResource' not in cluster:
        return None
      return cluster['CoherenceClusterSystemResource']
    else:
      if 'CoherenceClusterSystemResource' not in serverOrTemplate:
        return None
  else:
    if 'CoherenceClusterSystemResource' not in serverOrTemplate:
      return None
    return serverOrTemplate['CoherenceClusterSystemResource']


def getDynamicServerOrNone(cluster):
  if 'DynamicServers' not in cluster:
    return None

  return cluster['DynamicServers']


def getDynamicServerPropertyOrNone(dynamicServer, name):
  if name not in dynamicServer:
    return None

  return dynamicServer[name]


def getSecretManager():
  return secret_manager

def istioVersionRequiresLocalHostBindings():
  if env.getEnvOrDef("ISTIO_USE_LOCALHOST_BINDINGS", "true") == 'true':
    return True

  return False
monitorLog.sh:
----
#!/bin/bash

# Copyright (c) 2019, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# This script is used to monitor the server log until the server is
# started successfully.
# It is separate from startServer.sh so that it is easier to quickly 
# kill the process running this script.
#

echo $$ > /tmp/monitorLog-pid

SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
source ${SCRIPTPATH}/utils.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit 1

trace "Monitoring server log file $1 every $2 seconds for selected known log messages."

while true; do
  if grep -q "BEA-141335" $1 ; then
    msg=("WebLogic server failed to start due to missing or invalid"
         "situational configuration files, which may be due to invalid"
         "configOverride templates (these are specified via the"
         "configOverride attribute in the Domain custom resource)."
         "For details, please search your pod log or"
         "${SERVER_OUT_FILE} for the keyword 'situational'."
        )
    trace SEVERE "${msg[*]}"
    exit 0
  fi
  if grep -q "BEA-000360" $1 ; then
    trace "WebLogic Server started successfully."
    exit 0
  fi
  sleep $2
done


utils.sh:
----
# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

set -o pipefail

SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
source ${SCRIPTPATH}/utils_base.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils_base.sh" && exit 1

#
# Purpose:
#   Defines various shared utility functions, including a trace function.
#
# Load this file via the following pattern:
#   SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
#   source ${SCRIPTPATH}/utils.sh
#   [ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit 1
#

# exportInstallHomes
#   purpose:  export MW_HOME, WL_HOME, ORACLE_HOME
#             with defaults as needed 
function exportInstallHomes() {
  export ORACLE_HOME=${ORACLE_HOME:-${MW_HOME}}

  if [ -z ${ORACLE_HOME} ]; then
    if [ -z ${WL_HOME} ]; then
      export ORACLE_HOME='/u01/oracle'
    else
      export ORACLE_HOME="`dirname ${WL_HOME}`"
    fi
  fi

  export WL_HOME=${WL_HOME:-${ORACLE_HOME}/wlserver}
  export MW_HOME=${MW_HOME:-${ORACLE_HOME}}
}

#
# tracen
#   purpose: same as "trace -n"
#
function tracen() {
  (
  set +x
  trace -cloc "`basename $0`:${BASH_LINENO[0]}" -n "$@"
  )
}

#
# tracePipe
#   purpose:  same as "trace -pipe"
#
function tracePipe() {
  (
  set +x
  trace -cloc "`basename $0`:${BASH_LINENO[0]}" -pipe "$@"
  )
}

# 
# traceTiming
#   purpose: specially decorated trace statements for timing measurements
#
function traceTiming() {
  [ "${TRACE_TIMING^^}" = "TRUE" ] && trace INFO "TIMING: ""$@"
}

#
# traceEnv:
#   purpose: trace a curated set of env vars
#   warning: we purposely avoid dumping all env vars
#            (K8S provides env vars with potentially sensitive network information)
#
function traceEnv() {
  local env_var
  trace FINE "Env vars ${*}:"
  for env_var in \
    DOMAIN_UID \
    NAMESPACE \
    SERVER_NAME \
    SERVICE_NAME \
    ADMIN_NAME \
    AS_SERVICE_NAME \
    ADMIN_PORT \
    ADMIN_PORT_SECURE \
    USER_MEM_ARGS \
    JAVA_OPTIONS \
    FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR \
    STARTUP_MODE \
    DOMAIN_HOME \
    LOG_HOME \
    SERVER_OUT_IN_POD_LOG \
    DATA_HOME \
    KEEP_DEFAULT_DATA_HOME \
    EXPERIMENTAL_LINK_SERVER_DEFAULT_DATA_DIR \
    JAVA_HOME \
    ORACLE_HOME \
    WL_HOME \
    MW_HOME \
    NODEMGR_HOME \
    INTROSPECT_HOME \
    PATH \
    TRACE_TIMING \
    OPERATOR_ENVVAR_NAMES
  do
    echo "    ${env_var}='${!env_var}'"
  done
}

#
# internal helper for logFileRotate():
#   return all files that match ${1}NNNNN in numeric order
#
function logFiles() {
  ls -1 ${1}[0-9][0-9][0-9][0-9][0-9] 2>/dev/null
}

#
# internal helper for logFileRotate():
#   return all files that match ${1}NNNNN in reverse order
#
function logFilesReverse() {
  ls -1r ${1}[0-9][0-9][0-9][0-9][0-9] 2>/dev/null
}

#
# internal helper for logFileRotate():
#   parse NNNNN out of $1, but if not found at end of $1 return 0
#
function logFileNum() {
  local logNum=$(echo "$1" | sed 's/.*\([0-9][0-9][0-9][0-9][0-9]\)$/\1/' | sed 's/^0*//')
  echo ${logNum:-0}
}

#
# internal helper for logFileRotate():
#   Rotate file from ${1} to ${1}NNNNN
#     $1 = filename
#     $2 = max files to keep
#     $3 = if "quiet", then suppress any tracing 
#   See logFileRotate() for detailed usage.A
#
function logFileRotateInner() {
  local logmax=${2:-7}
  local logcur

  [ $logmax -le 1 ] && logmax=1
  [ $logmax -gt 40000 ] && logmax=40000

  # find highest numbered log file (0 if none found)

  local lastlogfile=$(logFiles "$1" | tail -n 1)
  local lastlognum=$(logFileNum $lastlogfile)

  # delete oldest log files

  local _logmax_=$logmax
  if [ -f "$1" ]; then
    # account for the current file (the one we're about to rotate) if there is one
    _logmax_=$((logmax - 1))
  fi
  for logcur in $(logFilesReverse ${1} | tail -n +${_logmax_}); do
    if [ -f "$logcur" ] ; then
      [ ! "$3" = "quiet" ] && trace "Removing old log file '${logcur}'."
      rm $logcur
    fi
  done

  # if highest lognum is 99999, renumber existing files starting with 1
  # (there should be no overlap because we just deleted older files)

  if [ $lastlognum -ge 99999 ]; then
    lastlognum=0
    local logcur
    for logcur in $(logFiles "$1"); do
      lastlognum=$((lastlognum + 1))
      if [ -f "$logcur" ] ; then
        mv "$logcur" "${1}$(printf "%0.5i" $lastlognum)"
      fi
    done
  fi

  # rotate $1 if it exists, or simply remove it if logmax is 1

  if [ ${logmax} -gt 1 ]; then
    if [ -f "$1" ]; then
      local nextlognum=$((lastlognum + 1))
      [ ! "$3" = "quiet" ] && trace "Rotating '$1' to '${1}$(printf "%0.5i" $nextlognum)'."
      if [ -f "$1" ] ; then
        mv "$1" "${1}$(printf "%0.5i" $nextlognum)"
      fi
    fi
  else
    if [ -f "$1" ] ; then
     rm -f "$1"
    fi
  fi
}
#
# internal helper for logFileRotate():
#
function testLFRWarn() {
  trace WARNING "File rotation test failed. Log files named '${1}' will not be rotated, errcode='${2}'."
}

#
# internal helper for logFileRotate():
#   Convert new-lines to space, multi-spaces to single space, and trim
#
function testTR() {
  tr '\n' ' ' | sed 's/  */ /g' | sed 's/ $//g'
}

#
# internal helper for logFileRotate():
#   Verify  logFileRotateInner works, return non-zero if not.
#
function testLogFileRotate() {
  local curfile=${1:-/tmp/unknown}
  local fname=$(dirname $curfile)/testFileRotate.$RANDOM.$SECONDS.tmp
  mkdir -p $(dirname $curfile)

  rm -f ${fname}*
  logFileRotateInner ${fname} 7 quiet
  [ ! "$(ls ${fname}* 2>/dev/null)" = "" ]                  && testLFRWarn "$curfile" A1 && return 1
  echo "a" > ${fname} && logFileRotateInner ${fname} 2 quiet
  [ ! "$(ls ${fname}*)" = "${fname}00001" ]                 && testLFRWarn "$curfile" B1 && return 1
  [ ! "$(cat ${fname}00001)" = "a" ]                        && testLFRWarn "$curfile" B2 && return 1
  logFileRotateInner ${fname} 2 quiet
  [ ! "$(ls ${fname}*)" = "${fname}00001" ]                 && testLFRWarn "$curfile" C1 && return 1
  [ ! "$(cat ${fname}00001)" = "a" ]                        && testLFRWarn "$curfile" C2 && return 1
  echo "b" > ${fname} && logFileRotateInner ${fname} 2 quiet
  [ ! "$(ls ${fname}*)" = "${fname}00002" ]                 && testLFRWarn "$curfile" C3 && return 1
  [ ! "$(cat ${fname}00002)" = "b" ]                        && testLFRWarn "$curfile" C4 && return 1
  echo "c" > ${fname} && logFileRotateInner ${fname} 0 quiet
  [ ! "$(ls ${fname}* 2>/dev/null)" = "" ]                  && testLFRWarn "$curfile" D1 && return 1

  echo 1 > ${fname} && logFileRotateInner ${fname} 3 quiet
  [ ! "$(ls ${fname}* | testTR)" = "${fname}00001" ]               && testLFRWarn "$curfile" E1 && return 1
  echo 2 > ${fname} && logFileRotateInner ${fname} 3 quiet
  [ ! "$(ls ${fname}* | testTR)" = "${fname}00001 ${fname}00002" ] && testLFRWarn "$curfile" E2 && return 1
  echo 3 > ${fname} && logFileRotateInner ${fname} 3 quiet
  [ ! "$(ls ${fname}* | testTR)" = "${fname}00002 ${fname}00003" ] && testLFRWarn "$curfile" E3 && return 1
  echo 4 > ${fname} && logFileRotateInner ${fname} 3 quiet
  [ ! "$(ls ${fname}* | testTR)" = "${fname}00003 ${fname}00004" ] && testLFRWarn "$curfile" E4 && return 1
  [ ! "$(cat ${fname}00003)" = "3" ]                               && testLFRWarn "$curfile" E5 && return 1
  [ ! "$(cat ${fname}00004)" = "4" ]                               && testLFRWarn "$curfile" E6 && return 1
  local count
  rm ${fname}*
  echo "0" > ${fname}
  for count in 99997 99998 99999; do
    echo $count > ${fname}${count}
  done
  logFileRotateInner ${fname} 4 quiet
  [ ! "$(ls ${fname}* | testTR)" = "${fname}00001 ${fname}00002 ${fname}00003" ]  \
                                                            && testLFRWarn "$curfile" F1 && return 1
  [ ! "$(cat ${fname}00001)" = "99998" ]                    && testLFRWarn "$curfile" F2 && return 1
  [ ! "$(cat ${fname}00002)" = "99999" ]                    && testLFRWarn "$curfile" F3 && return 1
  [ ! "$(cat ${fname}00003)" = "0" ]                        && testLFRWarn "$curfile" F4 && return 1
  logFileRotateInner ${fname} 2 quiet
  [ ! "$(ls ${fname}*)" = "${fname}00003" ]                 && testLFRWarn "$curfile" F5 && return 1
  rm ${fname}*
  return 0
}

#
# logFileRotate
#   Rotate file from ${1} to ${1}NNNNN, starting with 00001.
#     $1 = filename
#     $2 = max log files, default is 7
#   Notes:
#     - $2 = 0 or 1 implies there should be no saved files
#            and causes $1 to be removed instead of rotated
#
#     - Silently tests rotation on scratch files first, and,
#       if that fails logs a WARNING, does nothing, and returns.
#
#     - If current max file is 99999, then old files are
#       renumbered starting with 00001.
#
function logFileRotate() {
  # test rotation, if it fails, log a Warning that rotation of $1 is skipped.
  testLogFileRotate "$1" || return 0
  # now do the actual rotation
  logFileRotateInner "$1" $2
}


#
# exportEffectiveDomainHome
#   if DOMAIN_HOME='${ORACLE_HOME}/user_projects/domains':
#     1) look for a config.xml in DOMAIN_HOME/config and
#        and in DOMAIN_HOME/*/config
#     2) Export DOMAIN_HOME to reflect the actual location 
#     3) Trace an Error and return non-zero if not found or more than 1 found
#
function exportEffectiveDomainHome() {
  local count=0
  local cur_domain_home=""
  local eff_domain_home=""
  local found_configs=""

  exportInstallHomes

  if [ ! "${DOMAIN_HOME?}" = "${ORACLE_HOME}/user_projects/domains" ]; then
    # nothing to do
    return 0
  fi

  local tfile=$(mktemp /tmp/homes.`basename $0`.XXXXXXXXX)
  rm -f $tfile
  ls -d ${DOMAIN_HOME} ${DOMAIN_HOME}/* > $tfile
  exec 22<> $tfile

  while read -u 22 cur_domain_home; do

    config_path="${cur_domain_home}/config/config.xml"

    if [ ! -f "${config_path}" ]; then
      continue
    fi

    count=$((count + 1))

    if [ $count -eq 1 ]; then
      eff_domain_home="${cur_domain_home}"
      found_configs="'${config_path}'"
    else
      found_configs="${found_configs}, '${config_path}'"
    fi

  done

  rm -f $tfile

  if [ $count -eq 1 ]; then
    export DOMAIN_HOME="${eff_domain_home}"
    return 0
  fi

  if [ $count -eq 0 ]; then
    trace SEVERE "No config.xml found at DOMAIN_HOME/config/config.xml or DOMAIN_HOME/*/config/config.xml, DOMAIN_HOME='$DOMAIN_HOME'. Check your 'domainHome' setting in your WebLogic Operator Domain resource, and your pv/pvc mount location (if any)."
    return 1
  fi
    
  # if we get this far, count is > 1 
  trace SEVERE "More than one config.xml found at DOMAIN_HOME/config/config.xml and DOMAIN_HOME/*/config/config.xml, DOMAIN_HOME='$DOMAIN_HOME': ${found_configs}. Configure your 'domainHome' setting in your WebLogic Operator Domain resource to reference a single WebLogic domain."
  return 1
}

# generateDomainSecretMD5
#  puts MD5 of domain secret in file $1
#  returns 1 on failure
#  called by introspector job
# 
generateDomainSecretMD5File()
{
  local dsf="$DOMAIN_HOME/security/SerializedSystemIni.dat"
  local md5f="$1"

  trace "Placing MD5 checksum of current domain secret 'DOMAIN_HOME/security/SerializedSystemIni.dat' in file '$1'."

  if [ ! -f "$dsf" ]; then
    trace SEVERE "Missing domain secret. Expected to find the domain secret in file 'DOMAIN_HOME/security/SerializedSystemIni.dat' where DOMAIN_HOME='$DOMAIN_HOME'. The operator requires an initialized DOMAIN_HOME to ensure that all jobs and pods in the domain share the same domain secret. You can use WLST or WDT to setup a DOMAIN_HOME."
    return 1
  fi

  md5sum "$dsf" > $1 || return 1
}

# checkDomainSecretMD5
#  verifies MD5 of domain secret matches MD5 found by introspector job
#  called by WL server pods
#
checkDomainSecretMD5()
{
  local orig_md5_file="/weblogic-operator/introspector/DomainSecret.md5"

  trace "Comparing domain secret MD5 generated by introspector '$orig_md5_file' with MD5 of current domain secret."

  if [ ! -f "$orig_md5_file" ]; then
    trace "Skipping domain secret MD5 check. File '$orig_md5_file' is missing. This means we're upgrading a live domain from an older version of the operator (which won't generate this file)."
    return 0
  fi

  local cur_md5_file="$(mktemp /tmp/CurrentDomainSecret.md5.`basename $0`.XXXXXXXXX)"

  generateDomainSecretMD5File "$cur_md5_file" || return 1

  diff -wB "$cur_md5_file" "$orig_md5_file" > ${cur_md5_file}.diff 2>&1

  if [ ! "$?" = "0" ]; then
    trace SEVERE "Domain secret mismatch. The domain secret in 'DOMAIN_HOME/security/SerializedSystemIni.dat' where DOMAIN_HOME='$DOMAIN_HOME' does not match the domain secret found by the introspector job. WebLogic requires that all WebLogic servers in the same domain share the same domain secret. See 'Domain Secret Mismatch' in the operator FAQ (https://oracle.github.io/weblogic-kubernetes-operator/faq/domain-secret-mismatch/). MD5 checksum diff:"

    # The operator will copy over the following to its log at the same time it copies the above SEVERE:

    cat ${cur_md5_file}.diff

    # sleep to give administrators time to capture the pod log before the pod exits
    sleep 60

    return 1
  fi

  rm -f "${cur_md5_file}"
  rm -f "${cur_md5_file}.diff"

  trace "Domain secret MD5 matches."
}

# versionCmp
#   Compares two wl versions $1 $2 up to the length of $2
#     Expects form N.N.N.N
#     Uses '0' for an N in $1 if $1 is shorter than $2
#   echo "1"  if v1 >  v2
#   echo "-1" if v1 <  v2
#   echo "0"  if v1 == v2
versionCmp()
{
  IFS='.' read -r -a v1_arr <<< "`echo $1`"
  IFS='.' read -r -a v2_arr <<< "`echo $2`"

  for i in "${!v2_arr[@]}"
  do
    [ ${v1_arr[i]:-0} -gt ${v2_arr[i]:-0} ] && echo "1" && return
    [ ${v1_arr[i]:-0} -lt ${v2_arr[i]:-0} ] && echo "-1" && return
  done
  echo "0"
}

# versionGE
#   return success if WL v1 >= v2
versionGE()
{
  [ `versionCmp "$1" "$2"` -ge 0 ] && return 0
  return 1
}

# versionEQ
#   return success if v1 == v2
versionEQ()
{
  [ `versionCmp "$1" "$2"` -eq 0 ] && return 0
  return 1
}

# hasWebLogicPatches
#   check for the given patch numbers in the install inventory, 
#   and return 1 if not found
#   - if we can't find the install inventory then we 
#     assume the patch is there...
#   - we parse the install inventory as this is far faster than
#     using opatch or weblogic.version
hasWebLogicPatches()
{
  local reg_file=$ORACLE_HOME/inventory/registry.xml
  [ ! -f $reg_file ] && return 0
  for pnum in "$@"; do
    grep --silent "patch-id=\"$1\"" $reg_file || return 1
  done
}

# getWebLogicVersion
#   parse wl version from install inventory
#   - if we can't get a version number then we return
#     a high dummy version number that's sufficient
#     to pass version checks "9999.9999.9999.9999"
#   - we parse the install inventory as this is far faster than
#     using opatch or weblogic.version
getWebLogicVersion()
{
  local reg_file=$ORACLE_HOME/inventory/registry.xml

  [ ! -f $reg_file ] && echo "9999.9999.9999.9999" && return

  # The following grep captures both "WebLogic Server" and "WebLogic Server for FMW"
  local wlver="`grep 'name="WebLogic Server.*version=' $reg_file \
               | sed 's/.*version="\([0-9.]*\)".*/\1/g'`"

  echo ${wlver:-"9999.9999.9999.9999"}
}


# checkWebLogicVersion
#   check if the WL version is supported by the Operator
#   - skip check if SKIP_WL_VERSION_CHECK = "true"
#   - log an error if WL version < 12.2.1.3
#   - log an error if WL version == 12.2.1.3 && patch 29135930 is missing
#     - you can override the required 12.2.1.3 patches by exporting
#       global WL12213REQUIREDPATCHES to an empty string or to other
#       patch number(s)
#   - return 1 if logged an error
#   - return 0 otherwise
checkWebLogicVersion()
{
  [ "$SKIP_WL_VERSION_CHECK" = "true" ] && return 0
  local cur_wl_ver="`getWebLogicVersion`"
  local exp_wl_ver="12.2.1.3" 
  local exp_wl_12213_patches="${WL12213REQUIREDPATCHES:-"29135930"}"
  if versionEQ "$cur_wl_ver" "12.2.1.3" ; then
    if ! hasWebLogicPatches $exp_wl_12213_patches ; then
      trace SEVERE "The Operator requires that WebLogic version '12.2.1.3' have patch '$exp_wl_12213_patches'. To bypass this check, set env var SKIP_WL_VERSION_CHECK to 'true'."
      return 1
    fi
  fi
  if versionEQ "$cur_wl_ver" "9999.9999.9999.9999" ; then
    trace INFO "Could not determine WebLogic version. Assuming version is fine. (The Operator requires WebLogic version '${exp_wl_ver}' or higher, and also requires patches '$exp_wl_12213_patches' for version '12.2.1.3'.)."
    return 0
  fi
  if versionGE "$cur_wl_ver" "${exp_wl_ver}" ; then
    trace INFO "WebLogic version='$cur_wl_ver'. Version check passed. (The Operator requires WebLogic version '${exp_wl_ver}' or higher)."
  else
    trace SEVERE "WebLogic version='$cur_wl_ver' and the Operator requires WebLogic version '${exp_wl_ver}' or higher. To bypass this check, set env var SKIP_WL_VERSION_CHECK to 'true'."
    return 1
  fi
  return 0
}


#
# getAdminUrl
#   purpose: Get the admin URL used to connect to the admin server internally, e.g. when starting a managed server
#   sample:
#     ADMIN_URL=$(getAdminServerUrl)
#
function getAdminServerUrl() {
  local admin_protocol="http"
  if [ "${ISTIO_ENABLED}" = "true" ]; then
    admin_protocol="t3"
  fi 

  if [ "${ADMIN_SERVER_PORT_SECURE}" = "true" ]; then
    if [ "${ISTIO_ENABLED}" = "true" ]; then
      admin_protocol="t3s"
    else
      admin_protocol="https"
    fi
  fi
  echo ${admin_protocol}://${AS_SERVICE_NAME}:${ADMIN_PORT}
}

function waitForShutdownMarker() {
  #
  # Wait forever.   Kubernetes will monitor this pod via liveness and readyness probes.
  #
  trace "Wait indefinitely so that the Kubernetes pod does not exit and try to restart"
  while true; do
    if [ -e ${SHUTDOWN_MARKER_FILE} ] ; then
      exit 0
    fi
    # Set the SERVER_SLEEP_INTERVAL_SECONDS environment variable in the domain specs in
    # `spec.serverPod.env` stanza to override the default sleep interval value of 1 second.
    sleep ${SERVER_SLEEP_INTERVAL_SECONDS:-1}
  done
}

#
# Define helper fn for failure debugging
#   If the livenessProbeSuccessOverride file is available, do not exit from startServer.sh.
#   This will cause the pod to stay up instead of restart.
#   (The liveness probe checks the same file.)
#

function exitOrLoop {
  if [ -f /weblogic-operator/debug/livenessProbeSuccessOverride ]
  then
    waitForShutdownMarker
  else
    exit 1
  fi
}

#
# Define helper fn to create a folder
#

function createFolder {
  mkdir -m 750 -p $1
  if [ ! -d $1 ]; then
    trace SEVERE "Unable to create folder $1"
    exitOrLoop
  fi
}

# Returns the count of the number of files in the specified directory
function countFilesInDir() {
  dir=${1}
  cnt=`find ${dir} -type f | wc -l`
  [ $? -ne 0 ] && trace SEVERE "failed determining number of files in '${dir}'" && exitOrLoop
  trace "file count in directory '${dir}': ${cnt}"
  return ${cnt}
}

# Creates symbolic link from source directory to target directory
function createSymbolicLink() {
  targetDir=${1}
  sourceDir=${2}
  /bin/ln -sFf ${targetDir} ${sourceDir}
  [ $? -ne 0 ] && trace SEVERE "failed to create symbolic link from '${sourceDir}' to '${targetDir}'" && exitOrLoop
  trace "Created symbolic link from '${sourceDir}' to '${targetDir}'"
}

# The following function will attempt to create a symbolic link from the server's default 'data' directory,
# (${DOMAIN_HOME}/servers/${SERVER_NAME}/data) to the centralized data directory specified by the
# 'dataHome' attribute of the CRD ($DATA_HOME/${SERVER_NAME}/data).  If both the ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
# and $DATA_HOME/${SERVER_NAME}/data directories contain persistent files that the Operator can't resolve
# than an error message is logged asking the user to manually resolve the files and then exit.
function linkServerDefaultDir() {
  # if server's default 'data' directory (${DOMAIN_HOME}/servers/${SERVER_NAME}/data) does not exist than create
  # symbolic link to location specified by $DATA_HOME/${SERVER_NAME}/data
  if [ ! -d ${DOMAIN_HOME}/servers/${SERVER_NAME}/data ]; then
    trace "'${DOMAIN_HOME}/servers/${SERVER_NAME}/data' does NOT exist as a directory"

    # Create the server's directory in $DOMAIN_HOME/servers
    if [ ! -d ${DOMAIN_HOME}/servers/${SERVER_NAME} ]; then
      trace "Creating directory '${DOMAIN_HOME}/servers/${SERVER_NAME}'"
      createFolder ${DOMAIN_HOME}/servers/${SERVER_NAME}
    else
      trace "'${DOMAIN_HOME}/servers/${SERVER_NAME}' already exists as a directory"
    fi

    # If server's 'data' directory is not already a symbolic link than create the symbolic link to
    # $DATA_HOME/${SERVER_NAME}/data
    if [ ! -L ${DOMAIN_HOME}/servers/${SERVER_NAME}/data ]; then
      createSymbolicLink ${DATA_HOME}/${SERVER_NAME}/data ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
    else
      trace "'${DOMAIN_HOME}/servers/${SERVER_NAME}/data' is already a symbolic link"
    fi
  else
    trace "'${DOMAIN_HOME}/servers/${SERVER_NAME}/data' exists as a directory"

    # server's default 'data' directory (${DOMAIN_HOME}/servers/${SERVER_NAME}/data) exists so first verify it's
    # not a symbolic link.  If it's already a symbolic link than there is nothing to do.
    if [ -L ${DOMAIN_HOME}/servers/${SERVER_NAME}/data ]; then
      trace "'${DOMAIN_HOME}/servers/${SERVER_NAME}/data' is already a symbolic link"
    else
      # Server's default 'data' directory (${DOMAIN_HOME}/servers/${SERVER_NAME}/data) exists and is not
      # a symbolic link so must be a directory.

      # count number of files found under directory ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
      countFilesInDir ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
      fileCountServerDomainHomeDir=$?

      # count number of files found under directory ${DATA_HOME}/${SERVER_NAME}/data
      countFilesInDir ${DATA_HOME}/${SERVER_NAME}/data
      fileCountServerDataDir=$?

      # Use file counts to determine whether or not we can create a symbolic link to centralize
      # data directory in specified ${DATA_HOME}/${SERVER_NAME}/data directory.
      if [ ${fileCountServerDataDir} -eq 0 ]; then
        if [ ${fileCountServerDomainHomeDir} -ne 0 ]; then
          cp -rf ${DOMAIN_HOME}/servers/${SERVER_NAME}/data ${DATA_HOME}/${SERVER_NAME}
          [ $? -ne 0 ] && trace SEVERE "failed to copy directory/files from '${DOMAIN_HOME}/servers/${SERVER_NAME}/data' to '${DATA_HOME}/${SERVER_NAME}' directory" && exitOrLoop
          trace "Recursively copied directory/files from '${DOMAIN_HOME}/servers/${SERVER_NAME}/data' to '${DATA_HOME}/${SERVER_NAME}' directory"
        else
          trace "'${DOMAIN_HOME}/servers/${SERVER_NAME}/data' directory is empty"
        fi

        # forcefully delete the server's data directory so we can create symbolic link
        rm -rf ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
        [ $? -ne 0 ] && trace SEVERE "failed to delete '${DOMAIN_HOME}/servers/${SERVER_NAME}/data' directory" && exitOrLoop
        trace "Deleted directory '${DOMAIN_HOME}/servers/${SERVER_NAME}/data'"

        # Create the symbolic link from server's data directory to $DATA_HOME
        createSymbolicLink ${DATA_HOME}/${SERVER_NAME}/data ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
      elif [ ${fileCountServerDataDir} -ne 0 ]; then
        if [ ${fileCountServerDomainHomeDir} -ne 0 ]; then
          trace SEVERE "The directory located in DOMAIN_HOME at '${DOMAIN_HOME}/servers/${SERVER_NAME}/data' and the directory located in the domain resource dataHome directory at '${DATA_HOME}/${SERVER_NAME}/data' both contain persistent files and the Operator cannot resolve which directory to use. You must manually move any persistent files from the '${DOMAIN_HOME}/servers/${SERVER_NAME}/data' directory to '${DATA_HOME}/${SERVER_NAME}/data', or remove them, and then delete the '${DOMAIN_HOME}/servers/${SERVER_NAME}/data' directory. Once this is done you can then restart the Domain. Alternatively, you can avoid this validation by setting the 'KEEP_DEFAULT_DATA_HOME' environment variable, in which case WebLogic custom and default stores will use the dataHome location (ignoring any files in the DOMAIN_HOME location), and other services will use the potentially ephemeral DOMAIN_HOME location for their files."
          exitOrLoop
        else
          # forcefully delete the server's data directory so we can create symbolic link
          rm -rf ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
          [ $? -ne 0 ] && trace SEVERE "failed to delete '${DOMAIN_HOME}/servers/${SERVER_NAME}/data' directory" && exitOrLoop
          trace "Deleted directory '${DOMAIN_HOME}/servers/${SERVER_NAME}/data'"

          # Create the symbolic link from server's data directory to $DATA_HOME
          createSymbolicLink ${DATA_HOME}/${SERVER_NAME}/data ${DOMAIN_HOME}/servers/${SERVER_NAME}/data
        fi
      fi
    fi
  fi
}

#
# adjustPath
#   purpose: Prepend $PATH with $JAVA_HOME/bin if $JAVA_HOME is set
#            and if $JAVA_HOME/bin is not already in $PATH
#
function adjustPath() {
  if [ ! -z ${JAVA_HOME} ]; then
    if [[ ":$PATH:" != *":${JAVA_HOME}/bin:"* ]]; then
      export PATH="${JAVA_HOME}/bin:$PATH"
    fi
  fi
}

#
# checkAuxiliaryImage
#   purpose: If the AUXILIARY_IMAGE_PATH directory exists, it echoes the contents of output files
#            in ${AUXILIARY_IMAGE_PATH}/auxiliaryImagetLogs dir. It returns 1 if a SEVERE message
#            is found in any of the output files in ${AUXILIARY_IMAGE_PATH}/auxiliaryImageLogs dirs.
#            It also returns 1 if 'successfully' message is not found in the output files
#            or if the AUXILIARY_IMAGE_PATH directory is empty. Otherwise it returns 0 (success).
#            See also 'auxImage.sh'.
#            See also initAuxiliaryImage in 'utils_base.sh'.
#
function checkAuxiliaryImage() {
  # check auxiliary image results (if any)
  if [ -z "$AUXILIARY_IMAGE_PATHS" ]; then
    trace FINE "Auxiliary Image: Skipping auxiliary image checks (no auxiliary images configured)."
    return
  fi

  trace FINE "Auxiliary Image: AUXILIARY_IMAGE_PATHS is '$AUXILIARY_IMAGE_PATHS'."
  for AUXILIARY_IMAGE_PATH in ${AUXILIARY_IMAGE_PATHS/,/ }; do
    trace FINE "Auxiliary Image: AUXILIARY_IMAGE_PATH is '$AUXILIARY_IMAGE_PATH'."
    traceDirs $AUXILIARY_IMAGE_PATH
    touch ${AUXILIARY_IMAGE_PATH}/testaccess.tmp
    if [ $? -ne 0 ]; then
      trace SEVERE "Auxiliary Image: Cannot write to the AUXILIARY_IMAGE_PATH '${AUXILIARY_IMAGE_PATH}'. " \
                   "This path is configurable using the domain resource 'spec.auxiliaryImageVolumes.mountPath' " \
                   "attribute." && return 1
    fi
    rm -f ${AUXILIARY_IMAGE_PATH}/testaccess.tmp || return 1

    # The container .out files embed their container name, the names will sort in the same order in which the containers ran
    out_files=$(ls -1 $AUXILIARY_IMAGE_PATH/auxiliaryImageLogs/*.out 2>/dev/null | sort --version-sort)
    if [ -z "${out_files}" ]; then
      trace SEVERE "Auxiliary Image: Assertion failure. No files found in '$AUXILIARY_IMAGE_PATH/auxiliaryImageLogs/*.out'"
      return 1
    fi
    severe_found=false
    for out_file in $out_files; do
      if [ "$(grep -c SEVERE $out_file)" != "0" ]; then
        trace FINE "Auxiliary Image: Error found in file '${out_file}' while initializing auxiliaryImage."
        severe_found=true
      elif [ "$(grep -c successfully $out_file)" = "0" ]; then
        trace SEVERE "Auxiliary Image: Command execution was unsuccessful in file '${out_file}' while initializing auxiliaryImage. " \
                     "Contents of '${out_file}':"
        cat $out_file
        severe_found=true
        continue
      fi
      trace "Auxiliary Image: Contents of '${out_file}':"
      cat $out_file
      trace "Auxiliary Image: End of '${out_file}' contents"
    done
    [ "${severe_found}" = "true" ] && return 1
    [ -z "$(ls -A $AUXILIARY_IMAGE_PATH 2>/dev/null | grep -v auxiliaryImageLogs)" ] \
      && trace SEVERE "Auxiliary Image: No files found in '$AUXILIARY_IMAGE_PATH'. " \
       "Do your auxiliary images have files in their '$AUXILIARY_IMAGE_PATH' directories? " \
       "This path is configurable using the domain resource 'spec.auxiliaryImageVolumes.mountPath' attribute." \
      && return 1
  done
  return 0
}

utils.py:
----
# Copyright (c) 2018, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

# Usage: trace('string')
#        trace(logLevel,'string')
#
# Valid values for logLevel are SEVERE|WARNING|ERROR|INFO|CONFIG|FINE|FINER|FINEST
#    'ERROR' is converted to 'SEVERE' 
#    Unknown logLevels are converted to 'FINE'.
#
# This matches format of bash utils.sh trace and the operator's log format.
#
# Sample output:   @[2018-09-28T17:23:55.335000Z][introspectDomain.py:614][FINE] Domain introspection complete.
#
# Importing this file when it's not in sys.path of the calling script:
#
#   #Include this script's current directory in the import path
#   tmp_callerframerecord = inspect.stack()[0]    # 0 represents this line # 1 represents line at caller
#   tmp_info = inspect.getframeinfo(tmp_callerframerecord[0])
#   tmp_scriptdir=os.path.dirname(tmp_info[0])
#   sys.path.append(tmp_scriptdir)
#
#   from utils import *
#

import sys
import inspect
import os
from datetime import datetime

# NOTE: This may be parsed by the operator. Do not change the date or log format without 
#       also updating the parser.

def traceInner(logLevel,object):
  callerframerecord = inspect.stack()[2]    # 0 represents this line
                                            # 1 represents line at caller
                                            # 2 represents line at caller's caller
  info = inspect.getframeinfo(callerframerecord[0])
  dt=datetime.utcnow()
  filename=os.path.basename(info[0])
  lineno=info[1]
  # convert ERROR to SEVERE as operator has no ERROR level
  switcher = {
    'SEVERE'  : 'SEVERE',
    'ERROR'   : 'SEVERE',
    'WARNING' : 'WARNING',
    'INFO'    : 'INFO',
    'CONFIG'  : 'CONFIG',
    'FINE'    : 'FINE',
    'FINER'   : 'FINER',
    'FINEST'  : 'FINEST',
  }
  # use FINE as logLevel if logLevel is not a known type
  logLevel=switcher.get(logLevel.upper(),'FINE')
  print("@[%d-%.2d-%.2dT%.2d:%.2d:%.2d.%.6dZ][%s:%s][%s] %s"
        % (dt.year,dt.month,dt.day,dt.hour,dt.minute,dt.second,dt.microsecond,
           filename,lineno,logLevel,object))

def trace(arg1,arg2='SENTINEL'):
  if arg2 == 'SENTINEL': 
    traceInner('FINE',arg1)
  else:
    traceInner(arg1,arg2)

def raise_replication_port_conflict(name, listen_port, replication_port, SSL):
  raise ValueError('Server/ServerTemplate %s %s listen port %s conflicts with default replication channel port %s when '
                   'istio is enabled, please specify a different replication port for istio in '
                   'domain.spec.configuration.istio.replicationPort' % (name, SSL, listen_port, replication_port))
introspectDomain.py:
----
# Copyright (c) 2018, 2022, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# ------------
# Description:
# ------------
#
#   This code reads the configuration in a WL domain's domain home, and generates
#   multiple files that are copied to stdout.  It also checks whether the domain
#   configuration is 'valid' (suitable for running in k8s).  Finally, it 
#   populates customer supplied 'configOverrides' templates.
#
#   This code is used by the operator to introspect and validate an arbitrary
#   WL domain before its pods are started.  It generates information that's
#   useful for running the domain, setting up its networking, and for overriding
#   specific parts of its configuration so that it can run in k8s.
# 
#   For more details, see the Description in instrospectDomain.sh (which
#   calls this script).
#
# ---------------------
# Prerequisites/Inputs:
# ---------------------
#
#   A WebLogic install.
#
#   A WebLogic domain.
#
#   A running WL node manager at port 5556 (see introspectDomain.sh).
#
#   Plain text WL admin username/password in:
#     /weblogic-operator/secrets/username 
#     /weblogic-operator/secrets/password
#
#   Optional custom sit cfg 'configOverrides' templates in:
#     /weblogic-operator/config-overrides-secrets
#
#   Optional custom sit cfg 'configOverridesSecrets' in:
#     /weblogic-operator/config-overrides-secrets/<secret-name>/<key>
#
#   The following env vars:
#     DOMAIN_UID         - completely unique id for this domain
#     DOMAIN_HOME        - path for the domain configuration
#     LOG_HOME           - path to override WebLogic server log locations
#     CREDENTIALS_SECRET_NAME  - name of secret containing credentials
#
# ---------------------------------
# Outputs (files copied to stdout):
# ---------------------------------
#
#   topology.yaml                  -- Domain configuration summary for operator (server names, etc).
#                                           -and/or-
#                                     Domain validation warnings/errors. 
#
#   Sit-Cfg-CFG--introspector-situational-config.xml  
#                                  -- Automatic sit cfg overrides for domain configuration
#                                     (listen addresses, etc).
#
#   Sit-Cfg-*                      -- Expanded optional configOverrides sit cfg templates
#
#   boot.properties                -- Encoded credentials for starting WL.
#   userConfigNodeManager.secure   -- Encoded credentials for starting NM in a WL pod.
#   userKeyNodeManager.secure      -- Encoded credentials for starting NM in a WL pod.
#
# 
# Note:
#
#   This code partly depends on a node manager so that we can use it to encrypt 
#   the username and password and put them into files that can be used to connect
#   to the node manager later in the server pods (so that the server pods don't
#   have to mount the secret containing the username and password).
#
#   The configuration overrides are specified via situational config file(s), and 
#   include listen addresses, log file locations, etc.  Additional information
#   is provided in other files -- including encrypted credentials, domain
#   topology (server names, etc), and any validation warnings/errors.
#


import base64, md5
import distutils.dir_util
import inspect
import os
import re
import sys
import traceback
import xml.dom.minidom
from xml.dom.minidom import parse

# Include this script's current directory in the import path (so we can import utils, etc.)
# sys.path.append('/weblogic-operator/scripts')

# Alternative way to dynamically get script's current directory
tmp_callerframerecord = inspect.stack()[0]    # 0 represents this line # 1 represents line at caller
tmp_info = inspect.getframeinfo(tmp_callerframerecord[0])
tmp_scriptdir=os.path.dirname(tmp_info[0])
sys.path.append(tmp_scriptdir)

from utils import *
from weblogic.management.configuration import LegalHelper

ISTIO_NAP_NAMES = ['tcp-cbt', 'tcp-ldap', 'tcp-iiop', 'tcp-snmp', 'http-default', 'tcp-default', 'https-secure', 'tls-ldaps', 'tls-default', 'tls-cbts', 'tls-iiops', 'https-admin']

class OfflineWlstEnv(object):

  def open(self):

    # before doing anything, get each env var and verify it exists 
    global server_template_sslports
    global server_template_listening_ports


    self.DOMAIN_UID               = self.getEnv('DOMAIN_UID')
    self.DOMAIN_HOME              = self.getEnv('DOMAIN_HOME')
    self.LOG_HOME                 = self.getEnv('LOG_HOME')
    self.ACCESS_LOG_IN_LOG_HOME   = self.getEnvOrDef('ACCESS_LOG_IN_LOG_HOME', 'true')
    self.DATA_HOME                = self.getEnvOrDef('DATA_HOME', "")
    self.CREDENTIALS_SECRET_NAME  = self.getEnv('CREDENTIALS_SECRET_NAME')

    # initialize globals

    # The following 3 globals mush match prefix hard coded in startServer.sh
    self.CUSTOM_PREFIX_JDBC = 'Sit-Cfg-JDBC--'
    self.CUSTOM_PREFIX_JMS  = 'Sit-Cfg-JMS--'
    self.CUSTOM_PREFIX_WLDF = 'Sit-Cfg-WLDF--'
    self.CUSTOM_PREFIX_CFG  = 'Sit-Cfg-CFG--'

    self.INTROSPECT_HOME          = '/tmp/introspect/' + self.DOMAIN_UID
    self.TOPOLOGY_FILE            = self.INTROSPECT_HOME + '/topology.yaml'
    self.CM_FILE                  = self.INTROSPECT_HOME + '/' + self.CUSTOM_PREFIX_CFG + 'introspector-situational-config.xml'
    self.BOOT_FILE                = self.INTROSPECT_HOME + '/boot.properties'
    self.USERCONFIG_FILE          = self.INTROSPECT_HOME + '/userConfigNodeManager.secure'
    self.USERKEY_FILE             = self.INTROSPECT_HOME + '/userKeyNodeManager.secure'

    # Model in image attributes

    self.MII_DOMAIN_SECRET_MD5_FILE   = '/tmp/DomainSecret.md5'
    self.MII_DOMAIN_ZIP               = self.INTROSPECT_HOME + '/domainzip.secure'
    self.MII_PRIMORDIAL_DOMAIN_ZIP    = self.INTROSPECT_HOME + '/primordial_domainzip.secure'

    self.MII_INVENTORY_IMAGE_MD5      = self.INTROSPECT_HOME + '/inventory_image.md5'
    self.MII_INVENTORY_CM_MD5         = self.INTROSPECT_HOME + '/inventory_cm.md5'
    self.MII_INVENTORY_PASSPHRASE_MD5 = self.INTROSPECT_HOME + '/inventory_passphrase.md5'
    self.MII_MERGED_MODEL_FILE        = self.INTROSPECT_HOME + '/merged_model.json'
    self.MII_JRF_EWALLET              = self.INTROSPECT_HOME + '/ewallet.p12'
    self.WLS_VERSION                  = self.INTROSPECT_HOME + "/wls.version"
    self.JDK_PATH                     = self.INTROSPECT_HOME + "/jdk.path"
    self.MII_SECRETS_AND_ENV_MD5      = self.INTROSPECT_HOME + "/secrets_and_env.md5"
    self.MII_DOMAINZIP_HASH           = self.INTROSPECT_HOME + "/domainzip_hash"
    self.MII_WDT_CONFIGMAP_PATH       = self.getEnvOrDef('WDT_CONFIGMAP_PATH',
                                                    '/weblogic-operator/wdt-config-map')
    self.DOMAIN_SOURCE_TYPE           = self.getEnvOrDef("DOMAIN_SOURCE_TYPE", None)

    # The following 4 env vars are for unit testing, their defaults are correct for production.
    self.CREDENTIALS_SECRET_PATH = self.getEnvOrDef('CREDENTIALS_SECRET_PATH', '/weblogic-operator/secrets')
    self.CUSTOM_SECRET_ROOT      = self.getEnvOrDef('CUSTOM_SECRET_ROOT', '/weblogic-operator/config-overrides-secrets')
    self.CUSTOM_SITCFG_PATH      = self.getEnvOrDef('CUSTOM_SITCFG_PATH', '/weblogic-operator/config-overrides')
    self.NM_HOST                 = self.getEnvOrDef('NM_HOST', 'localhost')

    # maintain a list of errors that we include in topology.yaml on completion, if any

    self.errors             = []

    # maintain a list of files that we print on completion when there are no errors

    self.generatedFiles     = []

    # create tmp directory (mkpath == 'mkdir -p') 

    distutils.dir_util.mkpath(self.INTROSPECT_HOME)

    # remove any files that are already in the tmp directory

    for the_file in os.listdir(self.INTROSPECT_HOME):
      the_file_path = os.path.join(self.INTROSPECT_HOME, the_file)
      if os.path.isfile(the_file_path):
        os.unlink(the_file_path)

    server_template_sslports, server_template_listening_ports = get_server_template_listening_ports_from_configxml(self.getDomainHome() + os.sep
                                                                                                                   + 'config' + os.sep + 'config.xml')

    trace("About to load domain from "+self.getDomainHome())
    readDomain(self.getDomainHome())
    self.domain = cmo
    self.DOMAIN_NAME = self.getDomain().getName()

    # this should only be done for model in image case
    if self.DOMAIN_SOURCE_TYPE == "FromModel":
      self.handle_ModelInImageDomain()

  def handle_ModelInImageDomain(self):
    self.WDT_DOMAIN_TYPE = self.getEnvOrDef('WDT_DOMAIN_TYPE', 'WLS')

    if self.WDT_DOMAIN_TYPE == 'JRF':
      try:
        # Only export if it is not there already (i.e. have not been copied from the secrets
        if not os.path.exists('/tmp/opsswallet/ewallet.p12'):
          opss_passphrase_file = self.getEnv('OPSS_KEY_PASSPHRASE')
          opss_passphrase = self.readFile(opss_passphrase_file).strip()
          os.mkdir('/tmp/opsswallet')
          exportEncryptionKey(jpsConfigFile=self.getDomainHome() + '/config/fmwconfig/jps-config.xml', \
                              keyFilePath='/tmp/opsswallet', keyFilePassword=opss_passphrase)
      except:
        trace("SEVERE","Error in exporting OPSS key ")
        dumpStack()
        sys.exit(1)

  def close(self):
    closeDomain()

  def getDomain(self):
    return self.domain

  def getDomainUID(self):
    return self.DOMAIN_UID

  def getDomainHome(self):
    return self.DOMAIN_HOME

  def getDomainLogHome(self):
    return self.LOG_HOME

  def getDataHome(self):
    return self.DATA_HOME

  def isAccessLogInLogHome(self):
    return self.ACCESS_LOG_IN_LOG_HOME == 'true';

  def addError(self, error):
    self.errors.append(error)

  def getErrors(self):
    return self.errors

  def getClusterOrNone(self,serverOrTemplate):
    try:
      ret = serverOrTemplate.getCluster()
    except:
      trace("Ignoring getCluster() exception, this is expected.")
      ret = None
    return ret

  def addGeneratedFile(self, filePath):
    self.generatedFiles.append(filePath)

  def printGeneratedFiles(self):
    for filePath in self.generatedFiles:
      self.printFile(filePath)

  def encrypt(self, cleartext):
    return encrypt(cleartext, self.getDomainHome())

  def readFile(self, path):
    file = open(path, 'r')
    contents = file.read()
    file.close()
    return contents

  def readBinaryFile(self, path):
    file = open(path, 'rb')
    contents = file.read()
    file.close()
    return contents

  def printFile(self, path):
    trace("Printing file " + path)
    print ">>> ",path
    print self.readFile(path)
    print ">>> EOF"
    print

  def getEnv(self, name):
    val = os.getenv(name)
    if val is None or val == "null":
      trace("SEVERE","Env var "+name+" not set.")
      sys.exit(1)
    return val

  def getEnvOrDef(self, name, deflt):
    val = os.getenv(name)
    if val == None or val == "null" or len(val) == 0:
      return deflt
    return val

  def isEnvSet(self, name):
    val = os.getenv(name)
    if val is None or val == "null":
      return False
    return True

  def toDNS1123Legal(self, address):
    return address.lower().replace('_','-')


class SecretManager(object):

  def __init__(self, env):
    self.env = env

  def encrypt(self, cleartext):
    return self.env.encrypt(cleartext)

  def readCredentialsSecret(self, key):
    path = self.env.CREDENTIALS_SECRET_PATH + '/' + key
    return self.env.readFile(path)

class Generator(SecretManager):

  def __init__(self, env, path):
    SecretManager.__init__(self, env)
    self.env = env
    self.path = path
    self.indentStack = [""]

  def open(self):
    self.f =  open(self.path, 'w+')

  def close(self):
    self.f.close()

  def indent(self):
    self.indentStack.append(self.indentPrefix() + "  ")

  def undent(self):
    self.indentStack.pop()

  def indentPrefix(self):
    return self.indentStack[len(self.indentStack)-1]

  def write(self, msg):
    self.f.write(msg)

  def writeln(self, msg):
    self.f.write(self.indentPrefix() + msg + "\n")

  def quote(self, val):
    return "\"" + val + "\""

  def name(self, mbean):
    return "\"" + mbean.getName() + "\"";

  def addGeneratedFile(self):
    return self.env.addGeneratedFile(self.path)

class TopologyGenerator(Generator):

  def __init__(self, env):
    Generator.__init__(self, env, env.TOPOLOGY_FILE)

  def validate(self):
    self.validateAdminServer()
    self.validateClusters()
    self.validateServerCustomChannelName()
    self.validateDynamicClustersDuplicateServerNamePrefix()
    return self.isValid()

  def generate(self):
    self.open()
    try:
      if self.isValid():
        self.generateTopology()
      else:
        self.reportErrors()
      self.close()
      self.addGeneratedFile()
    finally:
      self.close()

  # Work-around bug in off-line WLST where cluster.getDynamicServers() may throw
  # when there are no 'real' DynamicServers.  Exception looks like:
  #     at com.sun.proxy.$Proxy46.getDynamicServers(Unknown Source)
  #     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  #     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  #     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  #     at java.lang.reflect.Method.invoke(Method.java:498)
  def getDynamicServersOrNone(self,cluster):
    try:
      ret = cluster.getDynamicServers()
      # Dynamic Servers must be configured with a ServerTemplate
      if ret is not None:
        if ret.getServerTemplate() is None:
          ret = None
    except:
      trace("Ignoring getDynamicServers() exception, this is expected.")
      ret = None
    return ret

  def validateAdminServer(self):
    adminServerName = self.env.getDomain().getAdminServerName()
    if adminServerName is None:
      addError("The admin server name is null.")
      return
    adminServer = None
    for server in self.env.getDomain().getServers():
      self.validateServerTemplateNapListenPortIsSet(server)
      if adminServerName == server.getName():
        adminServer = server
    if adminServer is None:
      addError("The admin server '" + adminServerName + "' does not exist.")
      return
    cluster = self.env.getClusterOrNone(adminServer)
    if cluster is not None:
      self.addError("The admin server " + self.name(adminServer) + " belongs to the WebLogic cluster " + self.name(cluster) + ", the operator does not support having an admin server participate in a cluster.")

  def validateDynamicClustersDuplicateServerNamePrefix(self):
    serverNamePrefixes = []
    for cluster in self.env.getDomain().getClusters():
      if self.getDynamicServersOrNone(cluster) is not None:
        if cluster.getDynamicServers().getServerNamePrefix() in serverNamePrefixes:
          self.addError("The ServerNamePrefix '" + cluster.getDynamicServers().getServerNamePrefix() + "' specified for WebLogic dynamic cluster " + self.name(cluster) + "'s dynamic servers is already in use. The ServerNamePrefix must be unique for each WebLogic dynamic cluster.")
        else:
          serverNamePrefixes.append(cluster.getDynamicServers().getServerNamePrefix())

  def validateClusters(self):
    for cluster in self.env.getDomain().getClusters():
      self.validateCluster(cluster)

  def validateCluster(self, cluster):
    if self.getDynamicServersOrNone(cluster) is None:
      self.validateNonDynamicCluster(cluster)
    else:
      self.validateDynamicCluster(cluster)

  def validateNonDynamicCluster(self, cluster):
    self.validateNonDynamicClusterReferencedByAtLeastOneServer(cluster)
    self.validateNonDynamicClusterNotReferencedByAnyServerTemplates(cluster)
    self.validateNonDynamicClusterServersHaveSameListenPort(cluster)
    self.validateNonDynamicClusterServerHaveSameCustomChannels(cluster)

  def validateNonDynamicClusterReferencedByAtLeastOneServer(self, cluster):
    for server in self.env.getDomain().getServers():
      if self.env.getClusterOrNone(server) is cluster:
        return
    self.addError("The WebLogic configured cluster " + self.name(cluster) + " is not referenced by any servers.")

  def validateNonDynamicClusterNotReferencedByAnyServerTemplates(self, cluster):
    for template in self.env.getDomain().getServerTemplates():
      if self.env.getClusterOrNone(template) is cluster:
        self.addError("The WebLogic configured cluster " + self.name(cluster) + " is referenced by the server template " + self.name(template) + ", the operator does not support 'mixed clusters' that host both dynamic (templated) servers and configured servers.")

  LISTEN_PORT = 'listen port'
  LISTEN_PORT_ENABLED = 'listen port enabled'
  SSL_LISTEN_PORT = 'ssl listen port'
  SSL_LISTEN_PORT_ENABLED = 'ssl listen port enabled'
  ADMIN_LISTEN_PORT = 'admin listen port'
  ADMIN_LISTEN_PORT_ENABLED = 'admin listen port enabled'

  def validateNonDynamicClusterServersHaveSameListenPort(self, cluster):
    firstServer = None
    firstListenPort = None
    firstListenPortEnabled = None
    firstSslListenPort = None
    firstSslListenPortEnabled = None
    firstAdminPort = None
    firstAdminPortEnabled = None
    for server in self.env.getDomain().getServers():
      if cluster is self.env.getClusterOrNone(server):
        listenPort = getRealListenPort(server)
        listenPortEnabled = isListenPortEnabledForServer(server, self.env.getDomain())
        # default is False
        sslListenPortEnabled = False
        sslListenPort = None
        ssl_listen_port = getSSLPortIfEnabled(server, self.env.getDomain(), is_server_template=False)
        if ssl_listen_port is not None:
          sslListenPort = ssl_listen_port
          sslListenPortEnabled = True

        # WLST does return ssl listen port as None if SSL is disabled

        adminPort = getAdministrationPort(server, self.env.getDomain())
        adminPortEnabled = isAdministrationPortEnabledForServer(server, self.env.getDomain())
        if firstServer is None:
          firstServer = server
          firstListenPort = listenPort
          firstListenPortEnabled = listenPortEnabled
          firstSslListenPort = sslListenPort
          firstSslListenPortEnabled = sslListenPortEnabled
          firstAdminPort = adminPort
          firstAdminPortEnabled = adminPortEnabled
        else:
          if listenPort != firstListenPort:
            self.addError("The WebLogic configured cluster " + self.name(cluster) + "'s server " + self.name(firstServer) + "'s listen port is " + str(firstListenPort) + " but its server " + self.name(server) + "'s listen port is " + str(listenPort) + ". All ports for the same channel in a cluster must be the same.")
          if listenPortEnabled != firstListenPortEnabled:
            self.addError("The WebLogic configured cluster " + self.name(cluster) + "'s server " + self.name(firstServer) + " has listen port enabled: " + self.booleanToString(firstListenPortEnabled) + " but its server " + self.name(server) + "'s listen port enabled: " + self.booleanToString(listenPortEnabled) + ".  Channels in a cluster must be either all enabled or disabled.")
          if sslListenPort is not None and sslListenPort != firstSslListenPort:
             self.addError("The WebLogic configured cluster " + self.name(cluster) + "'s server " + self.name(firstServer) + "'s ssl listen port is " + str(firstSslListenPort) + " but its server " + self.name(server) + "'s ssl listen port is " + str(sslListenPort)
                           + ".  All ports for the same channel in a cluster must be the same and they must either be "
                           + "enabled or disabled uniformly")
          if sslListenPortEnabled != firstSslListenPortEnabled:
            self.addError("The WebLogic configured cluster " + self.name(cluster) + "'s server " + self.name(firstServer) + " has ssl listen port enabled: " + self.booleanToString(firstSslListenPortEnabled) + " but its server " + self.name(server) + "'s ssl listen port enabled: " + self.booleanToString(sslListenPortEnabled)
                          + ".  All ports for the same channel in a cluster must be the same and they must either be "
                          + "enabled or disabled uniformly")
          if adminPort != firstAdminPort:
            self.addError("The WebLogic configured cluster " + self.name(cluster) + "'s server " + self.name(firstServer) + "'s ssl listen port is " + str(firstAdminPort) + " but its server " + self.name(server) + "'s ssl listen port is " + str(adminPort) + ".  All ports for the same channel in a cluster must be the same.")
          if adminPortEnabled != firstAdminPortEnabled:
            self.addError("The WebLogic configured cluster " + self.name(cluster) + "'s server " + self.name(firstServer) + " has ssl listen port enabled: " + self.booleanToString(firstAdminPortEnabled) + " but its server " + self.name(server) + "'s ssl listen port enabled: " + self.booleanToString(adminPortEnabled) + ".  Channels in a cluster must be either all enabled or disabled.")

  def validateNonDynamicClusterServerHaveSameCustomChannels(self, cluster):
     firstServer = None
     serverNap = {}
     for server in self.env.getDomain().getServers():
       if cluster is self.env.getClusterOrNone(server):
         if firstServer is None:
           for nap in server.getNetworkAccessPoints():
             serverNap[nap.getName()] = getNAPProtocol(nap, server, self.env.getDomain()) + "~" + str(nap.getListenPort());
           firstServer = server
         else:
           naps = server.getNetworkAccessPoints()
           if len(naps) != len(serverNap):
             self.addError("The WebLogic configured cluster " + self.name(cluster) + " has mismatched number of network access points in servers " + self.name(firstServer) + " and " + self.name(server) + ". All network access points in a cluster must be the same.")
             return
           else:
             for nap in naps:
               if nap.getName() in serverNap:
                 if serverNap[nap.getName()] != getNAPProtocol(nap, server, self.env.getDomain()) + "~" + str(nap.getListenPort()):
                   self.addError("The WebLogic configured cluster " + self.name(cluster) + " has mismatched network access point " + self.name(nap) + " in servers " + self.name(firstServer) + " and " + self.name(server) + ". All network access points in a cluster must be the same.")
                   return
               else:
                 self.addError("The WebLogic configured cluster " + self.name(cluster) + " has mismatched network access point " + self.name(nap) + " in servers " + self.name(firstServer) + " and " + self.name(server) + ". All network access points in a cluster must be the same.")
                 return


  def validateDynamicCluster(self, cluster):
    self.validateDynamicClusterReferencedByOneServerTemplate(cluster)
    self.validateDynamicClusterDynamicServersDoNotUseCalculatedListenPorts(cluster)
    self.validateDynamicClusterNotReferencedByAnyServers(cluster)

  def validateDynamicClusterReferencedByOneServerTemplate(self, cluster):
    server_template=None
    for template in self.env.getDomain().getServerTemplates():
      self.validateServerTemplateNapListenPortIsSet(template)
      if self.env.getClusterOrNone(template) is cluster:
        if server_template is None:
          server_template = template
        else:
          if server_template is not None:
            self.addError("The WebLogic dynamic cluster " + self.name(cluster) + " is referenced the server template " + self.name(server_template) + " and the server template " + self.name(template) + ".")
            return
    if server_template is None:
      self.addError("The WebLogic dynamic cluster " + self.name(cluster) + "' is not referenced by any server template.")

  def validateServerTemplateNapListenPortIsSet(self, server_or_template):
    naps = server_or_template.getNetworkAccessPoints()
    for nap in naps:
      if nap.getListenPort() == 0:
        self.addError(
              "Invalid listen port value '"
              + str(nap.getListenPort())
              + "' in the WebLogic Domain for "
              + server_or_template.getName()
              + ' Network Channel '
              + nap.getName()
              + '. Please provide a valid value for the listen port, this is likely because of not specifying the port '
                'value during domain '
                'creation')

  def validateDynamicClusterNotReferencedByAnyServers(self, cluster):
    for server in self.env.getDomain().getServers():
      if self.env.getClusterOrNone(server) is cluster:
        self.addError("The WebLogic dynamic cluster " + self.name(cluster) + " is referenced by configured server " + self.name(server) + ", the operator does not support 'mixed clusters' that host both dynamic (templated) servers and configured servers.")

  def validateDynamicClusterDynamicServersDoNotUseCalculatedListenPorts(self, cluster):
    if cluster.getDynamicServers().isCalculatedListenPorts() == True:
      self.addError("The WebLogic dynamic cluster " + self.name(cluster) + "'s dynamic servers use calculated listen ports.")

  def validateServerCustomChannelName(self):
    reservedNames = ['default','default-secure','default-admin']
    for server in self.env.getDomain().getServers():
      naps = server.getNetworkAccessPoints()
      for nap in naps:
        if nap.getName() in reservedNames:
          self.addError("The custom channel " + self.name(nap) + " is a reserved name.")

  def isValid(self):
    return len(self.env.getErrors()) == 0

  def addError(self, error):
    self.env.addError(error)

  def reportErrors(self):
    self.writeln("domainValid: false")
    self.writeln("validationErrors:")
    for error in self.env.getErrors():
      self.writeln("- \"" + error.replace("\"", "\\\"") + "\"")

  def generateTopology(self):
    self.writeln("domainValid: true")
    self.addDomain()

  def addDomain(self):
    self.writeln("domain:")
    self.indent()
    self.writeln("name: " + self.name(self.env.getDomain()))
    self.writeln("adminServerName: " + self.quote(self.env.getDomain().getAdminServerName()))
    self.addConfiguredClusters()
    self.addServerTemplates()
    self.addNonClusteredServers()
    self.undent()

  def addConfiguredClusters(self):
    clusters = self.env.getDomain().getClusters()
    if len(clusters) == 0:
      return
    self.writeln("configuredClusters:")
    self.indent()
    for cluster in clusters:
      self.addConfiguredCluster(cluster)
    self.undent()
  
  def getConfiguredClusters(self):
    rtn = []
    for cluster in self.env.getDomain().getClusters():
      if self.getDynamicServersOrNone(cluster) is None:
        rtn.append(cluster)
    return rtn

  def addConfiguredCluster(self, cluster):
    self.writeln("- name: " + self.name(cluster))
    dynamicServers = self.getDynamicServersOrNone(cluster)
    if dynamicServers is not None:
      self.indent();
      self.writeln("dynamicServersConfig:")
      self.indent()
      self.addDynamicServer(dynamicServers)
      self.undent()
      self.undent()
    servers = self.getClusteredServers(cluster)
    if len(servers) != 0:
      self.indent();
      self.writeln("servers:")
      self.indent()
      for server in servers:
        self.addServer(server)
      self.undent()
      self.undent()

  def addDynamicServer(self, dynamicServer):
    if dynamicServer.getName() is not None:
      name=self.name(dynamicServer)
      self.writeln("name: " + name)
    self.writeln("serverTemplateName: " + self.quote(dynamicServer.getServerTemplate().getName()))
    self.writeln("calculatedListenPorts: " + str(dynamicServer.isCalculatedListenPorts()))
    self.writeln("serverNamePrefix: " + self.quote(dynamicServer.getServerNamePrefix()))
    self.writeln("dynamicClusterSize: " + str(dynamicServer.getDynamicClusterSize()))
    self.writeln("maxDynamicClusterSize: " + str(dynamicServer.getMaxDynamicClusterSize()))
    self.writeln("minDynamicClusterSize: " + str(dynamicServer.getMinDynamicClusterSize()))

  def getClusteredServers(self, cluster):
    rtn = []
    for server in self.env.getDomain().getServers():
      if self.env.getClusterOrNone(server) is cluster:
        rtn.append(server)
    return rtn

  def addServer(self, server, is_server_template=False):
    name=self.name(server)
    self.writeln("- name: " + name)
    if isListenPortEnabledForServer(server, self.env.getDomain(), is_server_template):
      listen_port = getRealListenPort(server)
      self.writeln("  listenPort: " + str(listen_port))
    self.writeln("  listenAddress: " + self.quote(self.env.toDNS1123Legal(self.env.getDomainUID() + "-" + server.getName())))
    if isAdministrationPortEnabledForServer(server, self.env.getDomain(), is_server_template):
      self.writeln("  adminPort: " + str(getAdministrationPort(server, self.env.getDomain())))
    self.addSSL(server, is_server_template)
    self.addNetworkAccessPoints(server, is_server_template)

  def addSSL(self, server, is_server_template):
    '''
    Write the SSL topology information to the topology yaml output
    :param server: Server or ServerTemplate
    '''
    ssl_listen_port = getSSLPortIfEnabled(server, self.env.getDomain(), is_server_template)
    if ssl_listen_port is not None:
      self.indent()
      self.writeln("sslListenPort: " + str(ssl_listen_port))
      self.undent()

  def addServerTemplates(self):
    serverTemplates = self.env.getDomain().getServerTemplates()
    if len(serverTemplates) == 0:
      return
    self.writeln("serverTemplates:")
    self.indent()
    for serverTemplate in serverTemplates:
      if not (self.env.getClusterOrNone(serverTemplate) is None):
        self.addServerTemplate(serverTemplate)
    self.undent()

  def addServerTemplate(self, serverTemplate):
    self.addServer(serverTemplate, is_server_template=True)
    self.writeln("  clusterName: " + self.quote(serverTemplate.getCluster().getName()))

  def addDynamicClusters(self):
    clusters = self.getDynamicClusters()
    if len(clusters) == 0:
      return
    self.writeln("dynamicClusters:")
    self.indent()
    for cluster in clusters:
      self.addDynamicCluster(cluster)
    self.undent()
  
  def getDynamicClusters(self):
    rtn = []
    for cluster in self.env.getDomain().getClusters():
      if self.getDynamicServersOrNone(cluster) is not None:
        rtn.append(cluster)
    return rtn

  def addDynamicCluster(self, cluster):
    self.writeln(self.name(cluster) + ":")
    self.indent()
    template = self.findDynamicClusterServerTemplate(cluster)
    dyn_servers = cluster.getDynamicServers()
    listen_port = getRealListenPort(template)
    self.writeln("port: " + str(listen_port))
    self.writeln("maxServers: " + str(dyn_servers.getDynamicClusterSize()))
    self.writeln("baseServerName: " + self.quote(dyn_servers.getServerNamePrefix()))
    self.undent()

  def findDynamicClusterServerTemplate(self, cluster):
    for template in cmo.getServerTemplates():
      if self.env.getClusterOrNone(template) is cluster:
        return template
    # should never get here - the domain validator already checked that
    # one server template references the cluster
    return None

  def addNonClusteredServers(self):
    # the domain validator already checked that we have a non-clustered admin server
    # therefore we know there will be at least one non-clustered server
    self.writeln("servers:")
    self.indent()
    for server in self.env.getDomain().getServers():
      if self.env.getClusterOrNone(server) is None:
        self.addServer(server)
    self.undent()

  def addNetworkAccessPoints(self, server, is_server_template=False):
    """
    Add network access points for server or server template
    :param server:  server or server template
    """
    naps = server.getNetworkAccessPoints()
    added_nap = False
    if len(naps) != 0:
      added_nap = True
      self.writeln("  networkAccessPoints:")
      self.indent()
      for nap in naps:
        self.addNetworkAccessPoint(server, nap, is_server_template)

    added_istio_yaml = self.addIstioNetworkAccessPoints(server, is_server_template, added_nap)
    if len(naps) != 0 or added_istio_yaml:
      self.undent()

  def addNetworkAccessPoint(self, server, nap, is_server_template):

    # Change the name to follow the istio port naming convention
    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    nap_protocol = getNAPProtocol(nap, server, self.env.getDomain(), is_server_template)

    if istio_enabled == 'true':
      name = nap.getName()
      if name.startswith('http-') or name.startswith('tcp-') or name.startswith('tls-') \
          or name.startswith('https-'):
        # skip istio ports already defined by WDT filtering for MII
        return
      http_protocol = [ 'http' ]
      https_protocol = ['https','admin']
      tcp_protocol = [ 't3', 'snmp', 'ldap', 'cluster-broadcast', 'iiop', 'sip']
      tls_protocol = [ 't3s', 'iiops', 'cluster-broadcast-secure', 'sips']
      if nap_protocol in http_protocol:
        name = 'http-' + nap.getName().replace(' ', '_')
      elif nap_protocol in https_protocol:
        name = 'https-' + nap.getName().replace(' ', '_')
      elif nap_protocol in tcp_protocol:
        name = 'tcp-' + nap.getName().replace(' ', '_')
      elif nap_protocol in tls_protocol:
        name = 'tls-' + nap.getName().replace(' ', '_')
      else:
        name = 'tcp-' + nap.getName().replace(' ', '_')
    else:
      name=self.name(nap)
    self.writeln("  - name: " + name)
    self.writeln("    protocol: " + self.quote(nap_protocol))

    self.writeln("    listenPort: " + str(nap.getListenPort()))
    self.writeln("    publicPort: " + str(nap.getPublicPort()))


  def addIstioNetworkAccessPoints(self, server, is_server_template, added_nap):
    '''
    Write the container ports information for operator to create the container ports in the topology.xml file
    :param server:   server or template mbean
    :param is_server_template:  true if it is from ServerTemplate
    :param added_nap:  true if there are existing nap section in the output
    '''
    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    if istio_enabled == 'false':
      return False

    if not added_nap:
      self.writeln("  networkAccessPoints:")
      self.indent()

    self.addIstioNetworkAccessPoint("tcp-ldap", "ldap", getRealListenPort(server), 0)
    self.addIstioNetworkAccessPoint("tcp-default", "t3", getRealListenPort(server), 0)
    # No need to to http default, PodStepContext already handle it
    self.addIstioNetworkAccessPoint("http-default", "http", getRealListenPort(server), 0)
    self.addIstioNetworkAccessPoint("tcp-snmp", "snmp", getRealListenPort(server), 0)
    self.addIstioNetworkAccessPoint("tcp-iiop", "iiop", getRealListenPort(server), 0)

    ssl_listen_port = getSSLPortIfEnabled(server, self.env.getDomain(), is_server_template)

    if ssl_listen_port is not None:
      self.addIstioNetworkAccessPoint("https-secure", "https", ssl_listen_port, 0)
      self.addIstioNetworkAccessPoint("tls-ldaps", "ldaps", ssl_listen_port, 0)
      self.addIstioNetworkAccessPoint("tls-default", "t3s", ssl_listen_port, 0)
      self.addIstioNetworkAccessPoint("tls-iiops", "iiops", ssl_listen_port, 0)

    if isAdministrationPortEnabledForServer(server, self.env.getDomain(), is_server_template):
      self.addIstioNetworkAccessPoint("https-admin", "https", getAdministrationPort(server, self.env.getDomain()), 0)
    return True

  def addIstioNetworkAccessPoint(self, name, protocol, listen_port, public_port):
    self.writeln("  - name: " + name)
    self.writeln("    protocol: " + protocol)
    self.writeln("    listenPort: " + str(listen_port))
    self.writeln("    publicPort: " + str(public_port))

  def booleanToString(self, bool):
    if bool == 0:
      return "false"
    return "true"

class BootPropertiesGenerator(Generator):

  def __init__(self, env):
    Generator.__init__(self, env, env.BOOT_FILE)

  def generate(self):
    self.open()
    try:
      self.addBootProperties()
      self.close()
      self.addGeneratedFile()
    finally:
      self.close()

  def addBootProperties(self):
    self.writeln("username=" + self.encrypt(self.readCredentialsSecret("username")))
    self.writeln("password=" + self.encrypt(self.readCredentialsSecret("password")))

class UserConfigAndKeyGenerator(Generator):

  def __init__(self, env):
    Generator.__init__(self, env, env.USERKEY_FILE)
    self.env = env

  def generate(self):
    if not self.env.NM_HOST:
      # user config&key generation has been disabled for test purposes
      return
    self.open()
    try:
      # first, generate UserConfig file and add it, also generate UserKey file
      self.addUserConfigAndKey()
      self.close()
      # now add UserKey file
      self.addGeneratedFile()
    finally:
      self.close()

  def addUserConfigAndKey(self):
    username = self.readCredentialsSecret("username")
    password = self.readCredentialsSecret("password")
    nm_host = 'localhost'
    nm_port = '5556' 
    domain_name = self.env.getDomain().getName()
    domain_home = self.env.getDomainHome()
    isNodeManager = "true"
    userConfigFile = self.env.USERCONFIG_FILE
    userKeyFileBin = self.env.USERKEY_FILE + '.bin'

    trace("nmConnect " + username + ", " + nm_host + ", " + nm_port + ", " + domain_name + ", " + domain_home)
    nmConnect(username, password, nm_host, nm_port, domain_name, domain_home, 'plain')
    try:
      # the following storeUserConfig WLST command generates two files:
      storeUserConfig(userConfigFile, userKeyFileBin, isNodeManager)

      # user config is already a text file, so directly add it to the generated file list
      self.env.addGeneratedFile(userConfigFile)

      # but key is a binary, so we b64 it - and the caller of this method will add the file
      userKey = self.env.readBinaryFile(userKeyFileBin)
      b64 = ""
      for s in base64.encodestring(userKey).splitlines():
        b64 = b64 + s
      self.writeln(b64)
    finally:
      nmDisconnect()

class MII_DomainConfigGenerator(Generator):

  def __init__(self, env):
    Generator.__init__(self, env, env.MII_DOMAIN_ZIP)
    self.env = env
    self.domain_home = self.env.getDomainHome()
  def generate(self):
    self.open()
    try:
      self.addDomainConfig()
      self.close()
      self.addGeneratedFile()
    finally:
      self.close()

  def addDomainConfig(self):
    kubernetes_platform = self.env.getEnvOrDef("KUBERNETES_PLATFORM", "")
    if (str(kubernetes_platform).upper() == 'OPENSHIFT'):
      os.system("chmod -R g=u %s" % self.domain_home)

    # Note: only config type is needed fmwconfig, security is excluded because it's in the primordial and contain
    # all the many policies files
    packcmd = "tar -pczf /tmp/domain.tar.gz %s/config/config.xml %s/config/jdbc/ %s/config/jms %s/config/coherence " \
              "%s/config/diagnostics %s/config/startup %s/config/configCache %s/config/nodemanager " \
              "%s/config/security %s/config/fmwconfig/servers/*/logging.xml" % (
              self.domain_home, self.domain_home, self.domain_home, self.domain_home, self.domain_home,
              self.domain_home, self.domain_home, self.domain_home, self.domain_home, self.domain_home)
    os.system(packcmd)
    domain_data = self.env.readBinaryFile("/tmp/domain.tar.gz")
    b64 = ""
    for s in base64.encodestring(domain_data).splitlines():
      b64 = b64 + s
    self.writeln(b64)
    domainzip_hash = md5.new(domain_data).hexdigest()
    fh = open("/tmp/domainzip_hash", "w")
    fh.write(domainzip_hash)
    fh.close()
    trace('done zipping up domain ')


class MII_OpssWalletFileGenerator(Generator):

  def __init__(self, env):
    Generator.__init__(self, env, env.MII_JRF_EWALLET)
    self.env = env
    self.domain_home = self.env.getDomainHome()
  def generate(self):
    self.open()
    try:
      self.addWallet()
      self.close()
      self.addGeneratedFile()
    finally:
      self.close()

  def addWallet(self):
    wallet_data = self.env.readBinaryFile("/tmp/opsswallet/ewallet.p12")
    b64 = ""
    for s in base64.encodestring(wallet_data).splitlines():
      b64 = b64 + s
    self.writeln(b64)
    trace("done writing opss key")


class MII_PrimordialDomainGenerator(Generator):

  def __init__(self, env):
    Generator.__init__(self, env, env.MII_PRIMORDIAL_DOMAIN_ZIP)
    self.env = env
    self.domain_home = self.env.getDomainHome()
  def generate(self):
    self.open()
    try:
      self.addPrimordialDomain()
      self.close()
      self.addGeneratedFile()
    finally:
      self.close()

  def addPrimordialDomain(self):
    primordial_domain_data = self.env.readBinaryFile("/tmp/prim_domain.tar.gz")
    b64 = ""
    for s in base64.encodestring(primordial_domain_data).splitlines():
      b64 = b64 + s
    self.writeln(b64)
    trace("done writing primordial domain")


class MII_IntrospectCMFileGenerator(Generator):

  def __init__(self, env, inventory, fromfile):
    Generator.__init__(self, env, inventory)
    self.env = env
    self.fromfile = fromfile

  def generate(self):
    self.open()
    try:
      rc = self.addFile()
      self.close()
      if rc is not None:
        self.addGeneratedFile()
    finally:
      self.close()

  def addFile(self):
    if os.path.exists(self.fromfile):
      file_str = self.env.readFile(self.fromfile)
      self.writeln(file_str)
      return "hasfile"
    else:
      return None


class SitConfigGenerator(Generator):

  def __init__(self, env):
    Generator.__init__(self, env, env.CM_FILE)

  def generate(self):
    self.open()
    try:
      self.addSitCfg()
      self.close()
      self.addGeneratedFile()
    finally:
      self.close()

  def addSitCfg(self):
    self.addSitCfgXml()

  def addSitCfgXml(self):
    self.writeln("<?xml version='1.0' encoding='UTF-8'?>")
    self.writeln("<d:domain xmlns:d=\"http://xmlns.oracle.com/weblogic/domain\" xmlns:f=\"http://xmlns.oracle.com/weblogic/domain-fragment\" xmlns:s=\"http://xmlns.oracle.com/weblogic/situational-config\">")
    self.indent()
    self.writeln("<s:expiration> 2099-07-16T19:20+01:00 </s:expiration>")
    #self.writeln("<d:name>" + self.env.DOMAIN_NAME + "</d:name>")
    self.customizeNodeManagerCreds()
    self.customizeDomainLogPath()
    self.customizeCustomFileStores()
    self.customizeServers()
    self.customizeServerTemplates()
    self.customizeIstioClusters()
    self.undent()
    self.writeln("</d:domain>")

  def customizeNodeManagerCreds(self):
    username = self.readCredentialsSecret('username')
    password = self.encrypt(self.readCredentialsSecret('password'))
    self.writeln("<d:security-configuration>")
    self.indent()
    self.writeln("<d:node-manager-user-name f:combine-mode=\"replace\">" + username + "</d:node-manager-user-name>")
    self.writeln("<d:node-manager-password-encrypted f:combine-mode=\"replace\">" + password + "</d:node-manager-password-encrypted>")
    self.undent()
    self.writeln("</d:security-configuration>")

  def customizeDomainLogPath(self):
    self.customizeLog(self.env.getDomain().getName(), self.env.getDomain(), true)

  def customizeCustomFileStores(self):
    self.customizeFileStores(self.env.getDomain())

  def customizeServers(self):
    for server in self.env.getDomain().getServers():
      self.customizeServer(server)

  def customizeIstioClusters(self):
    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    if istio_enabled == 'false':
      return
    for cluster in self.env.getDomain().getClusters():
      repl_channel_name = cluster.getReplicationChannel()
      # Skip configuring replication channel if 'istiorepl' already defined
      # or replication chcannel is not the default name 'ReplicationChannel'
      # which basically means not configured.
      if repl_channel_name is not None and (repl_channel_name == 'istiorepl'
          or repl_channel_name != 'ReplicationChannel'):
        continue
      self.writeln("<d:cluster>")
      self.indent()
      self.writeln("<d:name>" + cluster.getName() + "</d:name>")
      self.writeln("<d:replication-channel f:combine-mode='add'>" + 'istiorepl' + "</d:replication-channel>")
      self.undent()
      self.writeln("</d:cluster>")

  def writeListenAddress(self, originalValue, newValue):
    repVerb="\"replace\""
    if originalValue is None or len(originalValue)==0:
      repVerb="\"add\""
    self.writeln("<d:listen-address f:combine-mode=" + repVerb + ">" + newValue + "</d:listen-address>")

  def writeTwoWaySSLEnabled(self, nap):
    # default twoWaySSLEnabled = 'false'
    if nap.isTwoWaySSLEnabled() == true:
      twoWaySSLEnabled = 'true'
      self.writeln("<d:two-way-ssl-enabled f:combine-mode=\"add\">" + twoWaySSLEnabled + "</d:two-way-ssl-enabled>")

  def writeClientCertificateEnforced(self, nap):
    # default clientCertificateEnforced = 'false'
    if nap.isClientCertificateEnforced() == true:
      clientCertificateEnforced = 'true'
      self.writeln("<d:client-certificate-enforced f:combine-mode=\"add\">" + clientCertificateEnforced + "</d:client-certificate-enforced>")

  def writeChannelIdentityCustomized(self, nap):
    # default channelIdentityCustomized = 'false'
    if nap.isChannelIdentityCustomized() == true:
      channelIdentityCustomized = 'true'
      self.writeln("<d:channel-identity-customized f:combine-mode=\"add\">" + channelIdentityCustomized + "</d:channel-identity-customized>")

  def writeCustomPrivateKeyAlias(self, nap):
    customPrivateKeyAlias = nap.getCustomPrivateKeyAlias()
    if customPrivateKeyAlias is not None:
      self.writeln("<d:custom-private-key-alias f:combine-mode=\"add\">" + customPrivateKeyAlias + "</d:custom-private-key-alias>")

  def writeCustomPrivateKeyPassPhraseEncrypted(self, nap):
    customPriveKeyPassPhraseEncrypted = nap.getCustomPrivateKeyPassPhraseEncrypted()
    if customPriveKeyPassPhraseEncrypted is not None and len(customPriveKeyPassPhraseEncrypted) > 0:
      self.writeln("<d:custom-private-key-pass-phrase-encrypted f:combine-mode=\"add\">" + customPriveKeyPassPhraseEncrypted + "</d:custom-private-key-pass-phrase-encrypted>")

  def writeCustomIdentityKeyStoreType(self, nap):
    customIdentityKeyStoreType = nap.getCustomIdentityKeyStoreType()
    if customIdentityKeyStoreType is not None and len(customIdentityKeyStoreType) > 0:
      self.writeln("<d:custom-identity-key-store-type f:combine-mode=\"add\">" + customIdentityKeyStoreType + "</d:custom-identity-key-store-type>")

  def writeCustomIdentityKeyStorePassPhraseEncrypted(self, nap):
    customIdentityKeyStorePassPhraseEncrypted = nap.getCustomIdentityKeyStorePassPhraseEncrypted()
    if customIdentityKeyStorePassPhraseEncrypted is not None and len(customIdentityKeyStorePassPhraseEncrypted) > 0:
      self.writeln("<d:custom-identity-key-store-pass-phrase-encrypted f:combine-mode=\"add\">" + customIdentityKeyStorePassPhraseEncrypted + "</d:custom-identity-key-store-pass-phrase-encrypted>")

  def writeHostnameVerificationIgnored(self, nap):
    # default hostnameVerificationIgnored = 'false'
    if nap.isHostnameVerificationIgnored() == true:
      hostnameVerificationIgnored = 'true'
      self.writeln("<d:hostname-verification-ignored f:combine-mode=\"add\">" + hostnameVerificationIgnored + "</d:hostname-verification-ignored>")

  def writeHostnameVerifier(self, nap):
    hostnameVerifier = nap.getHostnameVerifier()
    if hostnameVerifier is not None and len(hostnameVerifier) > 0:
      self.writeln("<d:hostname-verifier f:combine-mode=\"add\">" + hostnameVerifier + "</d:hostname-verifier>")

  def writeCiphersuites(self, nap):
    ciphersuites = nap.getCiphersuites()
    if ciphersuites is not None:
      for cipher in ciphersuites:
        self.writeln("<d:ciphersuite f:combine-mode=\"add\">" + cipher + "</d:ciphersuite>")

  def writeAllowUnencryptedNullCipher(self, nap):
    # default allowUnencryptedNullCipher = 'false'
    if nap.isAllowUnencryptedNullCipher() == true:
      allowUnencryptedNullCipher = 'true'
      self.writeln("<d:allow-unencrypted-null-cipher f:combine-mode=\"add\">" + allowUnencryptedNullCipher + "</d:allow-unencrypted-null-cipher>")

  def writeInboundCertificateValidation(self, nap):
    inboundCertificateValidation = nap.getInboundCertificateValidation()
    if inboundCertificateValidation is not None and len(inboundCertificateValidation) > 0:
      self.writeln("<d:inbound-certificate-validation f:combine-mode=\"add\">" + inboundCertificateValidation + "</d:inbound-certificate-validation>")

  def writeOutboundCertificateValidation(self, nap):
    outboundCertificateValidation = nap.getOutboundCertificateValidation()
    if outboundCertificateValidation is not None and len(outboundCertificateValidation) > 0:
      self.writeln("<d:outbound-certificate-validation f:combine-mode=\"add\">" + outboundCertificateValidation + "</d:outbound-certificate-validation>")


  def customizeServer(self, server):
    name=server.getName()
    listen_address=self.env.toDNS1123Legal(self.env.getDomainUID() + "-" + name)
    admin_server_name = self.env.getDomain().getAdminServerName()
    self.writeln("<d:server>")
    self.indent()
    self.writeln("<d:name>" + name + "</d:name>")
    self.customizeLog(name, server, false)
    self.customizeAccessLog(name)
    self.customizeDefaultFileStore(server)
    self.writeListenAddress(server.getListenAddress(),listen_address)
    self.customizeNetworkAccessPoints(server,listen_address)
    self.customizeServerIstioNetworkAccessPoint(listen_address, server)
    if server.getName() == admin_server_name:
      self.addAdminChannelPortForwardNetworkAccessPoints(server)
    else:
      self.customizeIstioReplicationChannel(server, listen_address,
                                            self.env.getEnvOrDef("ISTIO_REPLICATION_PORT", 4564),
                                            is_server_template=False)
    if self.getCoherenceClusterSystemResourceOrNone(server) is not None:
      self.customizeCoherenceMemberConfig(server.getCoherenceMemberConfig(),listen_address)
    self.undent()
    self.writeln("</d:server>")

  def customizeServerTemplates(self):
    for template in self.env.getDomain().getServerTemplates():
      if not (self.env.getClusterOrNone(template) is None):
        self.customizeServerTemplate(template)

  def customizeServerTemplate(self, template):
    name=template.getName()
    server_name_prefix=template.getCluster().getDynamicServers().getServerNamePrefix()
    listen_address=self.env.toDNS1123Legal(self.env.getDomainUID() + "-" + server_name_prefix + "${id}")
    self.writeln("<d:server-template>")
    self.indent()
    self.writeln("<d:name>" + name + "</d:name>")
    self.customizeLog(server_name_prefix + "${id}", template, false)
    self.customizeAccessLog(server_name_prefix + "${id}")
    self.customizeDefaultFileStore(template)
    self.writeListenAddress(template.getListenAddress(),listen_address)
    self.customizeNetworkAccessPoints(template,listen_address)
    self.customizeManagedIstioNetworkAccessPoint(listen_address, template)
    self.customizeIstioReplicationChannel(template, listen_address,
                                          self.env.getEnvOrDef("ISTIO_REPLICATION_PORT", 4564),
                                          is_server_template=True)
    if self.getCoherenceClusterSystemResourceOrNone(template) is not None:
      self.customizeCoherenceMemberConfig(template.getCoherenceMemberConfig(), listen_address)
    self.undent()
    self.writeln("</d:server-template>")

  def customizeNetworkAccessPoints(self, server, listen_address):
    for nap in server.getNetworkAccessPoints():
      self.customizeNetworkAccessPoint(nap,listen_address)

  def customizeNetworkAccessPoint(self, nap, listen_address):
    # Don't bother 'add' a nap listen-address, only do a 'replace'.
    # If we try 'add' this appears to mess up an attempt to 
    #   'add' PublicAddress/Port via custom sit-cfg.
    # FWIW there's theoretically no need to 'add' or 'replace' when empty
    #   since the runtime default is the server listen-address.

    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    istio_use_localhost_bindings = self.env.getEnvOrDef("ISTIO_USE_LOCALHOST_BINDINGS", "true")
    if istio_enabled  == 'true' and istio_use_localhost_bindings == 'true':
      listen_address = '127.0.0.1'

    nap_name=nap.getName()
    if nap_name in ISTIO_NAP_NAMES:
      # skip customizing internal channels that were generated
      return

    # replace listen address to bind to server pod IP
    if not (nap.getListenAddress() is None) and len(nap.getListenAddress()) > 0:
        self.writeln("<d:network-access-point>")
        self.indent()
        self.writeln("<d:name>" + nap_name + "</d:name>")
        self.writeListenAddress("force a replace",listen_address)
        self.undent()
        self.writeln("</d:network-access-point>")

  def writeSecureNetworkAccessPointConfiguration(self, nap):
    protocol = nap.getProtocol()
    if protocol.endswith('s') or protocol == 'admin':
      self.writeTwoWaySSLEnabled(nap)
      self.writeClientCertificateEnforced(nap)
      self.writeChannelIdentityCustomized(nap)
      self.writeCustomPrivateKeyAlias(nap)
      self.writeCustomPrivateKeyPassPhraseEncrypted(nap)
      self.writeCustomIdentityKeyStoreType(nap)
      self.writeCustomIdentityKeyStorePassPhraseEncrypted(nap)
      self.writeHostnameVerificationIgnored(nap)
      self.writeHostnameVerifier(nap)
      self.writeCiphersuites(nap)
      self.writeAllowUnencryptedNullCipher(nap)
      self.writeInboundCertificateValidation(nap)
      self.writeOutboundCertificateValidation(nap)


  def _getNapConfigOverrideAction(self, svr, testname):
    replace_action = 'f:combine-mode="replace"'
    add_action = 'f:combine-mode="add"'
    found = False
    for nap in svr.getNetworkAccessPoints():
      if nap.getName() == testname:
        found = True
        break

    if found:
      trace("SEVERE","Found NetworkAccessPoint with name %s in the WebLogic Domain, this is an internal name used by the WebLogic Kubernetes Operator, please remove it from your domain and try again." % testname)
      sys.exit(1)
    else:
      return add_action, "add"

  def _writeIstioNAP(self, name, server, listen_address, listen_port, protocol,
        http_enabled="true", bind_to_localhost="true", use_fast_serialization='false',
        tunneling_enabled='false', outbound_enabled='false'):

    action, type = self._getNapConfigOverrideAction(server, "http-probe")

    # For add, we must put the combine mode as add
    # For replace, we must omit it
    if type == "add":
      self.writeln('<d:network-access-point %s>' % action)
    else:
      self.writeln('<d:network-access-point>')

    self.indent()
    if type == "add":
      self.writeln('<d:name %s>%s</d:name>' % (action, name))
    else:
      self.writeln('<d:name>%s</d:name>' % name)

    self.writeln('<d:protocol %s>%s</d:protocol>' % (action, protocol))
    if bind_to_localhost == "true":
      self.writeln('<d:listen-address %s>127.0.0.1</d:listen-address>' % action)
    else:
      self.writeln('<d:listen-address %s>%s</d:listen-address>' % (action, listen_address))
    self.writeln('<d:public-address %s>%s.%s</d:public-address>' % (action, listen_address,
                                                          self.env.getEnvOrDef("ISTIO_POD_NAMESPACE", "default")))
    self.writeln('<d:listen-port %s>%s</d:listen-port>' % (action, listen_port))
    self.writeln('<d:http-enabled-for-this-protocol %s>%s</d:http-enabled-for-this-protocol>' %
                 (action, http_enabled))
    # This needs to be enabled, since we are splitting from server default channel
    self.writeln('<d:outbound-enabled %s>%s</d:outbound-enabled>' % (action, outbound_enabled))
    self.writeln('<d:enabled %s>true</d:enabled>' % action)
    self.writeln('<d:use-fast-serialization %s>%s</d:use-fast-serialization>' % (action, use_fast_serialization))
    self.writeln('<d:tunneling-enabled %s>%s</d:tunneling-enabled>' % (action, tunneling_enabled))
    self.undent()
    self.writeln('</d:network-access-point>')

  def _getPortForwardNapConfigOverrideAction(self, svr, testname):
    add_action = 'f:combine-mode="add"'
    found = False
    for nap in svr.getNetworkAccessPoints():
      if nap.getName() == testname:
        found = True
        break

    if found:
      trace("SEVERE","Found NetworkAccessPoint with name %s in the WebLogic Domain, this is an internal name used by the WebLogic Kubernetes Operator, please remove it from your domain and try again." % testname)
      sys.exit(1)
    else:
      return add_action, "add"

  def _writeAdminChannelPortForwardNAP(self, name, server, listen_port, protocol):
    action, type = self._getPortForwardNapConfigOverrideAction(server, name)

    # For add, we must put the combine mode as add
    # For replace, we must omit it
    if type == "add":
      self.writeln('<d:network-access-point %s>' % action)
    else:
      self.writeln('<d:network-access-point>')

    self.indent()
    if type == "add":
      self.writeln('<d:name %s>%s</d:name>' % (action, name))
    else:
      self.writeln('<d:name>%s</d:name>' % name)

    self.indent()
    self.writeln('<d:name %s>%s</d:name>' % (action, name))

    self.writeln('<d:protocol %s>%s</d:protocol>' % (action, protocol))
    self.writeln('<d:listen-address %s>localhost</d:listen-address>' % action)
    self.writeln('<d:listen-port %s>%s</d:listen-port>' % (action, listen_port))
    self.writeln('<d:http-enabled-for-this-protocol %s>true</d:http-enabled-for-this-protocol>' %
                 (action))
    self.writeln('<d:outbound-enabled %s>false</d:outbound-enabled>' % action)
    self.writeln('<d:enabled %s>true</d:enabled>' % action)
    self.undent()
    self.writeln('</d:network-access-point>')

  def _writeIstioReplicationChannelNAP(self, name, listen_address, listen_port,
                                       bind_to_localhost="true"):
    action = 'f:combine-mode="add"'
    self.writeln('<d:network-access-point %s>' % action)
    self.indent()
    self.writeln('<d:name %s>%s</d:name>' % (action, name))
    self.writeln('<d:protocol %s>%s</d:protocol>' % (action, 't3'))
    if bind_to_localhost == "true":
      self.writeln('<d:listen-address %s>127.0.0.1</d:listen-address>' % action)
    else:
      self.writeln('<d:listen-address %s>%s</d:listen-address>' % (action, listen_address))
    self.writeln('<d:public-address %s>%s.%s</d:public-address>' % (action, listen_address,
                                    self.env.getEnvOrDef("ISTIO_POD_NAMESPACE", "default")))
    self.writeln('<d:listen-port %s>%s</d:listen-port>' % (action, listen_port))
    self.writeln('<d:enabled %s>true</d:enabled>' % action)
    self.writeln('<d:outbound-enabled %s>false</d:outbound-enabled>' % action)
    self.writeln('<d:use-fast-serialization %s>true</d:use-fast-serialization>' % action)
    self.undent()
    self.writeln('</d:network-access-point>')

  def getCoherenceClusterSystemResourceOrNone(self, serverOrTemplate):
    try:
      cluster=serverOrTemplate.getCluster()
      if (cluster is not None):
        ret=cluster.getCoherenceClusterSystemResource()
      else:
        ret=serverOrTemplate.getCoherenceClusterSystemResource()
    except:
      trace("Ignoring getCoherenceClusterSystemResource () exception, this is expected.")
      ret = None
    return ret

  def customizeCoherenceMemberConfig(self, coherence_member_config, listen_address):
    repVerb='"add"'
    if (coherence_member_config is None):
      self.writeln('<d:coherence-member-config f:combine-mode=' + repVerb + '>')
      self.indent()
      self.writeln('<d:unicast-listen-address f:combine-mode=' + repVerb + '>%s</d:unicast-listen-address>' % listen_address)
      self.undent()
      self.writeln('</d:coherence-member-config>')
    else:
      unicastAddress=coherence_member_config.getUnicastListenAddress()
      if unicastAddress is not None:
        repVerb='"replace"'

      self.writeln('<d:coherence-member-config>')
      self.indent()
      self.writeln('<d:unicast-listen-address f:combine-mode=' + repVerb + '>%s</d:unicast-listen-address>' % listen_address)
      self.undent()
      self.writeln('</d:coherence-member-config>')

  def customizeServerIstioNetworkAccessPoint(self, listen_address, server):
    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    if istio_enabled == 'false':
      return
    istio_readiness_port = self.env.getEnvOrDef("ISTIO_READINESS_PORT", None)
    if istio_readiness_port is None:
      return
    admin_server_port = getRealListenPort(server)
    # readiness probe
    self._writeIstioNAP(name='http-probe', server=server, listen_address=listen_address,
                        listen_port=istio_readiness_port, protocol='http', http_enabled="true")

    # Generate NAP for each protocols
    if self.env.getEnvOrDef("ISTIO_USE_LOCALHOST_BINDINGS", "true") == 'true':
      self._writeIstioNAP(name='tcp-ldap', server=server, listen_address=listen_address,
                          listen_port=admin_server_port, protocol='ldap')

      self._writeIstioNAP(name='tcp-default', server=server, listen_address=listen_address,
                          listen_port=admin_server_port, protocol='t3')

      self._writeIstioNAP(name='http-default', server=server, listen_address=listen_address,
                          listen_port=admin_server_port, protocol='http')

      self._writeIstioNAP(name='tcp-snmp', server=server, listen_address=listen_address,
                          listen_port=admin_server_port, protocol='snmp')

      self._writeIstioNAP(name='tcp-cbt', server=server, listen_address=listen_address,
                          listen_port=admin_server_port, protocol='CLUSTER-BROADCAST')

      self._writeIstioNAP(name='tcp-iiop', server=server, listen_address=listen_address,
                          listen_port=admin_server_port, protocol='iiop')

      ssl_listen_port = getSSLPortIfEnabled(server, self.env.getDomain(), is_server_template=False)

      if ssl_listen_port is not None:
        self._writeIstioNAP(name='https-secure', server=server, listen_address=listen_address,
                          listen_port=ssl_listen_port, protocol='https', http_enabled="true")

        self._writeIstioNAP(name='tls-ldaps', server=server, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='ldaps')

        self._writeIstioNAP(name='tls-default', server=server, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='t3s')

        self._writeIstioNAP(name='tls-cbts', server=server, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='CLUSTER-BROADCAST-SECURE')

        self._writeIstioNAP(name='tls-iiops', server=server, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='iiops')

      if isAdministrationPortEnabledForServer(server, self.env.getDomain()):
        self._writeIstioNAP(name='https-admin', server=server, listen_address=listen_address,
                            listen_port=getAdministrationPort(server, self.env.getDomain()), protocol='https', http_enabled="true")
    else:
      self._writeIstioNAP(name='http-probe-ext', server=server, listen_address=listen_address,
                          listen_port=istio_readiness_port, protocol='http', http_enabled="true",
                          bind_to_localhost="false")

  def customizeManagedIstioNetworkAccessPoint(self, listen_address, template):
    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    if istio_enabled == 'false':
      return
    istio_readiness_port = self.env.getEnvOrDef("ISTIO_READINESS_PORT", None)
    if istio_readiness_port is None:
      return
    listen_port = getRealListenPort(template)
    self._writeIstioNAP(name='http-probe', server=template, listen_address=listen_address,
                        listen_port=istio_readiness_port, protocol='http')

    if self.env.getEnvOrDef("ISTIO_USE_LOCALHOST_BINDINGS", "true") == 'true':
      self._writeIstioNAP(name='tcp-default', server=template, listen_address=listen_address,
                          listen_port=listen_port, protocol='t3', http_enabled='false')

      self._writeIstioNAP(name='http-default', server=template, listen_address=listen_address,
                          listen_port=listen_port, protocol='http')

      self._writeIstioNAP(name='tcp-snmp', server=template, listen_address=listen_address,
                          listen_port=listen_port, protocol='snmp')

      self._writeIstioNAP(name='tcp-cbt', server=template, listen_address=listen_address,
                          listen_port=listen_port, protocol='CLUSTER-BROADCAST')

      self._writeIstioNAP(name='tcp-iiop', server=template, listen_address=listen_address,
                          listen_port=listen_port, protocol='iiop')

      ssl_listen_port = getSSLPortIfEnabled(template, self.env.getDomain())

      if ssl_listen_port is not None:
        self._writeIstioNAP(name='https-secure', server=template, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='https')

        self._writeIstioNAP(name='tls-ldaps', server=template, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='ldaps')

        self._writeIstioNAP(name='tls-default', server=template, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='t3s', http_enabled='false')

        self._writeIstioNAP(name='tls-cbts', server=template, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='CLUSTER-BROADCAST-SECURE')

        self._writeIstioNAP(name='tls-iiops', server=template, listen_address=listen_address,
                            listen_port=ssl_listen_port, protocol='iiops')
    else:
      self._writeIstioNAP(name='http-probe-ext', server=template, listen_address=listen_address,
                         listen_port=istio_readiness_port, protocol='http', bind_to_localhost="false")

  def addAdminChannelPortForwardNetworkAccessPoints(self, server):
    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    istio_use_localhost_bindings = self.env.getEnvOrDef("ISTIO_USE_LOCALHOST_BINDINGS", "true")
    admin_channel_port_forward_enabled = self.env.getEnvOrDef("ADMIN_CHANNEL_PORT_FORWARDING_ENABLED", "true")
    if (admin_channel_port_forward_enabled == 'false') or \
        (istio_enabled == 'true' and istio_use_localhost_bindings == 'true'):
      return

    index = 0
    for nap in server.getNetworkAccessPoints():
      if nap.getProtocol() == "admin":
        index += 1
        port=nap.getListenPort()
        self._writeAdminChannelPortForwardNAP(name='internal-admin' + str(index), server=server,
                                              listen_port=port, protocol='admin')

    if isAdministrationPortEnabledForServer(server, self.env.getDomain()):
      self._writeAdminChannelPortForwardNAP(name='internal-admin', server=server,
                                            listen_port=getAdministrationPort(server, self.env.getDomain()),
                                            protocol='admin')
    elif index == 0:
      admin_server_port = getRealListenPort(server)
      self._writeAdminChannelPortForwardNAP(name='internal-t3', server=server,
                                            listen_port=admin_server_port, protocol='t3')

      ssl_listen_port = getSSLPortIfEnabled(server, self.env.getDomain(), is_server_template=False)

      if ssl_listen_port is not None:
        self._writeAdminChannelPortForwardNAP(name='internal-t3s', server=server,
                                              listen_port=ssl_listen_port, protocol='t3s')

  def customizeIstioReplicationChannel(self, server, listen_address, listen_port, is_server_template):
    istio_enabled = self.env.getEnvOrDef("ISTIO_ENABLED", "false")
    if istio_enabled == 'false' or server.getCluster() is None :
      return

    repl_channel_name = server.getCluster().getReplicationChannel()
    if repl_channel_name is not None and (repl_channel_name == 'istiorepl'
                                          or repl_channel_name != 'ReplicationChannel'):
      return

    self._verify_replication_port_conflict(server, listen_port, is_server_template)

    bind_to_localhost = 'false'
    if self.env.getEnvOrDef("ISTIO_USE_LOCALHOST_BINDINGS", "true") == 'true':
      bind_to_localhost = 'true'

    self._writeIstioReplicationChannelNAP('istiorepl', listen_address, listen_port,
                                          bind_to_localhost)

  def _verify_replication_port_conflict(self, server, replication_port, is_server_template):
    name = server.getName()
    listen_port = getRealListenPort(server)
    ssl_listen_port = getSSLPortIfEnabled(server, self.env.getDomain(), is_server_template)
    if listen_port == replication_port:
      raise_replication_port_conflict(name, listen_port, replication_port, '')
    if ssl_listen_port == replication_port:
      raise_replication_port_conflict(name, ssl_listen_port, replication_port, 'SSL')

    if server.getNetworkAccessPoints() is not None:
      for nap in server.getNetworkAccessPoints():
        nap_name = nap.getName()
        listen_port = getRealListenPort(nap)
        ssl_listen_port = getSSLPortIfEnabled(nap, self.env.getDomain(), is_server_template)

        if listen_port == replication_port:
          raise_replication_port_conflict(nap_name, listen_port, replication_port, '')

        if ssl_listen_port == replication_port:
          raise_replication_port_conflict(nap_name, ssl_listen_port, replication_port, 'SSL')


  def getLogOrNone(self,server):
    try:
      ret = server.getLog()
    except:
      trace("Ignoring getLog() exception, this is expected.")
      ret = None
    return ret

  def customizeLog(self, name, bean, isDomainBean):
    logs_dir = self.env.getDomainLogHome()
    if logs_dir is None or len(logs_dir) == 0:
      return

    logaction=''
    fileaction=''
    log = self.getLogOrNone(bean)
    if log is None:
      if not isDomainBean:
        # don't know why, but don't need to "add" a missing domain log bean, and adding it causes trouble
        logaction=' f:combine-mode="add"'
      fileaction=' f:combine-mode="add"'
    else:
      if log.getFileName() is None:
        fileaction=' f:combine-mode="add"'
      else:
        fileaction=' f:combine-mode="replace"'

    self.writeln("<d:log" + logaction + ">")
    self.indent()
    self.writeln("<d:file-name" + fileaction + ">" + logs_dir + "/" + name + ".log</d:file-name>")
    self.undent()
    self.writeln("</d:log>")

  def customizeFileStores(self, domain):
    data_dir = self.env.getDataHome()
    if data_dir is None or len(data_dir) == 0:
      # do not override if dataHome not specified or empty ("")
      return

    for filestore in domain.getFileStores():
      self.customizeFileStore(filestore, data_dir)


  def customizeFileStore(self, filestore, data_dir):
    fileaction=''
    if filestore.getDirectory() is None:
      fileaction=' f:combine-mode="add"'
    else:
      fileaction=' f:combine-mode="replace"'

    self.writeln("<d:file-store>")
    self.indent()
    self.writeln("<d:name>" + filestore.getName() + "</d:name>")
    self.writeln("<d:directory"+ fileaction + ">" + data_dir + "</d:directory>")
    self.undent()
    self.writeln("</d:file-store>")

  def customizeDefaultFileStore(self, bean):
    data_dir = self.env.getDataHome()
    if data_dir is None or len(data_dir) == 0:
      # do not override if dataHome not specified or empty ("")
      return

    dfsaction=''
    fileaction=''
    if bean.getDefaultFileStore() is None:
      # don't know why, but don't need to "add" a missing default file store bean, and adding it causes trouble
      dfsaction=' f:combine-mode="add"'
      fileaction=' f:combine-mode="add"'
    else:
      if bean.getDefaultFileStore().getDirectory() is None:
        fileaction=' f:combine-mode="add"'
      else:
        fileaction=' f:combine-mode="replace"'

    self.writeln("<d:default-file-store" + dfsaction + ">")
    self.indent()
    self.writeln("<d:directory" + fileaction + ">" + data_dir + "</d:directory>")
    self.undent()
    self.writeln("</d:default-file-store>")

  def customizeAccessLog(self, name):
    # do not customize if LOG_HOME is not set
    logs_dir = self.env.getDomainLogHome()
    if logs_dir is None or len(logs_dir) == 0:
      return

    # customize only if ACCESS_LOG_IN_LOG_HOME is 'true'
    if self.env.isAccessLogInLogHome():
      self.writeln("<d:web-server>")
      self.indent()
      self.writeln("<d:web-server-log>")
      self.indent()
      # combine-mode "replace" works regardless of whether web-server and web-server-log is present or not
      self.writeln("<d:file-name f:combine-mode=\"replace\">"
                   + logs_dir + "/" + name + "_access.log</d:file-name>")
      self.undent()
      self.writeln("</d:web-server-log>")
      self.undent()
      self.writeln("</d:web-server>")

class CustomSitConfigIntrospector(SecretManager):

  def __init__(self, env):
    SecretManager.__init__(self, env)
    self.env = env
    self.macroMap={}
    self.macroStr=''
    self.moduleMap={}
    self.moduleStr=''

    # Populate macro map with known secrets and env vars, log them
    #   env macro format:         'env:<somename>'
    #   plain text secret macro:  'secret:<somename>'
    #   encrypted secret macro:   'secret:<somename>:encrypt'

    if os.path.exists(self.env.CUSTOM_SECRET_ROOT):
      for secret_name in os.listdir(self.env.CUSTOM_SECRET_ROOT):
        secret_path = os.path.join(self.env.CUSTOM_SECRET_ROOT, secret_name)
        self.addSecretsFromDirectory(secret_path, secret_name)

    self.addSecretsFromDirectory(self.env.CREDENTIALS_SECRET_PATH, 
                                 self.env.CREDENTIALS_SECRET_NAME)

    self.macroMap['env:DOMAIN_UID']  = self.env.DOMAIN_UID
    self.macroMap['env:DOMAIN_HOME'] = self.env.DOMAIN_HOME
    self.macroMap['env:LOG_HOME']    = self.env.LOG_HOME
    self.macroMap['env:DOMAIN_NAME'] = self.env.DOMAIN_NAME

    keys=self.macroMap.keys()
    keys.sort()
    for key in keys:
      val=self.macroMap[key]
      if self.macroStr:
        self.macroStr+=', '
      self.macroStr+='${' + key + '}'

    trace("Available macros: '" + self.macroStr + "'")

    # Populate module maps with known module files and names, log them

    self.jdbcModuleStr = self.buildModuleTable(
                           'jdbc', 
                           self.env.getDomain().getJDBCSystemResources(),
                           self.env.CUSTOM_PREFIX_JDBC)

    self.jmsModuleStr = self.buildModuleTable(
                           'jms', 
                           self.env.getDomain().getJMSSystemResources(),
                           self.env.CUSTOM_PREFIX_JMS)

    self.wldfModuleStr = self.buildModuleTable(
                           'diagnostics', 
                           self.env.getDomain().getWLDFSystemResources(),
                           self.env.CUSTOM_PREFIX_WLDF)

    trace('Available modules: ' + self.moduleStr)


  def addSecretsFromDirectory(self, secret_path, secret_name):
    if not os.path.isdir(secret_path):
      # The operator pod somehow put a file where we
      # only expected to find a directory mount.
      self.env.addError("Internal Error:  Secret path'" 
                        + secret_path + "'" +
                        + " is not a directory.")
      return
    for the_file in os.listdir(secret_path):
      the_file_path = os.path.join(secret_path, the_file)
      if os.path.isfile(the_file_path):
        val=self.env.readFile(the_file_path)
        key='secret:' + secret_name + "." + the_file
        self.macroMap[key] = val
        self.macroMap[key + ':encrypt'] = self.env.encrypt(val)


  def buildModuleTable(self, moduleTypeStr, moduleResourceBeans, customPrefix):

    # - Populate global 'moduleMap' with key of 'moduletype-modulename.xml'
    #   andvalue of 'module system resource file name' + '-situational-config.xml'.
    # - Populate global 'moduleStr' with list of known modules.
    # - Generate validation error if a module is not located in a config subdirectory
    #   that matches its type (e.g. jdbc modules are expected to be in directory 'jdbc').

    if self.moduleStr:
      self.moduleStr += ', '
    self.moduleStr += 'type.' + moduleTypeStr + "=("
    firstModule=true

    for module in moduleResourceBeans:

      mname=module.getName()
      mfile=module.getDescriptorFileName()

      if os.path.dirname(mfile) != moduleTypeStr:
        self.env.addError(
          "Error, the operator expects module files of type '" + moduleTypeStr + "'"
          + " to be located in directory '" + moduleTypeStr + "/'"
          + ", but the " + moduleTypeStr + " system resource module '" + mname + "'"
          + " is configured with DescriptorFileName='" + mfile + "'.")      

      if mfile.count(".xml") != 1 or mfile.find(".xml") + 4 != len(mfile):
        self.env.addError(
          "Error, the operator expects system resource module files"
          + " to end in '.xml'"
          + ", but the " + moduleTypeStr + " system resource module '" + mname + "'"
          + " is configured with DescriptorFileName='" + mfile + "'.")      

      if not firstModule:
        self.moduleStr += ", "
      firstModule=false
      self.moduleStr += "'" + mname + "'";

      mfile=os.path.basename(mfile)
      mfile=mfile.replace(".xml","-situational-config.xml")
      mfile=customPrefix + mfile

      self.moduleMap[moduleTypeStr + '-' + mname + '.xml'] = mfile

    # end of for loop

    self.moduleStr += ')' 


  def validateUnresolvedMacros(self, file, filestr):

    # Add a validation error if file contents have any unresolved macros
    # that contain a ":" or "." in  their name.  This step  is performed
    # after all known macros  are  already resolved.  (Other  macros are
    # considered  valid  server  template  macros in  config.xml,  so we
    # assume they're supposed to remain in the final sit-cfg xml).

    errstr = ''
    for unknown_macro in re.findall('\${[^}]*:[^}]*}', filestr):
      if errstr:
        errstr += ","
      errstr += unknown_macro
    for unknown_macro in re.findall('\${[^}]*[.][^}]*}', filestr):
      if errstr:
        errstr += ","
      errstr += unknown_macro
    if errstr:
      self.env.addError("Error, unresolvable macro(s) '" + errstr + "'" 
                        + " in custom sit config file '" + file + "'."
                        + " Known macros are '" + self.macroStr + "'.")


  def generateAndValidate(self):

    # For each custom sit-cfg template, generate a file using macro substitution,
    # validate that it has a correponding module if it's a module override file,
    # and validate that all of its 'secret:' and 'env:' macros are resolvable.

    if not os.path.exists(self.env.CUSTOM_SITCFG_PATH):
      return

    # We expect the user to include a 'version.txt' file in their situational
    # config directory.
    #
    # That file is expected to contain '2.0'
    #
    versionPath=os.path.join(self.env.CUSTOM_SITCFG_PATH,"version.txt")
    if not os.path.exists(versionPath):
        self.env.addError("Error, Required file, '"+versionPath+"', does not exist")
    else:
        version=self.env.readFile(versionPath).strip()
        if not version == "2.0":
            # truncate and ellipsify at 75 characters
            version = version[:75] + (version[75:] and '...')
            self.env.addError("Error, "+versionPath+" does not have the value of"
                              + " '2.0'. The current content: '" + version 
                              + "' is not valid.")

    for the_file in os.listdir(self.env.CUSTOM_SITCFG_PATH):

      if the_file == "version.txt":
        continue  

      the_file_path = os.path.join(self.env.CUSTOM_SITCFG_PATH, the_file)

      if not os.path.isfile(the_file_path):
        continue

      trace("Processing custom sit config file '" + the_file + "'")

      # check if file name corresponds with config.xml or a module

      if not self.moduleMap.has_key(the_file) and the_file != "config.xml":
        self.env.addError("Error, custom sit config override file '" + the_file + "'" 
          + " is not named 'config.xml' or has no matching system resource"
          + " module. Custom sit config files must be named 'config.xml'"
          + " to override config.xml or 'moduletype-modulename.xml' to override"
          + " a module. Known module names for each type: " + self.moduleStr + ".")
        continue

      # substitute macros and validate unresolved macros

      file_str = self.env.readFile(the_file_path)
      file_str_orig = 'dummyvalue'
      while file_str != file_str_orig:
        file_str_orig = file_str
        for key,val in self.macroMap.items():
          file_str=file_str.replace('${'+key+'}',val)

      self.validateUnresolvedMacros(the_file, file_str)

      # put resolved template into a file

      genfile = self.env.INTROSPECT_HOME + '/';

      if the_file == 'config.xml':
        genfile += self.env.CUSTOM_PREFIX_CFG + 'custom-situational-config.xml' 
      else:
        genfile += self.moduleMap[the_file]

      gen = Generator(self.env, genfile)
      gen.open()
      gen.write(file_str)
      gen.close()
      gen.addGeneratedFile()

class DomainIntrospector(SecretManager):

  def __init__(self, env):
    SecretManager.__init__(self, env)
    self.env = env

  def introspect(self):
    tg = TopologyGenerator(self.env)

    if tg.validate():
      DOMAIN_SOURCE_TYPE = self.env.getEnvOrDef("DOMAIN_SOURCE_TYPE", None)
      if DOMAIN_SOURCE_TYPE != "FromModel":
        SitConfigGenerator(self.env).generate()
      BootPropertiesGenerator(self.env).generate()
      UserConfigAndKeyGenerator(self.env).generate()

      if DOMAIN_SOURCE_TYPE == "FromModel":
        trace("cfgmap write primordial_domain")
        MII_PrimordialDomainGenerator(self.env).generate()
        trace("cfgmap write domain zip")
        MII_DomainConfigGenerator(self.env).generate()
        trace("cfgmap write merged model")
        MII_IntrospectCMFileGenerator(self.env, self.env.MII_MERGED_MODEL_FILE,
                                      self.env.DOMAIN_HOME +"/wlsdeploy/domain_model.json").generate()
        trace("cfgmap write md5 image")
        MII_IntrospectCMFileGenerator(self.env, self.env.MII_INVENTORY_IMAGE_MD5, '/tmp/inventory_image.md5').generate()
        trace("cfgmap write md5 cm")
        MII_IntrospectCMFileGenerator(self.env, self.env.MII_INVENTORY_CM_MD5, '/tmp/inventory_cm.md5').generate()
        trace("cfgmap write wls version")
        MII_IntrospectCMFileGenerator(self.env, self.env.WLS_VERSION, '/tmp/wls_version').generate()
        trace("cfgmap write jdk_path")
        MII_IntrospectCMFileGenerator(self.env, self.env.JDK_PATH, '/tmp/jdk_path').generate()
        trace("cfgmap write md5 secrets")
        MII_IntrospectCMFileGenerator(self.env, self.env.MII_SECRETS_AND_ENV_MD5, '/tmp/secrets_and_env.md5').generate()
        trace("cfgmap write model hash")
        # Must be called after MII_PrimordialDomainGenerator
        MII_IntrospectCMFileGenerator(self.env, self.env.MII_DOMAINZIP_HASH, '/tmp/domainzip_hash').generate()

        if self.env.WDT_DOMAIN_TYPE == 'JRF':
          trace("cfgmap write JRF wallet")
          MII_OpssWalletFileGenerator(self.env).generate()


    CustomSitConfigIntrospector(self.env).generateAndValidate()

    # If the topology is invalid, the generated topology
    # file contains a list of one or more validation errors
    # instead of a topology.
  
    tg.generate()

def getRealSSLListenPort(server, sslport):
  """
  Return the real listening port that will be used in runtime,
  which can be different than is reported by WLST off-line.

  The difference occurs when a user specifies 7002 in the model
  or wlst offline for a server template when creating the domain,
  which results in an empty entry or 7002 in the config.xml. When subsequently
  using wlst offline to read the domain, the mbean mistakenly
  returns 8100 but the actual listening port is 7002.

  If it is not a server template, then just return from the mbean.

  :param server:  server or server template
  :param sslport: sslport from wlst offline mbean
  :return: listening port
  """
  if server_template_sslports.has_key(server.getName()):
    configxml_ssl_port = server_template_sslports[server.getName()]
    if configxml_ssl_port is None or configxml_ssl_port == "7002":
      return 7002

  if sslport == 0:
    sslport = 7002

  return sslport

def getRealListenPort(template):
  """
  Return the real listening port that will be used in runtime,
  which can be different than is reported by WLST off-line.

  The difference occurs when a user specifies 7001 in the model
  or wlst offline for a server template when creating the domain,
  which results in an empty entry or 7001 entry in the config.xml. When subsequently
  using wlst offline to read the domain, the mbean mistakenly
  returns 7100 but the actual listening port is 7001.

  If it is not a server template, then just return from the mbean.

  :param server:  server or server template
  :return: listening port
  """
  if server_template_listening_ports.has_key(template.getName()):
    port = server_template_listening_ports[template.getName()]
    if port is None or port == "7001":
      return 7001

  port = template.getListenPort()
  # Probably don't need this - unlike NAP that can be 0.
  if port == 0:
    return 7001

  return port


# Derive the default value for SecureMode of a domain
def isSecureModeEnabledForDomain(domain):
  secureModeEnabled = false
  if domain.getSecurityConfiguration().getSecureMode() != None:
    secureModeEnabled = domain.getSecurityConfiguration().getSecureMode().isSecureModeEnabled()
  else:
    secureModeEnabled = domain.isProductionModeEnabled() and not LegalHelper.versionEarlierThan(domain.getDomainVersion(), "14.1.2.0")
  return secureModeEnabled

def isAdministrationPortEnabledForDomain(domain):
  administrationPortEnabled = false
  #"if domain.isSet('AdministrationPortEnabled'):" does not work in off-line WLST!
  # Go to the domain root
  cd('/')
  if isSet('AdministrationPortEnabled'):
    administrationPortEnabled = domain.isAdministrationPortEnabled()
  else:
    # AdministrationPortEnabled is not explicitly set so going with the default
    # Starting with 14.1.2.0, the domain's AdministrationPortEnabled default is derived from the domain's SecureMode
    administrationPortEnabled = isSecureModeEnabledForDomain(domain)
  return administrationPortEnabled

def isAdministrationPortEnabledForServer(server, domain, isServerTemplate=False):
  administrationPortEnabled = false
  #"if server.isSet('AdministrationPortEnabled'):" does not work in off-line WLST!
  cd('/')
  if isServerTemplate:
    cd('ServerTemplate')
  else:
    cd('Server')
  cd(server.getName())
  if isSet('AdministrationPortEnabled'):
    administrationPortEnabled = server.isAdministrationPortEnabled()
  else:
    administrationPortEnabled = isAdministrationPortEnabledForDomain(domain)
  return administrationPortEnabled

def getAdministrationPort(server, domain):
  port = server.getAdministrationPort()
  # In off-line WLST, the server's AdministrationPort default value is 0
  if port == 0:
    port = domain.getAdministrationPort()
  return port

def getNAPProtocol(nap, server, domain, is_server_template=False):
  protocol = nap.getProtocol()
  if len(server.getNetworkAccessPoints()) > 0:
    if is_server_template:
      cd('/ServerTemplate/' + server.getName() + '/NetworkAccessPoint/' + nap.getName())
    else:
      cd('/Server/' + server.getName() + '/NetworkAccessPoint/' + nap.getName())
    if not isSet('Protocol') and isSecureModeEnabledForDomain(domain):
      protocol = "t3s"
  return protocol

def isListenPortEnabledForServer(server, domain, is_server_template=False):
  enabled = server.isListenPortEnabled()
  if is_server_template:
    cd('/ServerTemplate')
  else:
    cd('/Server')
  cd(server.getName())
  if not isSet('ListenPortEnabled') and isSecureModeEnabledForDomain(domain):
    enabled = False
  return enabled

def isSSLListenPortEnabled(ssl, domain):
  enabled = False
  if ssl is not None:
    enabled = ssl.isEnabled()
  else:
    if isSecureModeEnabledForDomain(domain):
      enabled = True
  return enabled

def getSSLPortIfEnabled(server, domain, is_server_template=True):
  """
  return the SSL listen port if enabled -
    If SSL is enabled:
      If is_server_template is False then just return the SSL listen port from server mbean.
      If is_server_template is True then return the actual SSL listen port that it listens on.

    If SSL is not configured but domain has SecureMode enabled return 7002.
    If SSL is configured but explicitly disabled, return None.

  :param server: server or server template
  :param domain: domain mbean
  :return: SSL listen port
  """
  ssl = None
  ssl_listen_port = None
  try:
    # this can throw if SSL mbean not there
    ssl = server.getSSL()
    # this can throw if SSL mbean is there but enabled is false
    ssl.getListenPort()
    # this can throw if SSL mbean is there but enabled is false ??
    ssl.isEnabled()
  except:
    pass

  if ssl is not None and ssl.isEnabled():
    if not is_server_template:
      ssl_listen_port = ssl.getListenPort()
    else:
      ssl_listen_port = getRealSSLListenPort(server, ssl.getListenPort())
  elif ssl is None and isSecureModeEnabledForDomain(domain):
    ssl_listen_port = "7002"
  return ssl_listen_port

def get_server_template_listening_ports_from_configxml(config_xml):
  '''
  get_server_tempalate's listening port and ssl port from the config.xml
  :param config_xml:         full path to config.xml
  :return: dictionary of servertemplate ssl port and servertemplate listen port
  '''
  DOMTree = parse(config_xml)
  collection = DOMTree.documentElement

  templates = collection.getElementsByTagName("server-template")
  server_template_ssls = dict()
  server_template_ports = dict()

  # if port is not specified in config.xml, set to None

  for template in templates:
    sslport = None
    port = None
    if template.parentNode.nodeName != 'domain':
      continue
    template_name = template.getElementsByTagName('name')[0].firstChild.nodeValue
    # Get listen port
    listen_ports = template.getElementsByTagName('listen-port')

    for listen_port in listen_ports:
      if listen_port.parentNode.nodeName == 'server-template':
        port = listen_port.firstChild.nodeValue
        break
    server_template_ports[template_name] = port

    # Get ssl port
    ssls = template.getElementsByTagName('ssl')
    if len(ssls) > 0:
      ssl = ssls.item(0)
      listen_port = ssl.getElementsByTagName('listen-port')
      if len(listen_port) > 0:
        sslport = listen_port[0].firstChild.nodeValue
    server_template_ssls[template_name] = sslport

  return server_template_ssls, server_template_ports


def main(env):
  try:
    #  Needs to build the domain first


    env.open()
    try:
      env.addGeneratedFile(env.MII_DOMAIN_SECRET_MD5_FILE)
      DomainIntrospector(env).introspect()
      env.printGeneratedFiles()
      trace("Domain introspection complete.")
    finally:
      env.close()
    exit(exitcode=0)
  except WLSTException, e:
    trace("SEVERE","Domain introspection failed with WLST exception: " + str(e))
    print e
    traceback.print_exc()
    dumpStack()
    exit(exitcode=1)
  except:
    trace("SEVERE","Domain introspection unexpectedly failed:")
    traceback.print_exc()
    dumpStack()
    exit(exitcode=1)

main(OfflineWlstEnv())

livenessProbe.sh:
----
#!/bin/bash

# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

# Kubernetes periodically calls this liveness probe script to determine whether
# the pod should be restarted. The script checks a WebLogic Server state file which
# is updated by the node manager.

function copySitCfgWhileRunning() {
  # Helper fn to copy sit cfg xml files to the WL server's domain home.
  #   - params $1/$2/$3 == 'src_dir tgt_dir fil_prefix'
  #   - $src_dir files are assumed to start with $fil_prefix and end with .xml
  #   - copied $tgt_dir files are stripped of their $fil_prefix
  #   - any .xml files in $tgt_dir that are not in $src_dir/$fil_prefix+FILE are deleted
  #
  # This method is called while the server is already running, see 
  # 'copySitCfgWhileBooting' in 'startServer.sh' for a similar method that
  # is called just before the server boots.
  #
  # It will do nothing unless the environment variable DYNAMIC_CONFIG_OVERRIDE is set.

if [ ! "${DYNAMIC_CONFIG_OVERRIDE:-notset}" = notset ]; then
    (
    shopt -s nullglob  # force file matching 'glob' for loops below to run 0 times if 0 matches
    local src_dir=${1?}
    local tgt_dir=${2?}
    local fil_prefix=${3?}
    local local_fname
    local tgt_file
    mkdir -p $tgt_dir # TBD ignore any error?
    for local_fname in ${src_dir}/${fil_prefix}*.xml ; do
      tgt_file=${local_fname/$fil_prefix//}   # strip out file prefix from source file
      tgt_file=$(basename $tgt_file)          # strip out dir path since it's the source file path
      tgt_file=$tgt_dir/$tgt_file             # add back in tgt dir path
      [ -f "$tgt_file" ] && [ -z "$(diff $local_fname $tgt_file 2>&1)" ] && continue  # nothing changed
      trace "Copying file '$local_fname' to '$tgt_file'."
      cp $local_fname $tgt_file # TBD ignore any error?
      if [ -O "$tgt_file" ]; then
        chmod 770 $tgt_file # TBD ignore any error?
      fi
    done
    for local_fname in ${tgt_dir}/*.xml ; do
      if [ -f "$src_dir/${fil_prefix}$(basename ${local_fname})" ]; then
        continue
      fi
      trace "Deleting '$local_fname' since it has no corresponding '$src_dir' file."
      rm -f $local_fname # TBD ignore any error?
    done
    )
  fi
}


# if the livenessProbeSuccessOverride file is available, treat failures as success:
RETVAL=$(test -f /weblogic-operator/debug/livenessProbeSuccessOverride ; echo $?)

SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
source ${SCRIPTPATH}/utils.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit $RETVAL

# check DOMAIN_HOME for a config/config.xml, reset DOMAIN_HOME if needed:
exportEffectiveDomainHome || exit $RETVAL
exportInstallHomes || exit $RETVAL

DN=${DOMAIN_NAME?}
SN=${SERVER_NAME?}
DH=${DOMAIN_HOME?}

STATEFILE=${DH}/servers/${SN}/data/nodemanager/${SN}.state

if [ "${MOCK_WLS}" != 'true' ]; then
  # Adjust PATH if necessary before calling jps
  adjustPath

  if [ `jps -l | grep -c " weblogic.NodeManager"` -eq 0 ]; then
    trace SEVERE "WebLogic NodeManager process not found."
    exit $RETVAL
  fi
fi

if [ -f ${STATEFILE} ] && [ `grep -c "FAILED_NOT_RESTARTABLE" ${STATEFILE}` -eq 1 ]; then
  # WARNING: This state file check is essentially a public API and 
  #          must continue to be honored even if we remove the node
  #          manager from the life cycle.
  #
  #          (There is at least one WKO user that externally modifies
  #          the file to FAILED_NOT_RESTARTABLE to force a liveness
  #          failure when the user detects that their applications
  #          are unresponsive.)
  trace SEVERE "WebLogic Server state is FAILED_NOT_RESTARTABLE."
  exit $RETVAL
fi

if [ -x ${LIVENESS_PROBE_CUSTOM_SCRIPT} ]; then
  $LIVENESS_PROBE_CUSTOM_SCRIPT
elif [ -O ${LIVENESS_PROBE_CUSTOM_SCRIPT} ]; then
  chmod 770 $LIVENESS_PROBE_CUSTOM_SCRIPT && $LIVENESS_PROBE_CUSTOM_SCRIPT
fi
if [ $? != 0 ]; then
  trace SEVERE "Execution of custom liveness probe script ${LIVENESS_PROBE_CUSTOM_SCRIPT} failed."
  exit $RETVAL
fi

if [ ${DOMAIN_SOURCE_TYPE} != "FromModel" ]; then
  copySitCfgWhileRunning /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig             'Sit-Cfg-CFG--'
  copySitCfgWhileRunning /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig/jms         'Sit-Cfg-JMS--'
  copySitCfgWhileRunning /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig/jdbc        'Sit-Cfg-JDBC--'
  copySitCfgWhileRunning /weblogic-operator/introspector ${DOMAIN_HOME}/optconfig/diagnostics 'Sit-Cfg-WLDF--'
fi

exit 0

model-diff.py:
----
# Copyright (c) 2019, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#
# ------------
# Description:
# ------------
#
#   This code check the diffed model to see what kind of changes are in it.
#   The output is written as csv file /tmp/model_diff_rc  containing the return codes that represent the differences.
#
#   This script is invoked by jython.  See modelInImage.sh diff_model
#
import sys, os, traceback
from java.lang import System
UNSAFE_ONLINE_UPDATE=0
SAFE_ONLINE_UPDATE=1
FATAL_MODEL_CHANGES=2
MODELS_SAME=3
SECURITY_INFO_UPDATED=4
RCU_PASSWORD_CHANGED=5
NOT_FOR_ONLINE_UPDATE=6

class ModelDiffer:

    def is_not_safe_for_online_update(self, model, original_model):
        """
        Is it a safe difference to do online update.
        :param model: diffed model
        return 1 for not safe
            0 for safe
        """
        if os.environ.has_key('MII_USE_ONLINE_UPDATE'):
            if "false" == os.environ['MII_USE_ONLINE_UPDATE']:
                return 0
        else:
            return 0

        if self.in_forbidden_list(model, original_model):
            return 1

        return 0


    def is_safe_diff(self, model, original_model):
        """
        Is it a safe difference for update.
        :param model: diffed model
        return 0 - always return 0 for V1
        """

        # check for phase 1 any security changes in the domainInfo intersection

        if model.has_key('domainInfo'):
            domain_info = model['domainInfo']
            if domain_info.has_key('AdminUserName') or domain_info.has_key('AdminPassword') \
                    or domain_info.has_key('WLSRoles') or domain_info.has_key('WLSUserPasswordCredentialMappings'):
                changed_items.append(SECURITY_INFO_UPDATED)

            if domain_info.has_key('RCUDbInfo'):
                rcu_db_info = domain_info['RCUDbInfo']
                if rcu_db_info.has_key('rcu_schema_password'):
                    changed_items.append(RCU_PASSWORD_CHANGED)

                if rcu_db_info.has_key('rcu_db_conn_string') \
                    or rcu_db_info.has_key('rcu_prefix'):
                    changed_items.append(SECURITY_INFO_UPDATED)

        if model.has_key('topology'):
            if model['topology'].has_key('Security'):
                changed_items.append(SECURITY_INFO_UPDATED)
            if model['topology'].has_key('SecurityConfiguration'):
                changed_items.append(SECURITY_INFO_UPDATED)

        if self.is_not_safe_for_online_update(model, original_model):
            changed_items.append(NOT_FOR_ONLINE_UPDATE)

        return 0


    def in_forbidden_list(self, model, original_model):

        if os.environ.has_key('MII_USE_ONLINE_UPDATE'):
            if "false" == os.environ['MII_USE_ONLINE_UPDATE']:
                return 0
        else:
            return 0

        # Do not allow change ListenAddress, Port, enabled, SSL
        # Allow add
        # Do not allow delete
        _TOPOLOGY = 'topology'
        _NAP = 'NetworkAccessPoint'
        _SSL = 'SSL'
        forbidden_network_attributes = [ 'ListenAddress', 'ListenPort', 'ListenPortEnabled' ]
        if model.has_key(_TOPOLOGY):
            # Do not allow changing topology level attributes
            for key in model[_TOPOLOGY]:
                if not isinstance(model[_TOPOLOGY][key], dict):
                    return 1
            for key in [ 'Server', 'ServerTemplate']:
                # topology.Server|ServerTemplate
                if model[_TOPOLOGY].has_key(key):
                    temp = model[_TOPOLOGY][key]
                    for server in temp:
                        # cannot delete server or template
                        if server.startswith('!'):
                            return 1
                        # ok to add
                        if server not in original_model['topology'][key]:
                            continue
                        for not_this in forbidden_network_attributes:
                            if temp[server].has_key(not_this):
                                return 1
                        if temp[server].has_key(_NAP):
                            nap = temp[server][_NAP]
                            for n in nap:
                                for not_this in forbidden_network_attributes:
                                    if temp[server].has_key(not_this):
                                        return 1
                        # Do not allow any SSL changes
                        if temp[server].has_key(_SSL):
                            return 1

        return 0


class ModelFileDiffer:

    def eval_file(self, file):
        true = True
        false = False
        fh = open(file, 'r')
        content = fh.read()
        return eval(content)

    def compare(self):
        original_model = self.eval_file(sys.argv[1])
        # past_dict = self.eval_file(sys.argv[2])
        obj = ModelDiffer()
        if os.path.exists('/tmp/diffed_model.json'):
            net_diff = self.eval_file('/tmp/diffed_model.json')
        else:
            net_diff = {}
        return obj.is_safe_diff(net_diff, original_model)

def debug(format_string, *arguments):
    if os.environ.has_key('DEBUG_INTROSPECT_JOB'):
        print format_string % (arguments)
    return

def main():
    try:
        obj = ModelFileDiffer()
        obj.compare()
        rcfh = open('/tmp/model_diff_rc', 'w')
        rcfh.write(",".join(map(str,changed_items)))
        rcfh.close()
        System.exit(0)
    except:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        eeString = traceback.format_exception(exc_type, exc_obj, exc_tb)
        print eeString
        System.exit(-1)
if __name__ == "__main__":
    all_changes = []
    all_added = []
    all_removed = []
    changed_items = []
    main()



model-filters.json:
----
{
    "create": [
      { "name": "primordial_filter", "path": "/weblogic-operator/scripts/model-wdt-create-filter.py" }
    ],
    "deploy": [
    ],
    "discover": [
    ],
    "update": [
      { "name": "update_mii_filter", "path": "/weblogic-operator/scripts/model_wdt_mii_filter.py" }
    ],
    "validate": [
      { "name": "validate_mii_filter", "path": "/weblogic-operator/scripts/model_wdt_mii_filter.py" }
    ]
  }

readState.sh:
----
#!/bin/bash

# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

# Reads the current state of a server. The script checks a WebLogic Server state
# file which is updated by the node manager.

SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
source ${SCRIPTPATH}/utils.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit 1

# check DOMAIN_HOME for a config/config.xml, reset DOMAIN_HOME if needed:
exportEffectiveDomainHome || exit 1

DN=${DOMAIN_NAME?}
SN=${SERVER_NAME?}
DH=${DOMAIN_HOME?}

STATEFILE=/${DH}/servers/${SN}/data/nodemanager/${SN}.state

# Adjust PATH if necessary before calling jps
adjustPath

if [ `jps -v | grep -c " -Dweblogic.Name=${SERVER_NAME} "` -eq 0 ]; then
  trace "WebLogic server process not found"
  exit 1
fi

if [ ! -f ${STATEFILE} ]; then
  trace "WebLogic Server state file not found."
  exit 2
fi

cat ${STATEFILE} | cut -f 1 -d ':'
exit 0

startNodeManager.sh:
----
#!/bin/bash
# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# This script starts a node manager for either a WebLogic Server pod,
# or for the WebLogic Operator introspector job.
#
# Requires the following to already be set:
#
#   DOMAIN_UID        = Domain UID
#   JAVA_HOME         = Existing java home
#   DOMAIN_HOME       = Existing WebLogic domain home directory
#   NODEMGR_HOME      = Target directory for NM setup files, this script
#                       will append this value with /$DOMAIN_UID/$SERVER_NAME
#
# Optionally set:
#
#   SERVER_NAME       = If not set, assumes this is introspector.
#
#   ORACLE_HOME       = Oracle Install Home - defaults via utils.sh/exportInstallHomes
#   MW_HOME           = MiddleWare Install Home - defaults to ${ORACLE_HOME}
#   WL_HOME           = WebLogic Install Home - defaults to ${ORACLE_HOME}/wlserver
#
#   NODEMGR_LOG_HOME  = Directory that will contain contain both
#                          ${DOMAIN_UID}/${SERVER_NAME}_nodemanager.log
#                          ${DOMAIN_UID}/${SERVER_NAME}_nodemanager.out
#                       Default:
#                          Use LOG_HOME.  If LOG_HOME not set, use NODEMGR_HOME.
#   NODEMGR_LOG_FILE_MAX = max NM .log and .out files to keep around (default=11)
#
#   ADMIN_PORT_SECURE = "true" if the admin protocol is secure. Default is false
#
#   FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR = "true" if WebLogic server should fail to 
#                       boot if situational configuration related errors are 
#                       found. Default to "true" if unspecified.
#
#   NODEMGR_MEM_ARGS  = JVM mem args for starting the Node Manager instance
#   NODEMGR_JAVA_OPTIONS  = Java options for starting the Node Manager instance
#
# If SERVER_NAME is set, then this NM is for a WL Server and these must also be set:
# 
#   SERVICE_NAME      = Internal DNS name for WL Server SERVER_NAME
#   ADMIN_NAME        = Admin server name
#   AS_SERVICE_NAME   = Internal DNS name for Admin Server ADMIN_NAME
#   USER_MEM_ARGS     = JVM mem args for starting WL server
#   JAVA_OPTIONS      = Java options for starting WL server
#

###############################################################################
#
#  Assert that expected global env vars are already set, pre-req files/dirs exist, etc.
#

SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"

source ${SCRIPTPATH}/utils.sh 
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit 1 

# Set ORACLE_HOME/WL_HOME/MW_HOME to defaults if needed
exportInstallHomes

stm_script=${WL_HOME}/server/bin/startNodeManager.sh

SERVER_NAME=${SERVER_NAME:-introspector}
ADMIN_PORT_SECURE=${ADMIN_PORT_SECURE:-false}

trace "Starting node manager for domain-uid='$DOMAIN_UID' and server='$SERVER_NAME'."

checkEnv JAVA_HOME NODEMGR_HOME DOMAIN_HOME DOMAIN_UID ORACLE_HOME MW_HOME WL_HOME || exit 1

if [ "${SERVER_NAME}" = "introspector" ]; then
  SERVICE_NAME=localhost
  trace "Contents of '${DOMAIN_HOME}/config/config.xml':"
  cat ${DOMAIN_HOME}/config/config.xml
else
  checkEnv SERVER_NAME ADMIN_NAME AS_SERVICE_NAME SERVICE_NAME USER_MEM_ARGS || exit 1
fi

[ ! -d "${JAVA_HOME}" ]                     && trace SEVERE "JAVA_HOME directory not found '${JAVA_HOME}'."           && exit 1 
[ ! -d "${DOMAIN_HOME}" ]                   && trace SEVERE "DOMAIN_HOME directory not found '${DOMAIN_HOME}'."       && exit 1 
[ ! -f "${DOMAIN_HOME}/config/config.xml" ] && trace SEVERE "'${DOMAIN_HOME}/config/config.xml' not found."           && exit 1 
[ ! -d "${WL_HOME}" ]                       && trace SEVERE "WL_HOME '${WL_HOME}' not found."                         && exit 1 
[ ! -f "${stm_script}" ]                    && trace SEVERE "Missing script '${stm_script}' in WL_HOME '${WL_HOME}'." && exit 1 

#
# Helper fn to create a folder
# Arg $1 - path of folder to create
#
function createFolder {
  mkdir -m 750 -p "$1"
  if [ ! -d "$1" ]; then
    trace SEVERE "Unable to create folder '$1'."
    exit 1
  fi
}


###############################################################################
#
# Determine WebLogic server log and out files locations
#
# -Dweblogic.Stdout system property is used to tell node manager to send server .out 
#  file to the configured location
#

if [ "${SERVER_NAME}" = "introspector" ]; then
  # introspector pod doesn't start a WL server
  serverOutOption=""
else
  # setup ".out" location for a WL server
  serverLogHome="${LOG_HOME:-${DOMAIN_HOME}/servers/${SERVER_NAME}/logs}"
  export SERVER_OUT_FILE="${serverLogHome}/${SERVER_NAME}.out"
  export SERVER_PID_FILE="${serverLogHome}/${SERVER_NAME}.pid"
  export SHUTDOWN_MARKER_FILE="${serverLogHome}/${SERVER_NAME}.shutdown"
  serverOutOption="-Dweblogic.Stdout=${SERVER_OUT_FILE}"
  createFolder "${serverLogHome}"
  rm -f ${SHUTDOWN_MARKER_FILE}
fi


###############################################################################
#
# Init/create nodemanager home and nodemanager log env vars and directory
#

export NODEMGR_HOME=${NODEMGR_HOME}/${DOMAIN_UID}/${SERVER_NAME}

createFolder ${NODEMGR_HOME} 

NODEMGR_LOG_HOME=${NODEMGR_LOG_HOME:-${LOG_HOME:-${NODEMGR_HOME}/${DOMAIN_UID}}}
FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR=${FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR:-true}

trace "NODEMGR_HOME='${NODEMGR_HOME}'"
trace "LOG_HOME='${LOG_HOME}'"
trace "SERVER_NAME='${SERVER_NAME}'"
trace "DOMAIN_UID='${DOMAIN_UID}'"
trace "NODEMGR_LOG_HOME='${NODEMGR_LOG_HOME}'"
trace "FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR='${FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR}'"

createFolder ${NODEMGR_LOG_HOME}

nodemgr_log_file=${NODEMGR_LOG_HOME}/${SERVER_NAME}_nodemanager.log
nodemgr_out_file=${NODEMGR_LOG_HOME}/${SERVER_NAME}_nodemanager.out
nodemgr_lck_file=${NODEMGR_LOG_HOME}/${SERVER_NAME}_nodemanager.log.lck

checkEnv NODEMGR_LOG_HOME nodemgr_log_file nodemgr_out_file nodemgr_lck_file

trace "remove nodemanager .lck file"
rm -f ${nodemgr_lck_file}


###############################################################################
#
# Determine domain name by parsing ${DOMAIN_HOME}/config/config.xml
#
# We need the domain name to register the domain with the node manager
# but we only have the domain home.
#
# The 'right' way to find the domain name is to use offline wlst to
# read the domain then get it from the domain mbean, but that's slow
# and complicated. Instead, just get it by reading config.xml directly.
#

# Look for the 1st occurence of <name>somestring</name> and assume somestring
# is the domain name:
domain_name=`cat ${DOMAIN_HOME}/config/config.xml | sed 's/[[:space:]]//g' | grep '^<name>' | head -1 | awk -F'<|>' '{print $3}'`
if [ "$domain_name" = "" ]; then
  trace SEVERE "Could not determine domain name"
  exit 1
fi


###############################################################################
#
# Create nodemanager.properties and nodemanager.domains files in NM home 
#

nm_domains_file=${NODEMGR_HOME}/nodemanager.domains
cat <<EOF > ${nm_domains_file}
  ${domain_name}=${DOMAIN_HOME}
EOF
[ ! $? -eq 0 ] && trace SEVERE "Failed to create '${nm_domains_file}'." && exit 1

nm_props_file=${NODEMGR_HOME}/nodemanager.properties

cat <<EOF > ${nm_props_file}
  #Node manager properties
  NodeManagerHome=${NODEMGR_HOME}
  JavaHome=${JAVA_HOME}
  DomainsFile=${nm_domains_file}
  DomainsFileEnabled=true
  DomainsDirRemoteSharingEnabled=true
  NativeVersionEnabled=true
  PropertiesVersion=12.2.1
  ListenAddress=127.0.0.1
  ListenPort=5556
  ListenBacklog=50
  AuthenticationEnabled=false
  SecureListener=false
  weblogic.StartScriptEnabled=true
  weblogic.StartScriptName=startWebLogic.sh
  weblogic.StopScriptEnabled=false
  QuitEnabled=false
  StateCheckInterval=500
  CrashRecoveryEnabled=false
  LogFile=${nodemgr_log_file}
  LogToStderr=true
  LogFormatter=weblogic.nodemanager.server.LogFormatter
  LogAppend=true
  LogLimit=0
  LogLevel=FINEST
  LogCount=1

EOF

[ ! $? -eq 0 ] && trace SEVERE "Failed to create '${nm_props_file}'." && exit 1

###############################################################################
#
#  If we're a WL Server pod, cleanup its old state file and 
#  create its NM startup.properties file.
#

if [ ! "${SERVER_NAME}" = "introspector" ]; then

  wl_data_dir=${DOMAIN_HOME}/servers/${SERVER_NAME}/data/nodemanager
  wl_state_file=${wl_data_dir}/${SERVER_NAME}.state
  wl_props_file=${wl_data_dir}/startup.properties

  createFolder ${wl_data_dir}

  # Remove state file, because:
  #   1 - The liveness probe checks this file
  #   2 - It might have a stale value
  #   3 - NM checks this file, and may auto-start the server if it's missing
  
  if [ -f "$wl_state_file" ]; then
    trace "Removing stale file '$wl_state_file'."
    rm -f ${wl_state_file} 
    [ ! $? -eq 0 ] && trace SEVERE "Could not remove stale file '$wl_state_file'." && exit 1
  fi

  if [ ${DOMAIN_SOURCE_TYPE} == "FromModel" ]; then
    # Domain source type is 'FromModel' (MII) then disable Situation config override for WebLogic.
    failBootOnErrorOption=""
  else
    failBootOnErrorOption="-Dweblogic.SituationalConfig.failBootOnError=${FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR}"
  fi

cat <<EOF > ${wl_props_file}
# Server startup properties
AutoRestart=true
RestartMax=2
RestartInterval=3600
NMHostName=${SERVICE_NAME}
Arguments=${USER_MEM_ARGS} ${failBootOnErrorOption} ${serverOutOption} ${JAVA_OPTIONS}

EOF
 
  [ ! $? -eq 0 ] && trace SEVERE "Failed to create '${wl_props_file}'." && exit 1

  if [ ! "${ADMIN_NAME}" = "${SERVER_NAME}" ]; then
    ADMIN_URL=$(getAdminServerUrl)
    echo "AdminURL=$ADMIN_URL" >> ${wl_props_file}
  fi
fi

###############################################################################
#
#  Set additional env vars required to start NM
#

#  Customized properties
export JAVA_PROPERTIES="-DLogFile=${nodemgr_log_file} -DNodeManagerHome=${NODEMGR_HOME}"

#  Copied from ${DOMAIN_HOME}/bin/setNMJavaHome.sh
#  (We assume a Oracle Sun Hotspot JVM since we're only using Linux VMs
#  and only support other JVM types on non-Linux OS (HP-UX, IBM AIX, IBM zLinux)).
export BEA_JAVA_HOME=""
export DEFAULT_BEA_JAVA_HOME=""
export SUN_JAVA_HOME="${JAVA_HOME?}"
export DEFAULT_SUN_JAVA_HOME="${JAVA_HOME?}"
export JAVA_VENDOR="Oracle"
export VM_TYPE="HotSpot"

#  Copied from ${DOMAIN_HOME}/bin/startNodeManager.sh 
export NODEMGR_HOME="${NODEMGR_HOME?}"
export DOMAIN_HOME="${DOMAIN_HOME?}"

# Apply JAVA_OPTIONS to Node Manager if NODEMGR_JAVA_OPTIONS not specified
if [ -z ${NODEMGR_JAVA_OPTIONS} ]; then
  NODEMGR_JAVA_OPTIONS="${JAVA_OPTIONS}"
fi

if [ -z "${NODEMGR_MEM_ARGS}" ]; then
  # Default JVM memory arguments for Node Manager
  NODEMGR_MEM_ARGS="-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom "
fi

# We prevent USER_MEM_ARGS from being applied to the NM here and only pass
# USER_MEM_ARGS to WL Servers via the WL Server startup properties file above.
# This is so that WL Servers and NM can have different tuning. Use NODEMGR_MEM_ARGS or
# NODEMGR_JAVA_OPTIONS to specify JVM memory arguments for NMs.
# NOTE: Specifying USER_MEM_ARGS with ' ' (space, not empty string)
# prevents MEM_ARGS from being implicitly set by the WebLogic env
# scripts in the WebLogic installation and WLS from inserting default
# values for memory arguments. (See commBaseEnv.sh).
USER_MEM_ARGS=" "
export USER_MEM_ARGS

# NODEMGR_MEM_ARGS and NODEMGR_JAVA_OPTIONS are exported to Node Manager as JAVA_OPTIONS
# environment variable.
export JAVA_OPTIONS="${NODEMGR_MEM_ARGS} ${NODEMGR_JAVA_OPTIONS} -Dweblogic.RootDirectory=${DOMAIN_HOME}"

###############################################################################
#
#  Start the NM
#  1) rotate old NM log file, and old NM out file, if they exist
#  2) start NM in background
#  3) wait up to ${NODE_MANAGER_MAX_WAIT:-60} seconds for NM by monitoring NM's .out file
#  4) log SEVERE, log INFO with 'exit 1' if wait more than ${NODE_MANAGER_MAX_WAIT:-60} seconds
# 

trace "Start the nodemanager, node manager home is '${NODEMGR_HOME}', log file is '${nodemgr_log_file}', out file is '${nodemgr_out_file}'."

logFileRotate ${nodemgr_log_file} ${NODEMGR_LOG_FILE_MAX:-11}
logFileRotate ${nodemgr_out_file} ${NODEMGR_LOG_FILE_MAX:-11}

${stm_script} > ${nodemgr_out_file} 2>&1 &

start_secs=$SECONDS
max_wait_secs=${NODE_MANAGER_MAX_WAIT:-60}
while [ 1 -eq 1 ]; do
  sleep 1
  if [ -e ${nodemgr_log_file} ] && [ `grep -c "Plain socket listener started" ${nodemgr_log_file}` -gt 0 ]; then
    break
  fi
  if [ $((SECONDS - $start_secs)) -ge $max_wait_secs ]; then
    pid=$(jps | grep NodeManager | awk '{ print $1 }')
    if [ -z $pid ]; then
      trace INFO "Node manager process id not found. Cannot create thread dump."
    else
      trace INFO "Node manager process id is '$pid'."
      trace INFO "Trying to put a node manager thread dump in '$nodemgr_out_file'."
      kill -3 $pid
      if [ -x "$(command -v $JAVA_HOME/bin/jcmd)" ]; then
        trace INFO "Node manager thread dump:"
        $JAVA_HOME/bin/jcmd $pid Thread.print
      fi
    fi
    trace INFO "Entropy: "
    cat /proc/sys/kernel/random/entropy_avail
    trace INFO "Contents of node manager log '$nodemgr_log_file':"
    cat ${nodemgr_log_file}
    trace INFO "Contents of node manager out '$nodemgr_out_file':"
    cat ${NODEMGR_OUT_FILE}

    trace SEVERE $(cat << EOF
The node manager failed to start within $max_wait_secs seconds.
To increase this timeout, define the NODE_MANAGER_MAX_WAIT
environment variable in your domain resource, and set it higher
than $max_wait_secs. To diagnose the problem, see the above INFO
messages for node manager log contents, stdout contents, pid,
thread dump, and entropy. If the log and stdout contents are
sparse and reveal no errors, then the node manager may be stalled
while generating entropy -- especially if entropy is below 500.
If entropy is the problem, then for testing purposes you can
temporarily work around this problem by specifying
'-Djava.security.egd=file:/dev/./urandom' in a USER_MEM_ARGS
environment variable defined via your domain resource, but
for production purposes the problem should be solved by following
the guidance in
'https://docs.oracle.com/en/middleware/fusion-middleware/weblogic-server/12.2.1.4/nodem/starting_nodemgr.html#GUID-53961E3A-D8E1-4556-B78A-9A56B676D57E'
(search for keyword 'rngd').
EOF
)
    exit 1
  fi
done

trace "Nodemanager started in $((SECONDS - start_secs)) seconds."

tailLog.sh:
----
#!/bin/bash

# Copyright (c) 2019, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# This script is used to tail the server log and is separate from
# startServer.sh so that it is easier to quickly kill the process
# running this script.
#

echo $$ > $2
chmod g-wx,o-rwx $2

while true ; do
  if [ -f $1 ]; then
    tail -F -s 0.1 -n +0 $1 || sleep 10
  fi
  sleep 0.1
done

#
# Work around note:
#
#   The forever loop and '[ -f $1 ]' check above work around an
#   unexpected  behavior  from  'tail -F'.  The  '-F'  expected 
#   behavior is meant to handle 'rolling' files by both:
#
#     A- Waiting  until the  file appears  instead of exiting
#        with an error code.
#
#     B- Once the file is found, gracefully handling the file
#        getting deleted or moved  (by pausing while the file
#        is gone,  and starting up again once a new file with
#        the same name appears in its place).
#
#   But 'A' is not working on WL pods and thus the work around.
#   (Strangely and thankfully 'B' works fine.)
#


utils_base.sh:
----
# Copyright (c) 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

# Important: Functions defined in this file can work with unknown shells,
#            e.g. pure shell instead of bash, when UNKNOWN_SHELL has been set to true. 

#
# trace [-cloc caller-location] -n [log-level] [text]*
# trace [-cloc caller-location] -pipe [log-level] [text]*
# trace [-cloc caller-location] [log-level] [text]*
#
#   Generate logging in a format similar to WLST utils.py using the
#   same timestamp format as the Operator, and using the same 
#   log levels as the Operator. This logging is may be parsed by the
#   Operator when it reads in a job or pod log.
#
#   log-level can be one of SEVERE|ERROR|WARNING|INFO|CONFIG|FINE|FINER|FINEST
#     - Default is 'FINE'.
#     - NOTE: Use SEVERE, ERROR, WARNING, INFO sparingly since these log-levels
#             are visible by default in the Operator log and the Operator captures
#             some script logs and echos them to the Operator log.
#     - if it's ERROR it's converted to SEVERE
#     - if there's no log-level and the text starts with a
#       recognized keyword like 'Error:' then the log-level is inferred
#     - if there's no log-level and the text does not start with
#       a recognized keyword, the log-level is assumed to be 'FINE' (the default)
#
#   -n     Suppress new-line.
#
#   -pipe  Redirect stdout through a trace, see example below.
#
#   -cloc  Use the supplied value as the caller location
#
#   examples:
#     trace "Situation normal."
#     @[2018-09-28T18:10:52.417000Z][myscript.sh:91][FINE] Situation normal.
#
#     trace INFO "Situation normal."
#     @[2018-09-28T18:10:52.417000Z][myscript.sh:91][INFO] Situation normal.
#
#     trace "Info: Situation normal."
#     @[2018-09-28T18:10:52.417000Z][myscript.sh:91][INFO] Info: Situation normal.
#
#     ls 2>&1 | tracePipe FINE "ls output: "
#     @[2018-09-28T18:10:52.417000Z][myscript.sh:91][FINE] ls output: file1
#     @[2018-09-28T18:10:52.417000Z][myscript.sh:91][FINE] ls output: file2
#
#   Set TRACE_INCLUDE_FILE env var to false to suppress file name and line number.
#
function trace() {
  (
  set +x

  local logLoc=""
  if [ ${TRACE_INCLUDE_FILE:-true} = "true" ]; then
    if [ "$1" = "-cloc" ]; then
      logLoc="$2"
      shift
      shift
    else
      if [[ "$UNKNOWN_SHELL" = "true" ]]; then
        logLoc=`basename $0`:$LINENO
      else
        logLoc="$(basename ${BASH_SOURCE[1]}):${BASH_LINENO[0]}"
      fi
    fi
  else
    if [ "$1" = "-cloc" ]; then
      shift
      shift
    fi
  fi

  local logMode='-normal'
  case $1 in
    -pipe|-n) logMode=$1; shift; ;;
  esac

  # Support log-levels in operator, if unknown then assume FINE
  #  SEVERE|WARNING|INFO|CONFIG|FINE|FINER|FINEST
  local logLevel='FINE'

  #(Convert the var to upper case.)
  local level=`echo $1 | tr [a-z] [A-Z]`
  case ${level} in
    SEVERE|WARNING|INFO|CONFIG|FINE|FINER|FINEST)
      logLevel=$level
      shift
      ;;
    ERROR)
      logLevel='SEVERE'
      shift
      ;;
    WARNING*)
      logLevel='WARNING'
      ;;
    ERROR*|SEVERE*)
      logLevel='SEVERE'
      ;;
    INFO*)
      logLevel='INFO'
      ;;
    CONFIG*)
      logLevel='CONFIG'
      ;;
    FINEST*)
      logLevel='FINEST'
      ;;
    FINER*)
      logLevel='FINER'
      ;;
    FINE*)
      logLevel='FINE'
      ;;
  esac

  function logPrefix() {
    echo "@[`timestamp`][$logLoc][$logLevel]"
  }

  case $logMode in 
    -pipe) 
          (
          # IFS='' causes read line to preserve leading spaces
          # -r cause read to treat backslashes as-is, e.g. '\n' --> '\n'
          IFS=''
          while read -r line; do
            echo "$(logPrefix)" "$@" "$line"
          done
          )
          ;;
    -n)
          echo -n "$(logPrefix)" "$@"
          ;;
    *)
          echo "$(logPrefix)" "$@"
          ;;
  esac
  )
}

#
# traceDirs before|after DOMAIN_HOME LOG_HOME DATA_HOME ...
#   Trace contents and owner of directory for the specified env vars...

function traceDirs() {
  trace "id = '`id`'"
  local keyword="$1"
  shift
  local indir
  local val_indir
  for indir in $*; do
    eval "val_indir=\"\${indir}\""
    [ -z "${val_indir}" ] && continue
    trace "Directory trace for $indir=${val_indir} ($keyword)"
    local cnt=0
    local odir=""
    local cdir="${val_indir}/*"
    while [ ${cnt} -lt 30 ] && [ ! "$cdir" = "$odir" ]; do
      echo "  ls -ld $cdir:"
      ls -ld $cdir 2>&1 | sed 's/^/    /'
      odir="$cdir"
      cdir="`dirname "$cdir"`"
      cnt=$((cnt + 1))
    done
  done
}

# timestamp
#   purpose:  echo timestamp in the form yyyy-mm-ddThh:mm:ss.nnnnnnZ
#   example:  2018-10-01T14:00:00.000001Z
function timestamp() {
  local timestamp="`date --utc '+%Y-%m-%dT%H:%M:%S.%NZ' 2>&1`"
  if [ ! "${timestamp/illegal/xyz}" = "${timestamp}" ]; then
    # old shell versions don't support %N or --utc
    timestamp="`date -u '+%Y-%m-%dT%H:%M:%S.000000Z' 2>&1`"
  fi
  echo "${timestamp}"
}

# 
# checkEnv [-q] envvar1 envvar2 ...
#
#   purpose: Check and trace the values of the provided env vars.
#            If any env vars don't exist or are empty, return non-zero
#            and trace an '[SEVERE]'.
#            (Pass '-q' to suppress FINE tracing.)
#
#   sample:  checkEnv HOST NOTSET1 USER NOTSET2
#            @[2018-10-05T22:48:04.368000Z][FINE] HOST='esscupcakes'
#            @[2018-10-05T22:48:04.393000Z][FINE] USER='friendly'
#            @[2018-10-05T22:48:04.415000Z][SEVERE] The following env vars are missing or empty:  NOTSET1 NOTSET2
#
function checkEnv() {
  local do_fine="true"
  if [ "$1" = "-q" ]; then 
    do_fine="false"
    shift
  fi
  
  local not_found=""
  local val_1
  while [ ! -z "${1}" ]; do 
    eval "val_1=\"\$${1}\""
    if [ -z "${val_1}" ]; then
      not_found="$not_found ${1}"
    else
      [ "$do_fine" = "true" ] && trace FINE "${1}='${val_1}'"
    fi
    shift
  done
  if [ ! -z "${not_found}" ]; then
    trace SEVERE "The following env vars are missing or empty: ${not_found}"
    return 1
  fi
  return 0
}

#
# initAuxiliaryImage
#   purpose: Execute the AUXILIARY_IMAGE_COMMAND specified as part of the auxiliary image init container.
#            If the specified AUXILIARY_IMAGE_COMMAND is empty, it logs an error message and returns.
#            If the AUXILIARY_IMAGE_PATH directory doesn't exist or is empty, it logs error and returns.
#            If the command execution fails, it logs error message with failure details. Otherwise it
#            logs a success message with details.
#            See also 'auxImage.sh'.
#            See also checkAuxiliaryImage in 'utils.sh'.
#
function initAuxiliaryImage() {

  if [ -z "${AUXILIARY_IMAGE_COMMAND}" ]; then
    trace SEVERE "Auxiliary Image: The 'serverPod.auxiliaryImages.command' is empty for the " \
                "container image='$AUXILIARY_IMAGE_CONTAINER_IMAGE'. Exiting."
    return
  fi

  trace FINE "Auxiliary Image: About to execute command '$AUXILIARY_IMAGE_COMMAND' in container image='$AUXILIARY_IMAGE_CONTAINER_IMAGE'. " \
             "AUXILIARY_IMAGE_PATH is '$AUXILIARY_IMAGE_PATH' and AUXILIARY_IMAGE_TARGET_PATH is '${AUXILIARY_IMAGE_TARGET_PATH}'."
  traceDirs before $AUXILIARY_IMAGE_PATH

  trace FINE "Auxiliary Image: About to execute AUXILIARY_IMAGE_COMMAND='$AUXILIARY_IMAGE_COMMAND' ."
  results=$(eval $AUXILIARY_IMAGE_COMMAND 2>&1)
  if [ $? -ne 0 ]; then
    trace SEVERE "Auxiliary Image: Command '$AUXILIARY_IMAGE_COMMAND' execution failed in container image='$AUXILIARY_IMAGE_CONTAINER_IMAGE' " \
                "with AUXILIARY_IMAGE_PATH=$AUXILIARY_IMAGE_PATH. Error -> '$results' ."
  else
    trace FINE "Auxiliary Image: Command '$AUXILIARY_IMAGE_COMMAND' executed successfully. Output -> '$results'."
  fi
}

start-server.py:
----
# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# This is a WLST script for starting WL server via the node manager.
#
# It users key and data files that were generated by the introspector
# for its nmConnect credentials.
#

import sys;
import base64

def getEnvVar(var):
  val=os.environ.get(var)
  if val==None:
    print "ERROR: Env var ",var, " not set."
    sys.exit(1)
  return val

domain_uid = getEnvVar('DOMAIN_UID')
server_name = getEnvVar('SERVER_NAME')
domain_name = getEnvVar('DOMAIN_NAME')
domain_path = getEnvVar('DOMAIN_HOME')
service_name = getEnvVar('SERVICE_NAME')

print 'domain path is %s' % domain_path
print 'server name is %s' % server_name

# convert b64 encoded user key file into binary

file = open('/weblogic-operator/introspector/userKeyNodeManager.secure', 'r')
contents = file.read()
file.close()
decoded=base64.decodestring(contents)

file = open('/tmp/userKeyNodeManager.secure.bin', 'wb')
file.write(decoded)
file.close()

# Connect to nodemanager and start server

try:
  nmConnect(userConfigFile='/weblogic-operator/introspector/userConfigNodeManager.secure',
            userKeyFile='/tmp/userKeyNodeManager.secure.bin',
            host='127.0.0.1',port='5556',
            domainName=domain_name,
            domainDir=domain_path,nmType='plain')
  nmStart(server_name)
  nmDisconnect()
except WLSTException, e:
  nmDisconnect()
  print e

exit()

stop-server.py:
----
# Copyright (c) 2017, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

# This is called by the preStop hook script (stopServer.sh) that is specified
# by the WL operator for WL pod resource.  Before killing a pod, Kubernetes calls
# the preStop hook script.  Kubernetes will then kill the pod once the script returns
# or when the grace period timeout expires(see Operator shutdown.timeoutSeconds)
#
# This script shuts down the local server running in the container (admin or managed).
# There are 2 main scenarios, a domain with a Coherence cluster and one without.
# If there is no Coherence cluster, then the code will attempt to connect to the
# local server and do a shutdown.  If that doesn't work it will attempt to shutdown
# using NodeManager.  If that fails it just exits and kubernetes will kill the pod.
# If a Coherence cluster exists, then we must wait for the Coherence services to be
# safe before the server is shutdown, or we risk losing data.  This requires a
# a connection to the admin server and inspection of the Coherence MBeans.  The code
# will not shutdown the server until Coherence is safe.  If Coherence never becomes safe,
# then, eventually, the grace period will expire and Kubernetes will kill the pod.
#
# It users key and data files that were generated by the introspector
# for its nmConnect credentials.
#

import sys
import os
import traceback
import base64
import time as systime
import re

# Get an ENV var
def getEnvVar(var):
  val=os.environ.get(var)
  if val==None:
    print "ERROR: Env var ",var, " not set."
    sys.exit(1)
  return val


# Connect to Node Manager and shut down the local server
#
def shutdownUsingNodeManager(domainName, domainDir):
  try:
    print('Shutdown: Attempting shutdown using NodeManager')
    nmConnect(userConfigFile='/weblogic-operator/introspector/userConfigNodeManager.secure',
              userKeyFile='/tmp/userKeyNodeManager.secure.bin',
              host='127.0.0.1',port='5556',
              domainName=domainName,
              domainDir=domainDir,
              nmType='plain')
    print('Shutdown: Successfully connected to NodeManager')
    nmKill(server_name)
    print('Shutdown: Successfully killed server using NodeManager')
  except Exception, e:
    traceback.print_exc(file=sys.stdout)
    print('Shutdown: Failed to kill server using NodeManager')
    raise


# Return True if Coherence exists
def doesCoherenceExist():
  try:
    f = open(domain_path+'/config/config.xml', 'r')
    configData = f.read()
    f.close()
    return checkCoherenceClusterExist(configData)
  except:
    print('Shutdown: Exception reading config.xml, assume Coherence exists')
    return True


# Return True if there is a CoherenceClusterSystemResource. This will indicate that the domain is
# using Coherence
def checkCoherenceClusterExist(configData):
  try:
    # remove all whitespace include CR
    spacelessData = ''.join(configData.split())

    ELEMENT_NAME =  "coherence-cluster-system-resource"
    x =  re.search('<coherence-cluster-system-resource>[\s\S]*?<\/coherence-cluster-system-resource>',spacelessData)
    if (x is None):
      return False
    else:
      first, last = x.span()
      # check if the element value is empty (add 5 for the XML slash and 4 angle brackets)
      return (last-first-5) > (2 * len(ELEMENT_NAME))
  except:
    traceback.print_exc(file=sys.stdout)
    print('Shutdown: Exception processing config data, assume Coherence exists')
    return True


# If there is a Coherence cluster then we wait until it is safe to shutdown
# all distributed services.  Each distributed cache (with backup count > 0) has data
# that is in both in a primary partition on one cluster node (i.e. WLS server) and
# a backup partition on a different node.
# During rolling restart, Coherence rebalances the cluster and moves both primary and
# backup partitions across the nodes.  During this time the distributed services affected
# are consider ENDANGERED.  For example, in a 3 node cluster, if node 1 was being restarted
# and we shut down node 2 before Coherence was safe then we may lose data.
# NOTE: The overall timeout used to control the termination of this container is
# controlled by the Kubernetes graceful timeout value. So even
# if this code looped forever, Kubernetes will kill the pod once the timeout expires.
# The user must set that timeout to a large enough value to give Coherence time to get safe.
def waitUntilCoherenceSafe():
  print ('Shutdown: getting all service Coherence MBeans')
  query='Coherence:type=PartitionAssignment,service=*,*'

  # By default, Coherence will use a single WebLogic Runtime MBean server to managed
  # its MBeans.  That server will correspond to the current Coherence senior member,
  # which means that the Coherence MBeans will migrate to the oldest cluster member
  # during rolling restart.  By using the DomainRuntime MBean server (in admin server)
  # we can get access to the Coherence MBeans, regardless of the local Runtime MBean
  # server being used, since the DomainRuntime server is federated and will call the
  # individual MBean servers to get the MBeans throughout the domain.
  domainRuntime()

  # Wait forever until we get positive ack that it is ok to shutdown this server.
  done = False
  warnSleep = True
  while (not done):
    try:
      beans = list(mbs.queryMBeans(ObjectName(query), None))
      if beans is None or len(beans) == 0:
        # during rolling restart the beans might not be available right away
        # we need to wait since we know Coherence is enabled
        if warnSleep:
          print('Shutdown: Waiting until Coherence MBeans available... ')
          warnSleep = False
        systime.sleep(1)
        continue

      # Loop waiting for each service to be safe
      print('Shutdown: Coherence service bean count ' + str(len(beans)))
      for serviceBean in beans:
        objectName = serviceBean.getObjectName()
        waitUntilServiceSafeToShutdown(objectName)
      done = True
      print ('Shutdown: It is safe to shutdown Coherence')

    except:
      print ("Shutdown: Exception checking a service Coherence HAStatus, retrying...")
      traceback.print_exc(file=sys.stdout)
      dumpStack()
      systime.sleep(10)
      pass


# Wait until the specified Coherence service is safe to shutdown.
# If the cluster is a single node cluster then the service will always
# be ENDANGERED, therefore it is the responsibility of the user to
# set Coherence backup count to 0, or to set the terminate grace period
# to a low number since this method will just wait until the kubernetes kills the
# pod.
def waitUntilServiceSafeToShutdown(objectName):

  print ("Shutdown: checking Coherence service " + str(objectName))

  # NOTE: break loop when it safe to shutdown else stay in loop forever
  while (True):
    try:
      status = mbs.getAttribute(objectName,"HAStatus")
      if (status is None):
        print ("Shutdown: None returned for Coherence HAStatus")
        break

      print ('Shutdown: Coherence HAStatus is ' + status)
      if status != "ENDANGERED":
        break

      # Coherence caches are ENDANGERED meaning that we may lose data
      print ('Shutdown: Waiting until it is safe to shutdown Coherence server ...')
      systime.sleep(5)

    except:
      print ('Shutdown: An exception occurred getting Coherence MBeans, staying in loop checking for safe')
      traceback.print_exc(file=sys.stdout)
      dumpStack()
      systime.sleep(10)
      pass


#----------------------------------
# Main script
#----------------------------------
print ("Shutdown: main script")
domain_uid = getEnvVar('DOMAIN_UID')
admin_name = getEnvVar('ADMIN_NAME')
server_name = getEnvVar('SERVER_NAME')
domain_name = getEnvVar('DOMAIN_NAME')
domain_path = getEnvVar('DOMAIN_HOME')
service_name = getEnvVar('SERVICE_NAME')
local_admin_port = getEnvVar('SHUTDOWN_PORT_ARG')
local_admin_protocol = getEnvVar('SHUTDOWN_PROTOCOL_ARG')
timeout = getEnvVar('SHUTDOWN_TIMEOUT_ARG')
ignore_sessions = getEnvVar('SHUTDOWN_IGNORE_SESSIONS_ARG')
shutdown_type = getEnvVar('SHUTDOWN_TYPE_ARG')
admin_port = getEnvVar('ADMIN_PORT')
admin_host = getEnvVar('AS_SERVICE_NAME')

force = 'false'
if shutdown_type.lower() == 'forced':
  force = 'true'

# Convert b64 encoded user key into binary
file = open('/weblogic-operator/introspector/userKeyNodeManager.secure', 'r')
contents = file.read()
file.close()
decoded=base64.decodestring(contents)

file = open('/tmp/userKeyNodeManager.secure.bin', 'wb')
file.write(decoded)
file.close()

# check if Coherence cluster exists in this domain
cohExists = doesCoherenceExist()

# If Coherence exists then we need to connect to admin server, else local server
if (cohExists):
  print ('Shutdown: Coherence cluster exists')
  connect_url = local_admin_protocol + '://' + admin_host + ':' + admin_port

  # must use force shutdown for admin server since Coherence MBeans cannot be found after
  # a graceful admin server restart
  if admin_name == server_name:
    force = 'true'

else:
  print ('Shutdown: Coherence cluster does not exist')
  connect_url = local_admin_protocol + '://' + service_name + ':' + local_admin_port

# Stay in loop until the server is shutdown if Coherence exists.  For non-Coherence
# just make a best effort
stayInConnectLoop = True
cohSafe = False
while (stayInConnectLoop):
  try:
    stayInConnectLoop = False
    print ('Shutdown: Connecting to server at ' + connect_url)
    connect(userConfigFile='/weblogic-operator/introspector/userConfigNodeManager.secure',
            userKeyFile='/tmp/userKeyNodeManager.secure.bin',
            url=connect_url,
            domainName=domain_name,
            domainDir=domain_path,
            nmType='plain')

    print('Shutdown: Successfully connected to server')
    if (cohExists):
      waitUntilCoherenceSafe()
      cohSafe = True

    print('Shutdown: Calling server shutdown with force = ' + force)
    shutdown(server_name, 'Server', ignoreSessions=ignore_sessions, timeOut=int(timeout), block='true', force=force)
    print('Shutdown: Successfully shutdown the server')

  except Exception, e:
    print e
    print('Shutdown: Exception in connect or shutdown')
    if (cohExists and not cohSafe):
      print('Shutdown: Coherence not safe to shutdown. Sleeping before connect retry ...')
      stayInConnectLoop = True
      systime.sleep(10)
    else:
      try:
        shutdownUsingNodeManager(domain_name, domain_path)
        exit()
      except:
        exit(2)

# Exit WLST
exit()

wlst.sh:
----
#!/bin/bash
# Copyright (c) 2018, 2021, Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

#
# Summary:
#   This helper script finds and runs weblogic.WLST for a given WLST script,
#   and exits Exits non-zero on failure.
#
# Input env vars:
#    - JAVA_HOME - required
#    - Optionally set
#        ORACLE_HOME = Oracle Install Home - defaults via utils.sh/exportInstallHomes
#        MW_HOME     = MiddleWare Install Home - defaults to ${ORACLE_HOME}
#        WL_HOME     = WebLogic Install Home - defaults to ${ORACLE_HOME}/wlserver
#
# Usage:
#   SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
#   $SCRIPTPATH/wlst.sh myscript.py
#

SCRIPTPATH="$( cd "$(dirname "$0")" > /dev/null 2>&1 ; pwd -P )"
source ${SCRIPTPATH}/utils.sh
[ $? -ne 0 ] && echo "[SEVERE] Missing file ${SCRIPTPATH}/utils.sh" && exit 1 

wlst_script=${1?}

trace "About to run wlst script '${wlst_script}'"

# Set ORACLE_HOME/WL_HOME/MW_HOME to defaults if needed
exportInstallHomes

checkEnv JAVA_HOME \
         ORACLE_HOME \
         WL_HOME \
         MW_HOME \
         || exit 1

[ ! -f "$wlst_script" ] && trace SEVERE "Missing file '$wlst_script'." && exit 1 

wlst_sh=""
wlst_loc1="${WL_HOME}/../oracle_common/common/bin/wlst.sh"
wlst_loc2="${MW_HOME}/oracle_common/common/bin/wlst.sh"
[ -f "$wlst_loc2" ]   && wlst_sh="$wlst_loc2"
[ -f "$wlst_loc1" ]   && wlst_sh="$wlst_loc1"
[ -z "$wlst_sh" ] && trace SEVERE "'${wlst_loc1}' or '${wlst_loc2}' not found, make sure ORACLE_HOME, WL_HOME, or MW_HOME is set correctly." && exit 1

urandom_prop="-Djava.security.egd=file:/dev/./urandom"
if [ ! -v WLST_EXTRA_PROPERTIES ]; then
  trace "Env var WLST_EXTRA_PROPERTIES not set, defaulting to '$urandom_prop'."
  WLST_EXTRA_PROPERTIES="$urandom_prop"
else
  trace "Env var WLST_EXTRA_PROPERTIES has been externally set to '$WLST_EXTRA_PROPERTIES' (it will not default to '$urandom_prop')."
fi
export WLST_PROPERTIES="$WLST_PROPERTIES $WLST_EXTRA_PROPERTIES"
trace "Env var WLST_PROPERTIES='$WLST_PROPERTIES' (includes WLST_EXTRA_PROPERTIES)."
if [ "${WLST_PROPERTIES/$urandom_prop/}" = "${WLST_PROPERTIES}" ]; then
  trace "Env var WLST_PROPERTIES does not include '$urandom_prop'; this may significantly slow any use of the WLST encrypt command on low entropy hosts."
fi

trace "Running wlst script '${wlst_script}'"

${wlst_sh} -skipWLSModuleScanning ${wlst_script} "${@:2}"
res="$?"

if [ $res -eq 0 ]; then
  trace "WLST script '${wlst_script}' completed."
else
  trace SEVERE "WLST script '${wlst_script}' failed with exit code '$res'." 
fi

exit $res

Events:  <none>
